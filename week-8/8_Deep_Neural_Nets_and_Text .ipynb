{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcKvlaPu_vEV"
   },
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "0wkOkQEq_vEX",
    "outputId": "18012e04-da9e-49da-fbd0-89b4e85be4d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud_2020\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5gisRvu_vEk"
   },
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERTâ€™s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N95gohHk_vEm"
   },
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4Q8Ko8k_vEp"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxY5BiGc_vEw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1201KxIm_vEy"
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "eiV-Z99Z_vE2",
    "outputId": "57a1fc90-3669-4f49-a79a-c0fe0ff6dda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfLgjFaP_vE4"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0Dj0Lre_vE8"
   },
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nD6TGA2b_vE_"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNEW8qu8_vFB"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYIs4ut__vFE"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSbb5XG__vFH"
   },
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTELVHHG_vFI"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tI_X2fiL_vFK"
   },
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "a8j9NVi1_vFN",
    "outputId": "15284a5e-f5b5-4d68-9b18-15ebb7ee46ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "gVIMarSt_vFQ",
    "outputId": "6166f170-d69f-47d7-cf4a-8cd922af9045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6120 - acc: 0.7015\n",
      "Epoch 2/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6096 - acc: 0.7038\n",
      "Epoch 3/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6086 - acc: 0.7038\n",
      "Epoch 4/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6090 - acc: 0.7038\n",
      "Epoch 5/10\n",
      "7695/7695 [==============================] - 45s 6ms/step - loss: 0.6087 - acc: 0.7038\n",
      "Epoch 6/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6082 - acc: 0.7038\n",
      "Epoch 7/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6083 - acc: 0.7038\n",
      "Epoch 8/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6081 - acc: 0.7038\n",
      "Epoch 9/10\n",
      "7695/7695 [==============================] - 45s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
      "Epoch 10/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6085 - acc: 0.7038\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5IsHLeNV_vFS"
   },
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "iptTlZNb_vFT",
    "outputId": "ef639903-49ef-40a1-bbd5-ebbc2dc1f7a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7695/7695 [==============================] - 47s 6ms/step - loss: 0.6132 - acc: 0.7012\n",
      "Epoch 2/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6087 - acc: 0.7038\n",
      "Epoch 3/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6094 - acc: 0.7038\n",
      "Epoch 4/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6086 - acc: 0.7038\n",
      "Epoch 5/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6087 - acc: 0.7038\n",
      "Epoch 6/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6089 - acc: 0.7038\n",
      "Epoch 7/10\n",
      "7695/7695 [==============================] - 45s 6ms/step - loss: 0.6085 - acc: 0.7038\n",
      "Epoch 8/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6085 - acc: 0.7038\n",
      "Epoch 9/10\n",
      "7695/7695 [==============================] - 45s 6ms/step - loss: 0.6083 - acc: 0.7038\n",
      "Epoch 10/10\n",
      "7695/7695 [==============================] - 46s 6ms/step - loss: 0.6082 - acc: 0.7038\n"
     ]
    }
   ],
   "source": [
    "model_lstm2 = Sequential()\n",
    "model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm2.add(LSTM(unit))\n",
    "model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_lstm2 = model_lstm2.fit(train_inputs, train_labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWfZdfV5_vFV"
   },
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWFFo2cU_vFX"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkZMOZFx_vFa"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRAUjuKj_vFc"
   },
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## NOTE\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, run the similar cells on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n",
    "You won't need to run the cells below. It will take about 12 hours on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y76Qnssn_vFc"
   },
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IulKWsN8_vFf",
    "outputId": "b94252a4-68c8-47cd-886d-77c73c2f7e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgpMaoGn_vFh"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5-ac3AT_vFj"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLcFx4dW_vFm"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_kq_HUd_vFo"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IO19CYL_vFr"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErGRihxa_vFv"
   },
   "source": [
    "## WARNING\n",
    "\n",
    "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BuzYFnt_vFw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ucz6Pj2Y_vF0"
   },
   "source": [
    "### Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "7lI-hYQg_vF0",
    "outputId": "fa421cf3-ebcf-44cd-af06-b734c167ba3b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREOhAl8CIiIiIiHcj/A0zGoudfaAfVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZyAqwVq_vF4"
   },
   "source": [
    "### Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "VTnWxgKl_vF5",
    "outputId": "598f3605-6de7-47b9-83ad-07a4f1adced8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"../content/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "33-X_z-u_vF7",
    "outputId": "2133566e-373d-4cf8-c64e-cf169a83b5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GUIDtGs-_vF9",
    "outputId": "185c4352-43a9-4588-de8f-ef0f9b111c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Wd6ecUoO_vF_",
    "outputId": "62a13b66-5696-4205-c293-20c634cb644b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "82VpmvAl_vGB",
    "outputId": "d26ff52d-a453-4687-fbb9-31627eb80cd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.049286405809014416,\n",
       " -0.050964719143762556,\n",
       " 0.4732058754737091,\n",
       " 0.23372319715296222,\n",
       " 0.44440090347500916,\n",
       " 0.7410010097502685,\n",
       " 0.4152273992686999,\n",
       " 0.47519096331149147,\n",
       " 1.0,\n",
       " 0.7704873741021288,\n",
       " 0.8459051693633014,\n",
       " 0.5673665146135802,\n",
       " 0.7562449037944323,\n",
       " 0.7141684885491869,\n",
       " 0.3268228676411533,\n",
       " 0.4252964776724258,\n",
       " 0.0]"
      ]
     },
     "execution_count": 143,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "z499DRax_vGE",
    "outputId": "8711b7b5-1b45-4f2a-bc15-3274e4c82916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.524\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBgeCgDK_vGG"
   },
   "source": [
    "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. Let us now save this model to disk. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "FVziJV6b_vGG",
    "outputId": "5ef80246-af0b-4dde-ecbe-cd24f47a1fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 145,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXD7ELnJ_vGK"
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yc4IOcBzJ9vI",
    "outputId": "0ba426d5-0704-4813-b7d3-a0a4319796d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
      "  Cloning git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git to /tmp/pip-req-build-nh7n9e_4\n",
      "  Running command git clone -q git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git /tmp/pip-req-build-nh7n9e_4\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.17.5)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.25.3)\n",
      "Collecting python-docx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.5MB 14.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (6.2.2)\n",
      "Collecting pdfminer2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/97/bd2a2de878438c27ffd710b5d6562c7a0230b0f3ca86059ec635ed231eb1/pdfminer2-20151206-py2.py3-none-any.whl (117kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 61.3MB/s \n",
      "\u001b[?25hCollecting GitPython\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 62.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wordcloud in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.22.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.2.5)\n",
      "Requirement already satisfied, skipping upgrade: gensim in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.1.3)\n",
      "Collecting pyanno3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/1a/ee2b136ea0283adb2a9302c29594127f84b6e34cb0b02b91c63bed0a534b/pyanno3-2.0.2.tar.gz (76kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 13.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (4.6.3)\n",
      "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.11.15)\n",
      "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.4)\n",
      "Collecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
      "Collecting speechrecognition\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.8MB 89kB/s \n",
      "\u001b[?25hCollecting pysoundfile\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: IPython in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (5.5.0)\n",
      "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.1.9)\n",
      "Collecting transformers==2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481kB 28.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.2.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->lucem-illud-2020==8.0.1) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->lucem-illud-2020==8.0.1) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx->lucem-illud-2020==8.0.1) (4.2.6)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from pdfminer2->lucem-illud-2020==8.0.1) (1.12.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 11.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lucem-illud-2020==8.0.1) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->lucem-illud-2020==8.0.1) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (0.10.0)\n",
      "Collecting traits\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/af/c6dc88130106d69e4f9a192043c85ed4cb522f83b9041b8691f0b0678405/traits-6.0.0.tar.gz (441kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450kB 60.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (1.14.15)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->lucem-illud-2020==8.0.1) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=0.6 in /usr/local/lib/python3.6/dist-packages (from pysoundfile->lucem-illud-2020==8.0.1) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->lucem-illud-2020==8.0.1) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->lucem-illud-2020==8.0.1) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (1.0.18)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (2.1.3)\n",
      "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (7.0.8)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.2.4)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.6.0)\n",
      "Collecting tokenizers==0.0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 49.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.27.1)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->lucem-illud-2020==8.0.1) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras->lucem-illud-2020==8.0.1) (2.8.0)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->lucem-illud-2020==8.0.1) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->lucem-illud-2020==8.0.1) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=0.6->pysoundfile->lucem-illud-2020==8.0.1) (2.19)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->lucem-illud-2020==8.0.1) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->lucem-illud-2020==8.0.1) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->lucem-illud-2020==8.0.1) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->lucem-illud-2020==8.0.1) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->lucem-illud-2020==8.0.1) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->lucem-illud-2020==8.0.1) (1.0.0)\n",
      "Building wheels for collected packages: lucem-illud-2020, python-docx, pyanno3, traits\n",
      "  Building wheel for lucem-illud-2020 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lucem-illud-2020: filename=lucem_illud_2020-8.0.1-cp36-none-any.whl size=35151 sha256=6fe262d2519515d4d0c8ee57b09fee88968279d169e22676e5289bbcfc1ce189\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pkfg_lb5/wheels/a8/16/91/3c63788e494d360378317fe5ec9f4972f661844af8ae8c26f0\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=07354d02ee73e209140488e1ee1b44273bb4c9172a60a845375b30b54226b81a\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
      "  Building wheel for pyanno3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyanno3: filename=pyanno3-2.0.2-cp36-none-any.whl size=116993 sha256=7b2c2206d36e1ec1f36bba1633109f0113cfaeb054db3e4bd4b056780427e225\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/5d/f6/7c1618b7471ec03ac97f6913baba31ae007003c4fa2bc99855\n",
      "  Building wheel for traits (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for traits: filename=traits-6.0.0-cp36-cp36m-linux_x86_64.whl size=385396 sha256=9341908ec11bfc320f1916904bf169e5d3f7a852626460dd18b6e6126389e131\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/a7/f0/d1dfae8d3a4e5638a40818830c741c1c0e9f8a590b9ea22935\n",
      "Successfully built lucem-illud-2020 python-docx pyanno3 traits\n",
      "Installing collected packages: python-docx, pdfminer2, smmap, gitdb, GitPython, traits, pyanno3, pydub, speechrecognition, pysoundfile, tokenizers, transformers, lucem-illud-2020\n",
      "  Found existing installation: tokenizers 0.5.2\n",
      "    Uninstalling tokenizers-0.5.2:\n",
      "      Successfully uninstalled tokenizers-0.5.2\n",
      "  Found existing installation: transformers 2.5.1\n",
      "    Uninstalling transformers-2.5.1:\n",
      "      Successfully uninstalled transformers-2.5.1\n",
      "Successfully installed GitPython-3.1.0 gitdb-4.0.2 lucem-illud-2020-8.0.1 pdfminer2-20151206 pyanno3-2.0.2 pydub-0.23.1 pysoundfile-0.9.0.post1 python-docx-0.8.10 smmap-3.0.1 speechrecognition-3.8.1 tokenizers-0.0.11 traits-6.0.0 transformers-2.4.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tokenizers",
         "transformers"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKuDC9HSU7rC"
   },
   "outputs": [],
   "source": [
    "#to prepare the model (gpus, built-in functions, import relevant packages, etc.)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from transformers import get_linear_schedule_with_warmup,BertTokenizer, BertConfig,AdamW, BertForSequenceClassification\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from keras import optimizers\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import lucem_illud_2020\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "8515a6cd31b94ce3b90f393af567f60e",
      "da53fd3c326f48cd86f6c87d3d3e2515",
      "7428a9bdac9a439d9caa83f4f95f440f",
      "52c4fadf83ee4ebf825dab5a3ca82177",
      "2214f935952241c896122dc30fdba56c",
      "450ec91b8b154f3d9b1bb9de297132bf",
      "0366a1c198b74cf8bc59f7b989a78962",
      "835df7e043fb439492cc8a0a79c8c1c0"
     ]
    },
    "colab_type": "code",
    "id": "wmi5o7i1XQTh",
    "outputId": "750f3b7c-315b-4560-8d51-d8297b447e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First we load the database. In this exercise, we use the business description for about 60000 companies in U.S from 10 different industries.In the first exercise, we tries to predict the industry of the companies. We use the max length of descriptions, 390, as our maximized input length.To make it more efficient, we randomly sample 20% from our data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8515a6cd31b94ce3b90f393af567f60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "Here, we tokenize the first sentence:\n",
      "['[CLS]', 'view', '##lo', '##city', ',', 'inc', '.', 'is', 'engaged', 'in', 'providing', 'adaptive', 'supply', 'chain', 'management', '(', 'as', '##cm', ')', 'software', 'and', 'service', 'solutions', '.', 'the', 'company', \"'\", 's', 'solutions', 'enable', 'companies', 'to', 'operate', 'their', 'complex', 'supply', 'chains', 'more', 'efficiently', 'and', 'reach', 'their', 'customers', 'more', 'effectively', 'by', 'providing', 'solutions', 'for', 'planning', 'and', 'managing', 'the', 'processes', ',', 'interactions', 'and', 'transactions', 'that', 'are', 'inherent', 'in', 'operating', 'an', 'extended', 'supply', 'chain', 'among', 'vendors', 'and', 'customers', '.', 'the', 'company', \"'\", 's', 'solutions', 'enable', 'companies', 'to', 'bridge', 'the', 'gap', 'between', 'supply', 'chain', 'planning', 'and', 'execution', 'while', 'managing', 'the', 'constant', 'flow', 'of', 'events', 'and', 'exceptions', 'to', 'their', 'plan', 'that', 'inhibit', 'the', 'financial', 'and', 'operational', 'performance', 'of', 'the', 'supply', 'chain', '.', '[SEP]']\n",
      " \n",
      "We separate train set and validation set with the proportion of 9:1.\n"
     ]
    }
   ],
   "source": [
    "print(\"First we load the database. In this exercise, we use the business description for about 60000 companies in U.S from 10 different industries.In the first exercise, we tries to predict the industry of the companies. We use the max length of descriptions, 390, as our maximized input length.To make it more efficient, we randomly sample 20% from our data.\")\n",
    "df = pd.read_csv(\"../content/description.csv\",sep=',', header=0)\n",
    "df['siccolor'].fillna(11, inplace=True)\n",
    "df=df.sample(frac=0.2, axis=0,random_state=2020)\n",
    "sentences = df.companybusinessdescriptionlong.values\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = np.array([int(x) for x in df['siccolor']])\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(\" \")\n",
    "print (\"Here, we tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])\n",
    "MAX_LEN = 390\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "print(\" \")\n",
    "print(\"We separate train set and validation set with the proportion of 9:1.\")\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "llAFxMxLYu4m",
    "outputId": "27c5c450-a12d-4d1e-bdbd-c4c0829b838d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, Keras deep classification model with LSTM.\n",
      "In this model, embedding dim is set to 32, unit set to 100 and batch_size set to 128.\n",
      "Since our outcome is a categorical variable, we change the output layer to a 11-dimensional vector.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      " \n",
      "Here is its summary and training process.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 390, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 1,031,116\n",
      "Trainable params: 1,031,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "11007/11007 [==============================] - 50s 5ms/step - loss: 2.4665 - acc: 0.3302\n",
      "Epoch 2/4\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 2.4333 - acc: 0.4144\n",
      "Epoch 3/4\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 2.2737 - acc: 0.4144\n",
      "Epoch 4/4\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 1.7819 - acc: 0.4144\n",
      "It can be seen that the performance of this model is keep increasing, though still not so accurate.\n"
     ]
    }
   ],
   "source": [
    "print(\"1, Keras deep classification model with LSTM.\")\n",
    "print(\"In this model, embedding dim is set to 32, unit set to 100 and batch_size set to 128.\")\n",
    "print(\"Since our outcome is a categorical variable, we change the output layer to a 11-dimensional vector.\")\n",
    "vocab_in_size = tokenizer.vocab_size\n",
    "categorical_labels = to_categorical(train_labels, num_classes=None)\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 128\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels+1, activation='softmax'))\n",
    "adam= optimizers.adam(0.00002)\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print(\" \")\n",
    "print(\"Here is its summary and training process.\")\n",
    "print(model_lstm.summary())\n",
    "classification_lstm = model_lstm.fit(train_inputs, categorical_labels, epochs=4,batch_size=batch_size)\n",
    "print(\"It can be seen that the performance of this model is keep increasing, though still not so accurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ujD248AMyUBx",
    "outputId": "3ae169d5-d605-481c-a1c9-7c1b055652de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, Stretch: stacking layers and changing parameters.\n",
      "In this model, first we stack several more layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      " \n",
      "Here is its summary and training process.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 390, 32)           976704    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 390, 100)          3300      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 390, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 1,071,716\n",
      "Trainable params: 1,071,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/8\n",
      "11007/11007 [==============================] - 48s 4ms/step - loss: 2.9634 - acc: 0.0561\n",
      "Epoch 2/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 2.8453 - acc: 0.0629\n",
      "Epoch 3/8\n",
      "11007/11007 [==============================] - 48s 4ms/step - loss: 2.6535 - acc: 0.0880\n",
      "Epoch 4/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 2.0830 - acc: 0.2348\n",
      "Epoch 5/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.8905 - acc: 0.3162\n",
      "Epoch 6/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.8219 - acc: 0.3569\n",
      "Epoch 7/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.7934 - acc: 0.3636\n",
      "Epoch 8/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.7809 - acc: 0.3720\n",
      "--\n",
      " \n",
      "Now we keep the new structure of the network and change the embedding dim (to 128).\n",
      " \n",
      "Here is its summary and training process.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 390, 128)          3906816   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 390, 100)          12900     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 390, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 4,011,428\n",
      "Trainable params: 4,011,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 3.0750 - acc: 0.0333\n",
      "Epoch 2/8\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 2.9392 - acc: 0.0438\n",
      "Epoch 3/8\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 2.7550 - acc: 0.0710\n",
      "Epoch 4/8\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 2.1698 - acc: 0.2978\n",
      "Epoch 5/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.9744 - acc: 0.3737\n",
      "Epoch 6/8\n",
      "11007/11007 [==============================] - 46s 4ms/step - loss: 1.8996 - acc: 0.3816\n",
      "Epoch 7/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.8683 - acc: 0.3736\n",
      "Epoch 8/8\n",
      "11007/11007 [==============================] - 47s 4ms/step - loss: 1.8416 - acc: 0.3784\n",
      "It can be seen that both stacking layers or increasing the number of embedding dims will improve the original performance of the neural network.\n"
     ]
    }
   ],
   "source": [
    "print(\"2, Stretch: stacking layers and changing parameters.\")\n",
    "print(\"In this model, first we stack several more layers.\")\n",
    "from keras.layers import Dense, Dropout\n",
    "vocab_in_size = tokenizer.vocab_size\n",
    "categorical_labels = to_categorical(train_labels, num_classes=None)\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 128\n",
    "model_lstm21 = Sequential()\n",
    "model_lstm21.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm21.add(Dense(unit,activation='relu'))\n",
    "model_lstm21.add(Dropout(0.4))\n",
    "model_lstm21.add(LSTM(unit))\n",
    "model_lstm21.add(Dense(unit,activation='sigmoid'))\n",
    "model_lstm21.add(Dropout(0.4))\n",
    "model_lstm21.add(Dense(no_labels+1, activation='softmax'))\n",
    "adam= optimizers.adam(0.00002)\n",
    "model_lstm21.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print(\" \")\n",
    "print(\"Here is its summary and training process.\")\n",
    "print(model_lstm21.summary())\n",
    "classification_lstm = model_lstm21.fit(train_inputs, categorical_labels, epochs=8,batch_size=batch_size)\n",
    "print(\"--\")\n",
    "print(\" \")\n",
    "print(\"Now we keep the new structure of the network and change the embedding dim (to 128).\")\n",
    "vocab_in_size = tokenizer.vocab_size\n",
    "categorical_labels = to_categorical(train_labels, num_classes=None)\n",
    "embedding_dim = 128\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 128\n",
    "model_lstm22 = Sequential()\n",
    "model_lstm22.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm22.add(Dense(unit,activation='relu'))\n",
    "model_lstm22.add(Dropout(0.4))\n",
    "model_lstm22.add(LSTM(unit))\n",
    "model_lstm22.add(Dense(unit,activation='sigmoid'))\n",
    "model_lstm22.add(Dropout(0.4))\n",
    "model_lstm22.add(Dense(no_labels+1, activation='softmax'))\n",
    "adam= optimizers.adam(0.00002)\n",
    "model_lstm22.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print(\" \")\n",
    "print(\"Here is its summary and training process.\")\n",
    "print(model_lstm22.summary())\n",
    "classification_lstm = model_lstm22.fit(train_inputs, categorical_labels, epochs=8,batch_size=batch_size)\n",
    "print(\"It can be seen that both stacking layers or increasing the number of embedding dims will improve the original performance of the neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zl4Ju0YIYclr",
    "outputId": "a5678a66-a7e8-49fc-be89-ed4a6fda6b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3, BERT\n",
      "Here is the result of BERT.\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,376.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  1,376.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  1,376.    Elapsed: 0:00:46.\n",
      "  Batch   160  of  1,376.    Elapsed: 0:01:01.\n",
      "  Batch   200  of  1,376.    Elapsed: 0:01:16.\n",
      "  Batch   240  of  1,376.    Elapsed: 0:01:31.\n",
      "  Batch   280  of  1,376.    Elapsed: 0:01:47.\n",
      "  Batch   320  of  1,376.    Elapsed: 0:02:02.\n",
      "  Batch   360  of  1,376.    Elapsed: 0:02:17.\n",
      "  Batch   400  of  1,376.    Elapsed: 0:02:32.\n",
      "  Batch   440  of  1,376.    Elapsed: 0:02:47.\n",
      "  Batch   480  of  1,376.    Elapsed: 0:03:03.\n",
      "  Batch   520  of  1,376.    Elapsed: 0:03:18.\n",
      "  Batch   560  of  1,376.    Elapsed: 0:03:33.\n",
      "  Batch   600  of  1,376.    Elapsed: 0:03:48.\n",
      "  Batch   640  of  1,376.    Elapsed: 0:04:04.\n",
      "  Batch   680  of  1,376.    Elapsed: 0:04:19.\n",
      "  Batch   720  of  1,376.    Elapsed: 0:04:34.\n",
      "  Batch   760  of  1,376.    Elapsed: 0:04:49.\n",
      "  Batch   800  of  1,376.    Elapsed: 0:05:04.\n",
      "  Batch   840  of  1,376.    Elapsed: 0:05:20.\n",
      "  Batch   880  of  1,376.    Elapsed: 0:05:35.\n",
      "  Batch   920  of  1,376.    Elapsed: 0:05:50.\n",
      "  Batch   960  of  1,376.    Elapsed: 0:06:05.\n",
      "  Batch 1,000  of  1,376.    Elapsed: 0:06:21.\n",
      "  Batch 1,040  of  1,376.    Elapsed: 0:06:36.\n",
      "  Batch 1,080  of  1,376.    Elapsed: 0:06:51.\n",
      "  Batch 1,120  of  1,376.    Elapsed: 0:07:06.\n",
      "  Batch 1,160  of  1,376.    Elapsed: 0:07:22.\n",
      "  Batch 1,200  of  1,376.    Elapsed: 0:07:37.\n",
      "  Batch 1,240  of  1,376.    Elapsed: 0:07:52.\n",
      "  Batch 1,280  of  1,376.    Elapsed: 0:08:07.\n",
      "  Batch 1,320  of  1,376.    Elapsed: 0:08:23.\n",
      "  Batch 1,360  of  1,376.    Elapsed: 0:08:38.\n",
      "\n",
      "  Average training loss: 1.03\n",
      "  Training epcoh took: 0:08:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,376.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  1,376.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  1,376.    Elapsed: 0:00:46.\n",
      "  Batch   160  of  1,376.    Elapsed: 0:01:01.\n",
      "  Batch   200  of  1,376.    Elapsed: 0:01:16.\n",
      "  Batch   240  of  1,376.    Elapsed: 0:01:31.\n",
      "  Batch   280  of  1,376.    Elapsed: 0:01:46.\n",
      "  Batch   320  of  1,376.    Elapsed: 0:02:02.\n",
      "  Batch   360  of  1,376.    Elapsed: 0:02:17.\n",
      "  Batch   400  of  1,376.    Elapsed: 0:02:32.\n",
      "  Batch   440  of  1,376.    Elapsed: 0:02:47.\n",
      "  Batch   480  of  1,376.    Elapsed: 0:03:02.\n",
      "  Batch   520  of  1,376.    Elapsed: 0:03:18.\n",
      "  Batch   560  of  1,376.    Elapsed: 0:03:33.\n",
      "  Batch   600  of  1,376.    Elapsed: 0:03:48.\n",
      "  Batch   640  of  1,376.    Elapsed: 0:04:03.\n",
      "  Batch   680  of  1,376.    Elapsed: 0:04:18.\n",
      "  Batch   720  of  1,376.    Elapsed: 0:04:34.\n",
      "  Batch   760  of  1,376.    Elapsed: 0:04:49.\n",
      "  Batch   800  of  1,376.    Elapsed: 0:05:04.\n",
      "  Batch   840  of  1,376.    Elapsed: 0:05:19.\n",
      "  Batch   880  of  1,376.    Elapsed: 0:05:34.\n",
      "  Batch   920  of  1,376.    Elapsed: 0:05:50.\n",
      "  Batch   960  of  1,376.    Elapsed: 0:06:05.\n",
      "  Batch 1,000  of  1,376.    Elapsed: 0:06:20.\n",
      "  Batch 1,040  of  1,376.    Elapsed: 0:06:35.\n",
      "  Batch 1,080  of  1,376.    Elapsed: 0:06:50.\n",
      "  Batch 1,120  of  1,376.    Elapsed: 0:07:05.\n",
      "  Batch 1,160  of  1,376.    Elapsed: 0:07:21.\n",
      "  Batch 1,200  of  1,376.    Elapsed: 0:07:36.\n",
      "  Batch 1,240  of  1,376.    Elapsed: 0:07:51.\n",
      "  Batch 1,280  of  1,376.    Elapsed: 0:08:06.\n",
      "  Batch 1,320  of  1,376.    Elapsed: 0:08:21.\n",
      "  Batch 1,360  of  1,376.    Elapsed: 0:08:37.\n",
      "\n",
      "  Average training loss: 0.76\n",
      "  Training epcoh took: 0:08:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.70\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,376.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  1,376.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  1,376.    Elapsed: 0:00:46.\n",
      "  Batch   160  of  1,376.    Elapsed: 0:01:01.\n",
      "  Batch   200  of  1,376.    Elapsed: 0:01:16.\n",
      "  Batch   240  of  1,376.    Elapsed: 0:01:31.\n",
      "  Batch   280  of  1,376.    Elapsed: 0:01:46.\n",
      "  Batch   320  of  1,376.    Elapsed: 0:02:02.\n",
      "  Batch   360  of  1,376.    Elapsed: 0:02:17.\n",
      "  Batch   400  of  1,376.    Elapsed: 0:02:32.\n",
      "  Batch   440  of  1,376.    Elapsed: 0:02:47.\n",
      "  Batch   480  of  1,376.    Elapsed: 0:03:02.\n",
      "  Batch   520  of  1,376.    Elapsed: 0:03:18.\n",
      "  Batch   560  of  1,376.    Elapsed: 0:03:33.\n",
      "  Batch   600  of  1,376.    Elapsed: 0:03:48.\n",
      "  Batch   640  of  1,376.    Elapsed: 0:04:03.\n",
      "  Batch   680  of  1,376.    Elapsed: 0:04:18.\n",
      "  Batch   720  of  1,376.    Elapsed: 0:04:33.\n",
      "  Batch   760  of  1,376.    Elapsed: 0:04:49.\n",
      "  Batch   800  of  1,376.    Elapsed: 0:05:04.\n",
      "  Batch   840  of  1,376.    Elapsed: 0:05:19.\n",
      "  Batch   880  of  1,376.    Elapsed: 0:05:34.\n",
      "  Batch   920  of  1,376.    Elapsed: 0:05:49.\n",
      "  Batch   960  of  1,376.    Elapsed: 0:06:05.\n",
      "  Batch 1,000  of  1,376.    Elapsed: 0:06:20.\n",
      "  Batch 1,040  of  1,376.    Elapsed: 0:06:35.\n",
      "  Batch 1,080  of  1,376.    Elapsed: 0:06:50.\n",
      "  Batch 1,120  of  1,376.    Elapsed: 0:07:05.\n",
      "  Batch 1,160  of  1,376.    Elapsed: 0:07:21.\n",
      "  Batch 1,200  of  1,376.    Elapsed: 0:07:36.\n",
      "  Batch 1,240  of  1,376.    Elapsed: 0:07:51.\n",
      "  Batch 1,280  of  1,376.    Elapsed: 0:08:06.\n",
      "  Batch 1,320  of  1,376.    Elapsed: 0:08:21.\n",
      "  Batch 1,360  of  1,376.    Elapsed: 0:08:36.\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:08:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,376.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  1,376.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  1,376.    Elapsed: 0:00:46.\n",
      "  Batch   160  of  1,376.    Elapsed: 0:01:01.\n",
      "  Batch   200  of  1,376.    Elapsed: 0:01:16.\n",
      "  Batch   240  of  1,376.    Elapsed: 0:01:31.\n",
      "  Batch   280  of  1,376.    Elapsed: 0:01:46.\n",
      "  Batch   320  of  1,376.    Elapsed: 0:02:01.\n",
      "  Batch   360  of  1,376.    Elapsed: 0:02:17.\n",
      "  Batch   400  of  1,376.    Elapsed: 0:02:32.\n",
      "  Batch   440  of  1,376.    Elapsed: 0:02:47.\n",
      "  Batch   480  of  1,376.    Elapsed: 0:03:02.\n",
      "  Batch   520  of  1,376.    Elapsed: 0:03:17.\n",
      "  Batch   560  of  1,376.    Elapsed: 0:03:33.\n",
      "  Batch   600  of  1,376.    Elapsed: 0:03:48.\n",
      "  Batch   640  of  1,376.    Elapsed: 0:04:03.\n",
      "  Batch   680  of  1,376.    Elapsed: 0:04:18.\n",
      "  Batch   720  of  1,376.    Elapsed: 0:04:33.\n",
      "  Batch   760  of  1,376.    Elapsed: 0:04:48.\n",
      "  Batch   800  of  1,376.    Elapsed: 0:05:04.\n",
      "  Batch   840  of  1,376.    Elapsed: 0:05:19.\n",
      "  Batch   880  of  1,376.    Elapsed: 0:05:34.\n",
      "  Batch   920  of  1,376.    Elapsed: 0:05:49.\n",
      "  Batch   960  of  1,376.    Elapsed: 0:06:04.\n",
      "  Batch 1,000  of  1,376.    Elapsed: 0:06:20.\n",
      "  Batch 1,040  of  1,376.    Elapsed: 0:06:35.\n",
      "  Batch 1,080  of  1,376.    Elapsed: 0:06:50.\n",
      "  Batch 1,120  of  1,376.    Elapsed: 0:07:05.\n",
      "  Batch 1,160  of  1,376.    Elapsed: 0:07:20.\n",
      "  Batch 1,200  of  1,376.    Elapsed: 0:07:36.\n",
      "  Batch 1,240  of  1,376.    Elapsed: 0:07:51.\n",
      "  Batch 1,280  of  1,376.    Elapsed: 0:08:06.\n",
      "  Batch 1,320  of  1,376.    Elapsed: 0:08:21.\n",
      "  Batch 1,360  of  1,376.    Elapsed: 0:08:36.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 0:08:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "Training complete!\n",
      " \n",
      "Here is the training loss curve of BERT.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGRwBjKiYiIiIgE9n8th+HWFsWG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"3, BERT\")\n",
    "print(\"Here is the result of BERT.\")\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "batch_size = 8\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=16)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}]\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "loss_values = []\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():   \n",
    "            outputs = model(b_input_ids,      \n",
    "            token_type_ids=None, \n",
    "            attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\" \")\n",
    "print(\"Here is the training loss curve of BERT.\")\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.plot(loss_values, 'b-o')\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "u_BrsNwO41lO",
    "outputId": "f3400500-742d-4b52-b165-9ed413d43ca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, Matthews Correlation Coefficient\n",
      "From the models above, it is obvious that BERT has better performance than LSTM. What is its Matthews Correlation Coefficient?\n",
      "Predicting labels for 1,224 test sentences in the BERT model.\n",
      "  DONE.\n",
      "MCC: 0.559\n"
     ]
    }
   ],
   "source": [
    "print('4, Matthews Correlation Coefficient')\n",
    "print(\"From the models above, it is obvious that BERT has better performance than LSTM. What is its Matthews Correlation Coefficient?\")\n",
    "prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "print('Predicting labels for {:,} test sentences in the BERT model.'.format(len(validation_inputs)))\n",
    "model.eval()\n",
    "predictions , true_labels = [], [] \n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "print('  DONE.')\n",
    "matthews_set = []\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten() \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "QftbjHxHD5iz",
    "outputId": "464e9fe6-9ac8-491a-f2b6-35a8275a7935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../model_save/vocab.txt',\n",
       " '../model_save/special_tokens_map.json',\n",
       " '../model_save/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to save the model.\n",
    "output_dir = '../model_save/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FajaWdPj_vGK"
   },
   "source": [
    "## Embeddings, Context Words\n",
    "\n",
    "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-Jw-_4p_vGM"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ugc12hUb_vGP"
   },
   "source": [
    "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ma-KkSDM_vGP"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dOgVHAQ3_vGR",
    "outputId": "1a2c7f32-431c-4501-c424-8c624dc28f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGsg1bjs_vGV"
   },
   "source": [
    "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
    "A quick peek at what the voabulary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "IclhPAFr_vGX",
    "outputId": "a17ee21e-815d-414c-dfef-641ac7435bc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peninsula',\n",
       " 'adults',\n",
       " 'novels',\n",
       " 'emerged',\n",
       " 'vienna',\n",
       " 'metro',\n",
       " 'debuted',\n",
       " 'shoes',\n",
       " 'tamil',\n",
       " 'songwriter',\n",
       " 'meets',\n",
       " 'prove',\n",
       " 'beating',\n",
       " 'instance',\n",
       " 'heaven',\n",
       " 'scared',\n",
       " 'sending',\n",
       " 'marks',\n",
       " 'artistic',\n",
       " 'passage',\n",
       " 'superior',\n",
       " '03',\n",
       " 'significantly',\n",
       " 'shopping',\n",
       " '##tive',\n",
       " 'retained',\n",
       " '##izing',\n",
       " 'malaysia',\n",
       " 'technique',\n",
       " 'cheeks']"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[6000:6030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "opr8NEF2_vGZ",
    "outputId": "2e95ed2d-690b-43ce-a655-475da0773ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtqpfCet_vGb"
   },
   "source": [
    "### Segment ID\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in â€œtokenized_text,â€ we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the â€˜[SEP]â€™ token a 0, and all tokens of the second sentence a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CpJB-VAH_vGc",
    "outputId": "79972929-09a5-4f1e-e31e-70304d00f201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfAdn5CU_vGf"
   },
   "source": [
    "Like we did for classification, we now convert these segments to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtPZOZTw_vGg"
   },
   "source": [
    "The embedding layer is the hidden state layer, and this is what we pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2KiY5yu_vGg"
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8g8nge_Y_vGi",
    "outputId": "7d9c9e5b-7ef6-477d-c7ce-6d1798eadc31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_embedding.cuda()\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Tzdf__H_vGk"
   },
   "outputs": [],
   "source": [
    "output = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiwtvIHb_vGn"
   },
   "outputs": [],
   "source": [
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkG8wT5G_vGs"
   },
   "source": [
    "### Understanding the Output\n",
    "\n",
    "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
    "\n",
    "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4PW-zZ4_vGs"
   },
   "outputs": [],
   "source": [
    "word_embeddings, sentence_embedding = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvNZLaxi_vGu"
   },
   "outputs": [],
   "source": [
    "len(word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdzdUFYb_vGv"
   },
   "outputs": [],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_LkawA2_vGy"
   },
   "source": [
    "Letâ€™s take a quick look at the range of values for a given layer and token.\n",
    "\n",
    "Youâ€™ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcIYLjvu_vGy"
   },
   "outputs": [],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfVl53Rg_vG0"
   },
   "source": [
    "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUyZz8Tu_vG0"
   },
   "source": [
    "### Word Vectors\n",
    "\n",
    "So each of those tokens have embedding values - let us try and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awIJE1LG_vG1"
   },
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IRmzczW_vG3"
   },
   "outputs": [],
   "source": [
    "token_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7MN8J0L_vG8"
   },
   "source": [
    "Another method to create the vectors is to sum the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWG0igSV_vG-"
   },
   "source": [
    "### Sentence Vector\n",
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJgQ5D6C_vG_"
   },
   "outputs": [],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ev7RBEW_vHD"
   },
   "outputs": [],
   "source": [
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VCFlvu__vHI"
   },
   "outputs": [],
   "source": [
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BtUFw9S_vHR"
   },
   "source": [
    "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Bv9ghz3_vHS"
   },
   "outputs": [],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjsHuTI7_vHW"
   },
   "outputs": [],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7mzD-Jl_vHa"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_vlithG_vHe"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUu4SLmg_vHj"
   },
   "source": [
    "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33i_2N8p_vHk"
   },
   "outputs": [],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlS-BZDO_vHm"
   },
   "source": [
    "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IPYNMwFF_vHm"
   },
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jk5KLjmw_vHp"
   },
   "outputs": [],
   "source": [
    "word_10 = word_vector(text, 6, model)\n",
    "word_6 = word_vector(text, 10, model)\n",
    "word_19 = word_vector(text, 19, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4ZqsvCA_vHr"
   },
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLkoa0wY_vHv"
   },
   "outputs": [],
   "source": [
    "sen_vec_0 = sentence_vector(text, model)\n",
    "sen_vec_1 = sentence_vector(text, model, method=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0frBHJW_vHw"
   },
   "source": [
    "### Similarity metrics\n",
    "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river â€œbankâ€ and not a financial institution â€œbankâ€. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
    "\n",
    "### Using the Vectors\n",
    "\n",
    "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
    "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
    "\n",
    "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. Consider the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtljqUV5_vHx"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "42f9a4d3a0644586aa9a2300059b8b5f",
      "3f8be95358e742189ec05e713415d8bb",
      "2d04f9e061994fb1a25baa976bf7c000",
      "864d218ed3894b8f9d178fb6d91d57ce",
      "eeac5bb82c744e728b10fe7a1b461b3a",
      "32c5cc026037428da5a352cdd7b18ae3",
      "c4cfab5a2ec042d8a8b3a1d72bc60c4e",
      "0c8ac724bedf46ffbad468fcb7a62854",
      "4600154a77644e64a9b696920e76a798",
      "6c8e1a0cba1e49c29aeb80e8cd9f51d8",
      "b797454c7f0b4dfe9972afad19682e9d",
      "dd22f69726974e8692b388a3ead4481c",
      "8f5b4ac2eea540769fcf2462f2650186",
      "b088a650a0474d888ff52d1788de765c",
      "9c5d8864d6c142f484dc995faabbcaa4",
      "ba13ab99460f4dd8ac2d35059e890718",
      "b20275d471674a2fad60015b59eaee8b",
      "79ebefb8cd7247d8b2f70c163723f817",
      "15f69272b659412fb72b3fbbf2de3481",
      "81f0dc6a296c4dac97414eefbad06679",
      "20ab167e39954d12892198819771ecf0",
      "0459bebe18284628b8cb69153caa2ad9",
      "9a6f3dc00aca4b0fb527191f2407cd3f",
      "52f34b5a72794d2fa14a71405a36edf5",
      "2c53acb46651407aaa27ea7618ce4dfd",
      "e762d375038844ceb64ec475ab72810e",
      "0267db5236654a2cbab5f6d0a15055fc",
      "fc7d08ca0dda4e3abd5cc5ed64ff4aa8",
      "bcae8e6ce22b48a18c1ecad412a7ea80",
      "75eea0a1cb604f85aad6950b4cfe643e",
      "9ea22147ff7841c3b4e7ca3d44ed1e97",
      "23147d17f077408f84782289fea9d2d1"
     ]
    },
    "colab_type": "code",
    "id": "4RdcKZmZ_vHz",
    "outputId": "57d17b4a-4ad8-4d96-9987-948c167ae5d1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f9a4d3a0644586aa9a2300059b8b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=546, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4600154a77644e64a9b696920e76a798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=754, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20275d471674a2fad60015b59eaee8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c53acb46651407aaa27ea7618ce4dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=267844284, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fapaZh0M_vH0",
    "outputId": "da7c0265-c4c1-47fd-882c-ad5f8fe79425"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997735}]"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncaUqJtz_vH2"
   },
   "source": [
    "We get a strong positive sentiment, which we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UmaFe1BQ_vH3",
    "outputId": "dcb8a4b7-2db2-48da-d3dd-0d0adb5d56e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997195}]"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6aQP6SB_vH7"
   },
   "source": [
    "Negative label, bingo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264,
     "referenced_widgets": [
      "c281ba50ecf445058cc8f669c949ce52",
      "4d78a3d10ada4b7f817bc8ec8afcfa31",
      "4dbb3164b60949f89945db07f24b88c6",
      "778bbca832614cc88c611191a2d5fbff",
      "ea9ac97b52ab4b7a8829625a44436d9f",
      "2d148fd728ce46e79b36d378b1fd36da",
      "e4c89670ac2248348acdb8250660de80",
      "2db97fa8ea044d7397bfc87a36629403",
      "549ea68b85ba4235b5bcece65a99029e",
      "d49515c60cf847319a766c6ea71757d4",
      "63967fe3a91e4834996504ed6a3dbab5",
      "c4cbcbcf9b2d487b95e493243912cfed",
      "92bc2dd05ba24674b3bc505380775244",
      "ce75547630244baabe6837ab3f5f4c54",
      "484daf2c6f5540d9b8fdd65129d97ea5",
      "c533512948e24ecbb61011528ec5eb55",
      "c43e481e306a46c08639e3cdf903b758",
      "25b9f6330a4d4fa4b275b7c1e0d2d678",
      "06b046743c9c4677bc50476642edf86b",
      "1d474aaa9fcc4958906303906618f82d",
      "189879bb380e4174babb002788421dcc",
      "7754dc2db14f46819ae717b97d258a8b",
      "fa1e59f9356944169597cc80b15ceb34",
      "d071e6f1dbb84a42b3734f8453b4d5f2",
      "4256cedc972d4e34b118630cce1d3a94",
      "b94d9faee7c64a4f8d00e7544746a392",
      "6562a944aeef4f86bcc9c3c5ebf9512a",
      "277e4e17087b42c48277645acf81d7c2",
      "a82f1b1be1644d73891f311d0a1eb8eb",
      "44f31b755da44bccad5d98afffc6f9aa",
      "74fa92af960941d58ffa5372a43d7d0f",
      "b5845154f5c84d47966f9818145e995d",
      "0400d7b7a7de4f59a75cdcafb641788f",
      "cd2d2f02d6e14ec7bcbae92b5ab41ae2",
      "798c179ba832479abd0865f4122248ad",
      "8717d409a1d34953ab3c933f39a98a8a",
      "f9a268fa34bd420db35632a38e010bd5",
      "600b0c9be1084a829610ec52c5b89b44",
      "ecc1252ff4bf478aa8358e5172eff480",
      "04eedd44f1784e44b967e3e21da0cc2b"
     ]
    },
    "colab_type": "code",
    "id": "HMkLdZzi_vH7",
    "outputId": "b892c26e-eb66-4947-dd65-ba35c046fd61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c281ba50ecf445058cc8f669c949ce52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=939, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549ea68b85ba4235b5bcece65a99029e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e481e306a46c08639e3cdf903b758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4256cedc972d4e34b118630cce1d3a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1031, style=ProgressStyle(description_widthâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400d7b7a7de4f59a75cdcafb641788f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=260793700, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "nlp_question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "PRV4nUM2_vH_",
    "outputId": "753a9fcc-f260-42b7-b660-b9d424cefd7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 4766.25it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 5652.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'analysing complex textual data',\n",
       " 'end': 64,\n",
       " 'score': 0.986002966547499,\n",
       " 'start': 34}"
      ]
     },
     "execution_count": 161,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "    'question': 'What is my favorite thing to do on weekends ?',\n",
    "    'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sarRPP2a_vIA"
   },
   "source": [
    "It's also great at question-answering tasks!\n",
    "We can also extract features, as we manually did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "38d800a371544d7d914870a22a971791",
      "367489dd91ed4fcdb38b8f9e38392f5b",
      "573846cb97fe4b019f0e430c63e11624",
      "ccd661cec496474ebdfefa7e1f4a8926",
      "a02b997d8bd24a55ac5dd69c931e8e2d",
      "3edc98e5e74f4f23a4e886d7caa48c00",
      "168914421c44490ba1d9ebef4dd2285b",
      "a5c6c1f941914d2f9772e1a5016da6f0",
      "d1482d85804847ada9a43c0311f823cc",
      "30c428ba615a470ab6405498d5b47e26",
      "937686845ad344beac6752c4064ce331",
      "110aa51584474035b9885ddb15006d46",
      "37e659127e874855b294e29f3232c697",
      "b1eb6ce27e0743e99832105cca5f57e0",
      "dcd9948b90884364aaa0302020454586",
      "8ca99b53bae64f0f8826568b6458129c"
     ]
    },
    "colab_type": "code",
    "id": "rDrLg1-L_vIB",
    "outputId": "c37236b2-b741-498c-b600-4d2619e8283b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d800a371544d7d914870a22a971791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1482d85804847ada9a43c0311f823cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=263273408, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nqn5ld5P_vIC"
   },
   "outputs": [],
   "source": [
    "vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BQOWCMZl_vIE",
    "outputId": "5a07461b-0dbc-4330-a39b-cf096551d321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 164,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "14x3hIMq_vIG",
    "outputId": "215f0e92-8bf6-4cdb-edd8-1741ccd6f570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.28079766035079956,\n",
       "   0.06644467264413834,\n",
       "   -0.1676395982503891,\n",
       "   -0.16896063089370728,\n",
       "   -0.22660796344280243,\n",
       "   -0.14127105474472046,\n",
       "   0.4210294783115387,\n",
       "   -0.09199243038892746,\n",
       "   -0.03190780431032181,\n",
       "   -0.8537288308143616,\n",
       "   -0.26838091015815735,\n",
       "   0.08569961786270142,\n",
       "   -0.13404342532157898,\n",
       "   -0.0349603109061718,\n",
       "   -0.5356059670448303,\n",
       "   -0.029006753116846085,\n",
       "   0.20110894739627838,\n",
       "   0.08040326833724976,\n",
       "   -0.08755526691675186,\n",
       "   -0.08443038165569305,\n",
       "   0.053638435900211334,\n",
       "   -0.18426790833473206,\n",
       "   0.5852557420730591,\n",
       "   -0.1716005802154541,\n",
       "   0.051204681396484375,\n",
       "   0.123198501765728,\n",
       "   0.37301215529441833,\n",
       "   0.1615198403596878,\n",
       "   -0.17832453548908234,\n",
       "   0.5117568969726562,\n",
       "   -0.020196231082081795,\n",
       "   0.19047752022743225,\n",
       "   0.06237805262207985,\n",
       "   0.057217076420784,\n",
       "   -0.3619835078716278,\n",
       "   0.22840620577335358,\n",
       "   -0.18473580479621887,\n",
       "   -0.2177051603794098,\n",
       "   -0.07018119841814041,\n",
       "   -0.09863680601119995,\n",
       "   -0.5033520460128784,\n",
       "   0.11645977199077606,\n",
       "   0.5387324094772339,\n",
       "   -0.1830195039510727,\n",
       "   0.04567167907953262,\n",
       "   -0.48051828145980835,\n",
       "   0.07694832980632782,\n",
       "   0.039038240909576416,\n",
       "   -0.08155319094657898,\n",
       "   0.13631248474121094,\n",
       "   -0.0981353148818016,\n",
       "   0.09356576949357986,\n",
       "   -0.1931370347738266,\n",
       "   -0.04096975550055504,\n",
       "   0.15128158032894135,\n",
       "   0.08282732218503952,\n",
       "   -0.0629233717918396,\n",
       "   0.1353553980588913,\n",
       "   -0.557519257068634,\n",
       "   0.2614964246749878,\n",
       "   -0.07125771790742874,\n",
       "   0.03385477140545845,\n",
       "   0.33907976746559143,\n",
       "   0.0656089186668396,\n",
       "   -0.2229633927345276,\n",
       "   0.07824652642011642,\n",
       "   0.04569881781935692,\n",
       "   0.21588370203971863,\n",
       "   -0.19988054037094116,\n",
       "   -0.21166890859603882,\n",
       "   0.09120077639818192,\n",
       "   0.22718805074691772,\n",
       "   0.3030434846878052,\n",
       "   0.8904082775115967,\n",
       "   0.2298191338777542,\n",
       "   -0.21156863868236542,\n",
       "   0.33373427391052246,\n",
       "   -0.04453245550394058,\n",
       "   -0.0271195936948061,\n",
       "   -0.09593840688467026,\n",
       "   0.0577363520860672,\n",
       "   0.199104443192482,\n",
       "   -0.11624055355787277,\n",
       "   -0.22660037875175476,\n",
       "   -0.028790108859539032,\n",
       "   -0.13013114035129547,\n",
       "   0.10612837225198746,\n",
       "   -0.11570274829864502,\n",
       "   -0.08082135021686554,\n",
       "   0.15435479581356049,\n",
       "   0.16014045476913452,\n",
       "   -0.15008296072483063,\n",
       "   -0.24606406688690186,\n",
       "   0.04691636189818382,\n",
       "   -0.20780405402183533,\n",
       "   0.10484106093645096,\n",
       "   -0.04270917549729347,\n",
       "   0.0752374529838562,\n",
       "   6.085385322570801,\n",
       "   -0.061667975038290024,\n",
       "   -0.23195837438106537,\n",
       "   0.06998208165168762,\n",
       "   0.09388417750597,\n",
       "   -0.11683948338031769,\n",
       "   0.24049437046051025,\n",
       "   -0.3288649022579193,\n",
       "   -0.03494852036237717,\n",
       "   -0.4698139727115631,\n",
       "   0.03441007062792778,\n",
       "   0.4979923367500305,\n",
       "   0.3973345160484314,\n",
       "   0.08457006514072418,\n",
       "   0.12022974342107773,\n",
       "   -0.10584201663732529,\n",
       "   -0.09611059725284576,\n",
       "   -0.32829317450523376,\n",
       "   0.0967198833823204,\n",
       "   0.049704261124134064,\n",
       "   0.0741318091750145,\n",
       "   -0.21588212251663208,\n",
       "   0.15652692317962646,\n",
       "   0.005386210046708584,\n",
       "   0.9506387114524841,\n",
       "   0.10336972028017044,\n",
       "   -0.11216221004724503,\n",
       "   -0.001105794683098793,\n",
       "   -0.0011706794612109661,\n",
       "   -0.2742650508880615,\n",
       "   0.10748116672039032,\n",
       "   -0.1626313179731369,\n",
       "   -0.4893866181373596,\n",
       "   -0.22806045413017273,\n",
       "   -0.07158318161964417,\n",
       "   -0.1492321491241455,\n",
       "   0.10891911387443542,\n",
       "   0.00809627864509821,\n",
       "   -0.04215279966592789,\n",
       "   -0.036262091249227524,\n",
       "   -0.8590205907821655,\n",
       "   0.11949528753757477,\n",
       "   0.09703503549098969,\n",
       "   -0.09249464422464371,\n",
       "   0.09470713138580322,\n",
       "   -0.162554070353508,\n",
       "   -0.2843359112739563,\n",
       "   2.624415397644043,\n",
       "   -0.12164159119129181,\n",
       "   0.03022586554288864,\n",
       "   -0.1260499209165573,\n",
       "   0.05357092618942261,\n",
       "   -0.08468525111675262,\n",
       "   -0.1874813735485077,\n",
       "   -0.24895015358924866,\n",
       "   0.2305164933204651,\n",
       "   -0.25181275606155396,\n",
       "   -0.21527059376239777,\n",
       "   0.18334023654460907,\n",
       "   0.216752827167511,\n",
       "   0.05431952700018883,\n",
       "   0.09860686212778091,\n",
       "   -1.0202139616012573,\n",
       "   -0.00730120437219739,\n",
       "   -0.2283599078655243,\n",
       "   0.23675420880317688,\n",
       "   0.13464252650737762,\n",
       "   -0.2398895025253296,\n",
       "   -0.0015054390532895923,\n",
       "   -0.6380901336669922,\n",
       "   0.023236287757754326,\n",
       "   0.36025702953338623,\n",
       "   0.04916900768876076,\n",
       "   0.031636666506528854,\n",
       "   -2.1947357654571533,\n",
       "   0.3170285224914551,\n",
       "   0.13879603147506714,\n",
       "   0.1367831528186798,\n",
       "   0.00339842913672328,\n",
       "   0.13875330984592438,\n",
       "   0.0050305151380598545,\n",
       "   -0.2804129421710968,\n",
       "   -0.1710406392812729,\n",
       "   0.38404712080955505,\n",
       "   0.13680855929851532,\n",
       "   0.0855024978518486,\n",
       "   -0.4032360017299652,\n",
       "   0.015949618071317673,\n",
       "   0.12988649308681488,\n",
       "   -0.19909200072288513,\n",
       "   -0.039330389350652695,\n",
       "   0.007921336218714714,\n",
       "   -0.13287611305713654,\n",
       "   -0.33360233902931213,\n",
       "   -0.12062506377696991,\n",
       "   0.1958886981010437,\n",
       "   0.19319644570350647,\n",
       "   0.13310931622982025,\n",
       "   0.05858061462640762,\n",
       "   -0.090957410633564,\n",
       "   -0.06136384606361389,\n",
       "   0.170671746134758,\n",
       "   -0.18688489496707916,\n",
       "   -0.12096941471099854,\n",
       "   -0.14287254214286804,\n",
       "   0.14427267014980316,\n",
       "   0.36059895157814026,\n",
       "   0.1913674920797348,\n",
       "   -0.03786715492606163,\n",
       "   0.11734570562839508,\n",
       "   0.17427411675453186,\n",
       "   -0.06699509918689728,\n",
       "   -0.19577905535697937,\n",
       "   0.025702418759465218,\n",
       "   0.08491046726703644,\n",
       "   0.4213600158691406,\n",
       "   0.5814645290374756,\n",
       "   -0.39639267325401306,\n",
       "   -0.033143918961286545,\n",
       "   -0.08839762210845947,\n",
       "   0.09495900571346283,\n",
       "   0.046528201550245285,\n",
       "   -0.10590098053216934,\n",
       "   0.12356533855199814,\n",
       "   -0.17063236236572266,\n",
       "   -0.11698146909475327,\n",
       "   -0.23107419908046722,\n",
       "   0.05537489801645279,\n",
       "   0.29407262802124023,\n",
       "   0.2025465965270996,\n",
       "   0.011806037276983261,\n",
       "   -0.33295172452926636,\n",
       "   0.2028828263282776,\n",
       "   0.1849336326122284,\n",
       "   0.09737394750118256,\n",
       "   -0.29335346817970276,\n",
       "   -0.01471332460641861,\n",
       "   -0.03792145475745201,\n",
       "   -0.01711197756230831,\n",
       "   0.08761616051197052,\n",
       "   -0.03081195056438446,\n",
       "   -0.08504310250282288,\n",
       "   0.07453300058841705,\n",
       "   -0.001054207095876336,\n",
       "   0.32112956047058105,\n",
       "   0.024368038401007652,\n",
       "   0.007990234531462193,\n",
       "   0.06423601508140564,\n",
       "   0.3407353460788727,\n",
       "   0.29837697744369507,\n",
       "   0.0728345662355423,\n",
       "   -0.023525889962911606,\n",
       "   0.6100217700004578,\n",
       "   0.10052042454481125,\n",
       "   -0.18003582954406738,\n",
       "   -0.29149046540260315,\n",
       "   -0.534946858882904,\n",
       "   -0.14874963462352753,\n",
       "   -0.043226126581430435,\n",
       "   -1.198394775390625,\n",
       "   -0.11274407804012299,\n",
       "   -0.3336608409881592,\n",
       "   0.27879226207733154,\n",
       "   -2.934849739074707,\n",
       "   0.024574005976319313,\n",
       "   0.24009498953819275,\n",
       "   -0.0015403565485030413,\n",
       "   -0.10958191007375717,\n",
       "   0.2914097309112549,\n",
       "   0.09774932265281677,\n",
       "   -0.45511960983276367,\n",
       "   0.23169538378715515,\n",
       "   -0.46620824933052063,\n",
       "   0.021126780658960342,\n",
       "   -0.6199565529823303,\n",
       "   0.06250546872615814,\n",
       "   -0.12774658203125,\n",
       "   0.25704070925712585,\n",
       "   -0.062075644731521606,\n",
       "   -0.05455448850989342,\n",
       "   -0.23015941679477692,\n",
       "   -0.21192161738872528,\n",
       "   -0.1458439826965332,\n",
       "   0.05948420241475105,\n",
       "   -0.31239086389541626,\n",
       "   0.4359772205352783,\n",
       "   -0.27800852060317993,\n",
       "   0.3652208745479584,\n",
       "   0.34398332238197327,\n",
       "   0.19921986758708954,\n",
       "   0.10600966960191727,\n",
       "   3.681918144226074,\n",
       "   0.27655014395713806,\n",
       "   -0.15455572307109833,\n",
       "   0.09237849712371826,\n",
       "   -0.3022402822971344,\n",
       "   0.013119790703058243,\n",
       "   0.04309051111340523,\n",
       "   -0.45908620953559875,\n",
       "   0.03074515052139759,\n",
       "   -0.07204131036996841,\n",
       "   -0.08520232141017914,\n",
       "   -0.23873867094516754,\n",
       "   0.05929216742515564,\n",
       "   -0.4444529414176941,\n",
       "   -0.05480555072426796,\n",
       "   -0.4397672414779663,\n",
       "   0.04569718614220619,\n",
       "   0.03808160871267319,\n",
       "   0.10586254298686981,\n",
       "   -0.6242753863334656,\n",
       "   -0.11423888802528381,\n",
       "   0.0795188918709755,\n",
       "   0.0864611268043518,\n",
       "   -0.04021641984581947,\n",
       "   0.06479839235544205,\n",
       "   -0.0052055795677006245,\n",
       "   -0.2825013995170593,\n",
       "   -0.3815920650959015,\n",
       "   0.27384087443351746,\n",
       "   0.016155347228050232,\n",
       "   -0.7521321177482605,\n",
       "   0.2528271973133087,\n",
       "   0.17298519611358643,\n",
       "   -0.30314338207244873,\n",
       "   -0.05463718995451927,\n",
       "   0.0665101632475853,\n",
       "   -0.06294263899326324,\n",
       "   -0.04816797748208046,\n",
       "   -0.17646139860153198,\n",
       "   -0.23466050624847412,\n",
       "   -0.013834759593009949,\n",
       "   -0.16199547052383423,\n",
       "   0.0945359617471695,\n",
       "   -0.30075687170028687,\n",
       "   -0.17989668250083923,\n",
       "   -0.19597981870174408,\n",
       "   -0.1419202983379364,\n",
       "   -0.10136280208826065,\n",
       "   0.26560550928115845,\n",
       "   -0.21154838800430298,\n",
       "   0.03069956786930561,\n",
       "   -0.19061225652694702,\n",
       "   0.044668179005384445,\n",
       "   -0.16349683701992035,\n",
       "   -0.008528340607881546,\n",
       "   0.2678465247154236,\n",
       "   -0.07709266990423203,\n",
       "   0.3252994120121002,\n",
       "   -0.05382275581359863,\n",
       "   0.07142588496208191,\n",
       "   0.18285955488681793,\n",
       "   -0.028945907950401306,\n",
       "   0.09669452905654907,\n",
       "   0.11392459273338318,\n",
       "   0.10665500909090042,\n",
       "   -0.042286574840545654,\n",
       "   0.05124242603778839,\n",
       "   -0.08662646263837814,\n",
       "   -0.24653717875480652,\n",
       "   -0.014914401806890965,\n",
       "   0.14906875789165497,\n",
       "   -0.06340393424034119,\n",
       "   -1.8984973430633545,\n",
       "   0.17997215688228607,\n",
       "   0.0514875128865242,\n",
       "   -0.27428263425827026,\n",
       "   -0.022634921595454216,\n",
       "   -0.12399691343307495,\n",
       "   0.014951593242585659,\n",
       "   -0.031997472047805786,\n",
       "   0.3755050003528595,\n",
       "   0.5077052712440491,\n",
       "   0.0516841784119606,\n",
       "   0.3939165472984314,\n",
       "   -0.017290297895669937,\n",
       "   -0.3436947464942932,\n",
       "   0.12561126053333282,\n",
       "   0.10647275298833847,\n",
       "   0.5563498139381409,\n",
       "   0.16544215381145477,\n",
       "   -0.4380694031715393,\n",
       "   -0.2936636209487915,\n",
       "   0.13457031548023224,\n",
       "   0.4124670624732971,\n",
       "   -0.10533396899700165,\n",
       "   0.1439579725265503,\n",
       "   0.03666166588664055,\n",
       "   0.08479946106672287,\n",
       "   -0.3691881000995636,\n",
       "   0.01273003313690424,\n",
       "   -0.16313418745994568,\n",
       "   0.4313427805900574,\n",
       "   -0.15211859345436096,\n",
       "   0.029717644676566124,\n",
       "   -0.1978996843099594,\n",
       "   -0.1157858818769455,\n",
       "   -0.10289077460765839,\n",
       "   -0.11375582218170166,\n",
       "   0.160227969288826,\n",
       "   0.3630657196044922,\n",
       "   -0.3239903748035431,\n",
       "   -0.06578417867422104,\n",
       "   0.007671445608139038,\n",
       "   0.0018181904451921582,\n",
       "   0.13394422829151154,\n",
       "   -0.1764814555644989,\n",
       "   -0.2676338851451874,\n",
       "   0.15191711485385895,\n",
       "   -0.10827624797821045,\n",
       "   -1.3623865842819214,\n",
       "   -0.07092969119548798,\n",
       "   -0.18885068595409393,\n",
       "   -0.16689462959766388,\n",
       "   -0.14233064651489258,\n",
       "   0.14373226463794708,\n",
       "   0.20127639174461365,\n",
       "   -0.05581733211874962,\n",
       "   0.03404304385185242,\n",
       "   -0.12135680764913559,\n",
       "   0.31329843401908875,\n",
       "   0.32362455129623413,\n",
       "   0.3016144633293152,\n",
       "   0.07194515317678452,\n",
       "   -0.1475873589515686,\n",
       "   -0.35291188955307007,\n",
       "   -0.12904033064842224,\n",
       "   0.22641125321388245,\n",
       "   0.12182222306728363,\n",
       "   -0.2276514321565628,\n",
       "   -0.04301605373620987,\n",
       "   0.14558760821819305,\n",
       "   -0.5860746502876282,\n",
       "   -0.17569606006145477,\n",
       "   -0.010849635116755962,\n",
       "   -0.28004276752471924,\n",
       "   -0.32879337668418884,\n",
       "   -0.19692271947860718,\n",
       "   0.047642048448324203,\n",
       "   -0.11027007550001144,\n",
       "   0.0810319185256958,\n",
       "   4.825362682342529,\n",
       "   -0.3243462145328522,\n",
       "   0.10100094974040985,\n",
       "   -0.059740494936704636,\n",
       "   -0.2943131923675537,\n",
       "   -0.12848255038261414,\n",
       "   0.1781742125749588,\n",
       "   0.03799843788146973,\n",
       "   0.7144269943237305,\n",
       "   0.06037357077002525,\n",
       "   -0.24456264078617096,\n",
       "   -0.24537281692028046,\n",
       "   0.04036707431077957,\n",
       "   0.053284887224435806,\n",
       "   -0.4414789378643036,\n",
       "   0.4826921224594116,\n",
       "   0.028515681624412537,\n",
       "   -0.1197669506072998,\n",
       "   0.03353562951087952,\n",
       "   -0.12164315581321716,\n",
       "   -0.04954226315021515,\n",
       "   -0.0195802990347147,\n",
       "   0.2078152447938919,\n",
       "   -0.182301864027977,\n",
       "   0.12229619175195694,\n",
       "   0.025664091110229492,\n",
       "   0.08802041411399841,\n",
       "   0.0981065183877945,\n",
       "   -0.17344939708709717,\n",
       "   -0.25087007880210876,\n",
       "   -0.30231139063835144,\n",
       "   0.1517794132232666,\n",
       "   0.22892193496227264,\n",
       "   0.24345555901527405,\n",
       "   0.10894370824098587,\n",
       "   -0.1136324480175972,\n",
       "   0.0053328401409089565,\n",
       "   0.016617339104413986,\n",
       "   0.26598066091537476,\n",
       "   0.09045727550983429,\n",
       "   0.356760174036026,\n",
       "   0.1309858113527298,\n",
       "   0.9233299493789673,\n",
       "   -0.06542515009641647,\n",
       "   0.24041707813739777,\n",
       "   0.3053032457828522,\n",
       "   -0.1372370719909668,\n",
       "   -0.030320510268211365,\n",
       "   0.36350616812705994,\n",
       "   0.1491258442401886,\n",
       "   -0.22279642522335052,\n",
       "   0.2239559441804886,\n",
       "   0.13512301445007324,\n",
       "   -0.1511809527873993,\n",
       "   0.05975565314292908,\n",
       "   0.06742792576551437,\n",
       "   0.0200812928378582,\n",
       "   -0.14436721801757812,\n",
       "   -0.13655774295330048,\n",
       "   0.05709075555205345,\n",
       "   -0.029614590108394623,\n",
       "   -0.075485959649086,\n",
       "   -0.12393035739660263,\n",
       "   -0.1775447577238083,\n",
       "   0.18264071643352509,\n",
       "   -0.2513383626937866,\n",
       "   0.28174281120300293,\n",
       "   -0.1688462197780609,\n",
       "   -0.2702641785144806,\n",
       "   -0.17168937623500824,\n",
       "   -0.08322765678167343,\n",
       "   0.010901696048676968,\n",
       "   -0.5502955317497253,\n",
       "   0.2607720196247101,\n",
       "   0.2789187431335449,\n",
       "   -0.01403241977095604,\n",
       "   -0.3823215663433075,\n",
       "   0.16681969165802002,\n",
       "   -0.06155265122652054,\n",
       "   -0.22987572848796844,\n",
       "   -0.11973493546247482,\n",
       "   -0.15649090707302094,\n",
       "   -0.020268190652132034,\n",
       "   -0.3056461215019226,\n",
       "   0.051301248371601105,\n",
       "   0.1948675811290741,\n",
       "   -0.2882370054721832,\n",
       "   -0.3124910891056061,\n",
       "   -0.030300339683890343,\n",
       "   0.29838138818740845,\n",
       "   0.10992000997066498,\n",
       "   -0.23986923694610596,\n",
       "   -0.04719966650009155,\n",
       "   -0.021207403391599655,\n",
       "   0.020983614027500153,\n",
       "   0.3496756851673126,\n",
       "   -0.07224899530410767,\n",
       "   -0.11469609290361404,\n",
       "   -0.0013624572893604636,\n",
       "   -0.07586868107318878,\n",
       "   0.39446669816970825,\n",
       "   0.1848040521144867,\n",
       "   0.015380045399069786,\n",
       "   -0.0046891518868505955,\n",
       "   0.14852593839168549,\n",
       "   -0.1958693563938141,\n",
       "   -0.11002068966627121,\n",
       "   0.2183454930782318,\n",
       "   -0.03235162794589996,\n",
       "   0.11129403114318848,\n",
       "   0.2836688160896301,\n",
       "   0.10111171752214432,\n",
       "   0.2040702998638153,\n",
       "   -0.19913989305496216,\n",
       "   -0.04581537842750549,\n",
       "   0.1286441534757614,\n",
       "   -0.009162082336843014,\n",
       "   -0.32102662324905396,\n",
       "   -7.354912757873535,\n",
       "   0.2814885377883911,\n",
       "   0.0076491693034768105,\n",
       "   0.14334094524383545,\n",
       "   -0.2796061038970947,\n",
       "   -0.09527630358934402,\n",
       "   0.2902141809463501,\n",
       "   0.09965132176876068,\n",
       "   0.17775698006153107,\n",
       "   0.06827183812856674,\n",
       "   -0.29400748014450073,\n",
       "   -0.13373532891273499,\n",
       "   -1.7726633548736572,\n",
       "   0.08600641041994095,\n",
       "   -0.0731225237250328,\n",
       "   0.1307164430618286,\n",
       "   -0.12630921602249146,\n",
       "   -0.8496420979499817,\n",
       "   -0.012155606411397457,\n",
       "   0.3473060131072998,\n",
       "   -0.24470114707946777,\n",
       "   0.16649065911769867,\n",
       "   0.1710355579853058,\n",
       "   0.25135737657546997,\n",
       "   0.23906800150871277,\n",
       "   0.055686794221401215,\n",
       "   -0.028490446507930756,\n",
       "   -0.0439445786178112,\n",
       "   0.0737139955163002,\n",
       "   -0.08893968909978867,\n",
       "   0.024265073239803314,\n",
       "   -0.04721662029623985,\n",
       "   0.3958309292793274,\n",
       "   -0.06935460865497589,\n",
       "   0.0019106591353192925,\n",
       "   -0.2702769637107849,\n",
       "   -0.010834205895662308,\n",
       "   -0.1416132152080536,\n",
       "   0.20045189559459686,\n",
       "   -0.33274391293525696,\n",
       "   -0.09886781871318817,\n",
       "   -0.031035974621772766,\n",
       "   0.16586549580097198,\n",
       "   0.2994544208049774,\n",
       "   -0.36826080083847046,\n",
       "   -0.18471354246139526,\n",
       "   -0.46401065587997437,\n",
       "   -2.159057855606079,\n",
       "   0.12460532039403915,\n",
       "   0.11814321577548981,\n",
       "   0.3193499445915222,\n",
       "   0.13232389092445374,\n",
       "   0.016762033104896545,\n",
       "   -0.06380068510770798,\n",
       "   -0.15379369258880615,\n",
       "   0.1839069426059723,\n",
       "   -0.009505968540906906,\n",
       "   0.09738501906394958,\n",
       "   0.2518868148326874,\n",
       "   -0.20578747987747192,\n",
       "   -0.2148231863975525,\n",
       "   -0.016515785828232765,\n",
       "   0.5897845029830933,\n",
       "   0.0524393767118454,\n",
       "   -0.22500233352184296,\n",
       "   0.3014533519744873,\n",
       "   -0.30591723322868347,\n",
       "   -0.05621127039194107,\n",
       "   -0.19069892168045044,\n",
       "   -0.38098040223121643,\n",
       "   -0.1273062527179718,\n",
       "   -0.013144333846867085,\n",
       "   -0.11826546490192413,\n",
       "   0.2174554467201233,\n",
       "   0.24585261940956116,\n",
       "   0.20594345033168793,\n",
       "   -0.13156144320964813,\n",
       "   0.03846151381731033,\n",
       "   -0.2171790450811386,\n",
       "   -0.01995796151459217,\n",
       "   -0.3735862970352173,\n",
       "   -0.1325725018978119,\n",
       "   0.5215298533439636,\n",
       "   0.23171007633209229,\n",
       "   0.21484798192977905,\n",
       "   -0.20708879828453064,\n",
       "   -0.03268043324351311,\n",
       "   0.2988465130329132,\n",
       "   -0.06309177726507187,\n",
       "   -0.044207509607076645,\n",
       "   -0.11364055424928665,\n",
       "   0.00560729019343853,\n",
       "   0.07338634878396988,\n",
       "   -0.07835125923156738,\n",
       "   -0.1860550194978714,\n",
       "   0.1959027796983719,\n",
       "   -0.23281021416187286,\n",
       "   0.07054166495800018,\n",
       "   -0.2480519562959671,\n",
       "   -0.08754776418209076,\n",
       "   -0.18889033794403076,\n",
       "   -7.205169094959274e-05,\n",
       "   0.04395363852381706,\n",
       "   -0.3712468147277832,\n",
       "   0.0524509996175766,\n",
       "   -0.18718136847019196,\n",
       "   2.4119436740875244,\n",
       "   0.0413488931953907,\n",
       "   -0.007798351813107729,\n",
       "   -0.05522537603974342,\n",
       "   -0.03203286975622177,\n",
       "   0.258986234664917,\n",
       "   -0.026213183999061584,\n",
       "   0.2268061637878418,\n",
       "   1.4288954734802246,\n",
       "   0.09991719573736191,\n",
       "   -0.2188144475221634,\n",
       "   -0.03430629149079323,\n",
       "   -0.011797195300459862,\n",
       "   -0.06754478067159653,\n",
       "   -0.027796844020485878,\n",
       "   0.3310871422290802,\n",
       "   -0.09177825599908829,\n",
       "   0.13208715617656708,\n",
       "   0.0679222121834755,\n",
       "   -0.030220385640859604,\n",
       "   0.2578146755695343,\n",
       "   -0.11922557651996613,\n",
       "   0.20231787860393524,\n",
       "   -0.04075709730386734,\n",
       "   -0.042230986058712006,\n",
       "   -0.0688614621758461,\n",
       "   -0.29224449396133423,\n",
       "   -0.23770874738693237,\n",
       "   0.29817646741867065,\n",
       "   -0.3711106479167938,\n",
       "   0.13910309970378876,\n",
       "   -0.0005588901112787426,\n",
       "   -0.23229843378067017,\n",
       "   0.9053876996040344,\n",
       "   -0.13803423941135406,\n",
       "   0.1583109200000763,\n",
       "   0.27124202251434326,\n",
       "   -0.13938641548156738,\n",
       "   0.08495012670755386,\n",
       "   -0.4206407368183136,\n",
       "   -0.06801240891218185,\n",
       "   -0.0308045893907547,\n",
       "   -0.4409838318824768,\n",
       "   0.4754047989845276,\n",
       "   -0.07190146297216415,\n",
       "   0.0393749363720417,\n",
       "   -0.036772001534700394,\n",
       "   -0.20400381088256836,\n",
       "   0.15076126158237457,\n",
       "   -0.0846630185842514,\n",
       "   -0.003745696973055601,\n",
       "   -0.2378651350736618,\n",
       "   -0.26416730880737305,\n",
       "   0.26233211159706116,\n",
       "   -0.2391541600227356,\n",
       "   0.07545846700668335,\n",
       "   -0.5911707878112793,\n",
       "   -0.08598633855581284,\n",
       "   0.1406150609254837,\n",
       "   -0.08961525559425354,\n",
       "   -0.04917096346616745,\n",
       "   0.011406983248889446,\n",
       "   -0.011288423091173172,\n",
       "   0.05178024619817734,\n",
       "   -0.043529070913791656,\n",
       "   0.10861761122941971,\n",
       "   0.14760932326316833,\n",
       "   -0.14580807089805603,\n",
       "   -0.1142740324139595,\n",
       "   -0.2650632858276367,\n",
       "   -0.0852656289935112,\n",
       "   0.16805730760097504,\n",
       "   -2.1287779808044434,\n",
       "   -0.1218203529715538,\n",
       "   0.13027769327163696,\n",
       "   -0.2915595769882202,\n",
       "   -0.26336440443992615,\n",
       "   -0.2972117066383362,\n",
       "   0.2892131507396698,\n",
       "   0.17075979709625244,\n",
       "   0.17035460472106934,\n",
       "   0.3574211895465851,\n",
       "   -0.08114879578351974,\n",
       "   1.8210923671722412,\n",
       "   -0.10423428565263748,\n",
       "   -0.03336134925484657,\n",
       "   -0.08374733477830887,\n",
       "   -0.01773175224661827,\n",
       "   0.1813722401857376,\n",
       "   0.1661313772201538,\n",
       "   -0.12657371163368225,\n",
       "   -0.446445494890213,\n",
       "   -0.0948718786239624,\n",
       "   1.5146453380584717,\n",
       "   0.14150169491767883,\n",
       "   0.3442589044570923,\n",
       "   0.27152758836746216,\n",
       "   -0.20204071700572968,\n",
       "   -0.05952530354261398,\n",
       "   0.10047207027673721,\n",
       "   0.1868969351053238,\n",
       "   -0.009166626259684563,\n",
       "   -0.18844318389892578,\n",
       "   0.2977171838283539,\n",
       "   0.015029413625597954],\n",
       "  [-0.00015290944429580122,\n",
       "   0.08012732118368149,\n",
       "   -0.3884723484516144,\n",
       "   0.2897641062736511,\n",
       "   0.1003539189696312,\n",
       "   -0.11797257512807846,\n",
       "   0.26589858531951904,\n",
       "   -0.3086327612400055,\n",
       "   0.11265437304973602,\n",
       "   0.4406268000602722,\n",
       "   0.03702694922685623,\n",
       "   0.4588066041469574,\n",
       "   0.26594164967536926,\n",
       "   0.09318453818559647,\n",
       "   -0.31936776638031006,\n",
       "   -0.011046825908124447,\n",
       "   -0.11461178958415985,\n",
       "   0.060187775641679764,\n",
       "   0.03040582500398159,\n",
       "   0.03136981651186943,\n",
       "   -0.10687742382287979,\n",
       "   -0.07048746943473816,\n",
       "   0.017662741243839264,\n",
       "   0.15770241618156433,\n",
       "   -0.36370450258255005,\n",
       "   -0.06969498097896576,\n",
       "   0.41403496265411377,\n",
       "   0.016984732821583748,\n",
       "   -0.020112019032239914,\n",
       "   0.25843313336372375,\n",
       "   -0.054240379482507706,\n",
       "   0.11880556493997574,\n",
       "   0.34902423620224,\n",
       "   0.2729916572570801,\n",
       "   -0.12180633097887039,\n",
       "   0.24036897718906403,\n",
       "   0.023433923721313477,\n",
       "   0.6490580439567566,\n",
       "   -0.12018170952796936,\n",
       "   0.050108399242162704,\n",
       "   0.07274451106786728,\n",
       "   0.09980964660644531,\n",
       "   0.026752350851893425,\n",
       "   -0.2607763111591339,\n",
       "   0.2671689987182617,\n",
       "   0.10708972066640854,\n",
       "   -0.15078584849834442,\n",
       "   0.15027453005313873,\n",
       "   0.168278768658638,\n",
       "   -0.296958863735199,\n",
       "   0.1583959013223648,\n",
       "   -0.18971039354801178,\n",
       "   -0.11853650957345963,\n",
       "   -0.11579332500696182,\n",
       "   -0.17175517976284027,\n",
       "   -0.1540990173816681,\n",
       "   0.15222036838531494,\n",
       "   0.026888884603977203,\n",
       "   -0.36224886775016785,\n",
       "   0.6280122995376587,\n",
       "   -0.5350472331047058,\n",
       "   0.0847851112484932,\n",
       "   0.1890598088502884,\n",
       "   -0.029971010982990265,\n",
       "   -0.18735677003860474,\n",
       "   0.22901226580142975,\n",
       "   -0.07461284846067429,\n",
       "   -0.011661943979561329,\n",
       "   0.12498163431882858,\n",
       "   -0.20561730861663818,\n",
       "   0.4230872392654419,\n",
       "   -0.44935956597328186,\n",
       "   -0.23516760766506195,\n",
       "   0.01431258674710989,\n",
       "   -0.2371276170015335,\n",
       "   -0.3079022169113159,\n",
       "   0.06486822664737701,\n",
       "   -0.12396048754453659,\n",
       "   0.1217428594827652,\n",
       "   -0.2739606201648712,\n",
       "   -0.08850369602441788,\n",
       "   0.3755231499671936,\n",
       "   0.014344201423227787,\n",
       "   -0.2604110538959503,\n",
       "   0.028974926099181175,\n",
       "   0.16294503211975098,\n",
       "   0.23932965099811554,\n",
       "   0.10503587126731873,\n",
       "   -0.27573949098587036,\n",
       "   0.2669033110141754,\n",
       "   -0.12328848987817764,\n",
       "   -0.22090309858322144,\n",
       "   -0.2563081979751587,\n",
       "   -0.16973061859607697,\n",
       "   -0.2010665386915207,\n",
       "   0.10432104021310806,\n",
       "   -0.18833079934120178,\n",
       "   0.10759983956813812,\n",
       "   0.2190171778202057,\n",
       "   0.22199541330337524,\n",
       "   0.018200209364295006,\n",
       "   0.1314912736415863,\n",
       "   -0.5911529660224915,\n",
       "   -0.03853924944996834,\n",
       "   -0.19770702719688416,\n",
       "   0.16444151103496552,\n",
       "   0.20242074131965637,\n",
       "   -0.16990061104297638,\n",
       "   -0.0567864365875721,\n",
       "   0.6623995900154114,\n",
       "   0.2702384889125824,\n",
       "   -0.12408792972564697,\n",
       "   0.152226522564888,\n",
       "   -0.2540280222892761,\n",
       "   -0.31624987721443176,\n",
       "   0.4545539319515228,\n",
       "   0.17869524657726288,\n",
       "   -0.000368237029761076,\n",
       "   0.15513606369495392,\n",
       "   0.15764015913009644,\n",
       "   0.13469618558883667,\n",
       "   -0.06942039728164673,\n",
       "   0.16245608031749725,\n",
       "   -0.14433349668979645,\n",
       "   0.15981903672218323,\n",
       "   -0.07638783007860184,\n",
       "   0.13210740685462952,\n",
       "   -0.17590858042240143,\n",
       "   0.3516257703304291,\n",
       "   0.23922228813171387,\n",
       "   -0.33232226967811584,\n",
       "   0.29024335741996765,\n",
       "   0.22360777854919434,\n",
       "   -0.3945878744125366,\n",
       "   0.06739240884780884,\n",
       "   0.16413754224777222,\n",
       "   -0.10280200839042664,\n",
       "   0.40574365854263306,\n",
       "   -0.4385487139225006,\n",
       "   0.09518910944461823,\n",
       "   0.02076076716184616,\n",
       "   0.07459377497434616,\n",
       "   0.3355064392089844,\n",
       "   0.10091532766819,\n",
       "   -0.1044134795665741,\n",
       "   0.18404744565486908,\n",
       "   -0.6618705987930298,\n",
       "   -0.3159271478652954,\n",
       "   -0.17223934829235077,\n",
       "   0.23729778826236725,\n",
       "   -0.25026172399520874,\n",
       "   -0.017625611275434494,\n",
       "   -0.08399201184511185,\n",
       "   0.10148364305496216,\n",
       "   -0.3418375551700592,\n",
       "   -0.10066710412502289,\n",
       "   -0.012531126849353313,\n",
       "   0.2818206548690796,\n",
       "   0.3661157488822937,\n",
       "   0.09728693962097168,\n",
       "   -0.27088016271591187,\n",
       "   -0.043160632252693176,\n",
       "   -0.5030599236488342,\n",
       "   0.07583253085613251,\n",
       "   -0.19294442236423492,\n",
       "   0.07756518572568893,\n",
       "   -0.37598946690559387,\n",
       "   0.19966398179531097,\n",
       "   -0.31780359148979187,\n",
       "   0.2505808174610138,\n",
       "   0.038418397307395935,\n",
       "   -0.04967201128602028,\n",
       "   -0.10398883372545242,\n",
       "   0.472898006439209,\n",
       "   -0.3985276222229004,\n",
       "   -0.32598191499710083,\n",
       "   -0.1102256029844284,\n",
       "   -0.11366535723209381,\n",
       "   -0.3342837989330292,\n",
       "   -0.3177693486213684,\n",
       "   0.14207695424556732,\n",
       "   -0.11092116683721542,\n",
       "   0.13650692999362946,\n",
       "   -0.06089145690202713,\n",
       "   0.004765280522406101,\n",
       "   -0.17864760756492615,\n",
       "   0.1738060563802719,\n",
       "   -0.33048367500305176,\n",
       "   -0.16187557578086853,\n",
       "   0.1387745440006256,\n",
       "   0.2660764753818512,\n",
       "   -0.235383078455925,\n",
       "   -0.5548015832901001,\n",
       "   0.3508426547050476,\n",
       "   -0.012734772637486458,\n",
       "   -0.0018338485388085246,\n",
       "   -0.0075702364556491375,\n",
       "   0.20163124799728394,\n",
       "   0.08954039216041565,\n",
       "   0.15182508528232574,\n",
       "   -0.4681411683559418,\n",
       "   0.15189282596111298,\n",
       "   -0.10995184630155563,\n",
       "   0.0850309506058693,\n",
       "   -0.03314609453082085,\n",
       "   0.03849760815501213,\n",
       "   0.4278879165649414,\n",
       "   -0.09630083292722702,\n",
       "   -0.2791632115840912,\n",
       "   -0.12436139583587646,\n",
       "   -0.293655127286911,\n",
       "   -0.3437187373638153,\n",
       "   -0.18815186619758606,\n",
       "   0.24043601751327515,\n",
       "   -0.04566483572125435,\n",
       "   -0.20900605618953705,\n",
       "   -0.18979813158512115,\n",
       "   -0.21167269349098206,\n",
       "   -0.22652888298034668,\n",
       "   0.281828373670578,\n",
       "   0.49571579694747925,\n",
       "   -0.14773210883140564,\n",
       "   0.12212137877941132,\n",
       "   0.0011849231086671352,\n",
       "   -0.0054628886282444,\n",
       "   -0.17044618725776672,\n",
       "   -0.05154695361852646,\n",
       "   0.23919686675071716,\n",
       "   0.10938795655965805,\n",
       "   -0.0998833030462265,\n",
       "   -0.16368845105171204,\n",
       "   0.319743275642395,\n",
       "   0.35496020317077637,\n",
       "   -0.13813550770282745,\n",
       "   0.1228850781917572,\n",
       "   0.04264014586806297,\n",
       "   0.13763166964054108,\n",
       "   -0.14395982027053833,\n",
       "   -0.014803225174546242,\n",
       "   -0.21556241810321808,\n",
       "   -0.1052745133638382,\n",
       "   0.053625982254743576,\n",
       "   0.27002373337745667,\n",
       "   0.35781338810920715,\n",
       "   -0.1615096926689148,\n",
       "   -0.018870634958148003,\n",
       "   -0.08956097066402435,\n",
       "   0.21524330973625183,\n",
       "   0.2800585627555847,\n",
       "   -0.025290129706263542,\n",
       "   0.12048868834972382,\n",
       "   -0.1071685403585434,\n",
       "   -0.06003810092806816,\n",
       "   0.02519359439611435,\n",
       "   0.2860792577266693,\n",
       "   0.23131981492042542,\n",
       "   0.2370051145553589,\n",
       "   0.18063023686408997,\n",
       "   -0.08796130865812302,\n",
       "   -0.6042127013206482,\n",
       "   0.03309261426329613,\n",
       "   0.01919255033135414,\n",
       "   -0.09953437000513077,\n",
       "   0.13113047182559967,\n",
       "   -0.18557555973529816,\n",
       "   0.0067132459953427315,\n",
       "   -0.11718950420618057,\n",
       "   -0.054636821150779724,\n",
       "   -0.479715496301651,\n",
       "   -0.006999666802585125,\n",
       "   0.05780753865838051,\n",
       "   -0.1587858647108078,\n",
       "   -0.09185132384300232,\n",
       "   0.4564588963985443,\n",
       "   -0.32182425260543823,\n",
       "   0.24412395060062408,\n",
       "   -0.012574135325849056,\n",
       "   -0.09229447692632675,\n",
       "   0.10339941829442978,\n",
       "   0.05599670857191086,\n",
       "   0.0636843666434288,\n",
       "   0.14838463068008423,\n",
       "   0.0699344128370285,\n",
       "   0.5977007150650024,\n",
       "   0.04449950158596039,\n",
       "   0.2567230463027954,\n",
       "   0.20496827363967896,\n",
       "   0.01716310903429985,\n",
       "   -0.0651281476020813,\n",
       "   0.029149312525987625,\n",
       "   -0.03319735825061798,\n",
       "   -0.2012181580066681,\n",
       "   -0.056524716317653656,\n",
       "   0.09818068146705627,\n",
       "   -0.29535120725631714,\n",
       "   -0.11224716156721115,\n",
       "   -0.4462212920188904,\n",
       "   0.42616507411003113,\n",
       "   -0.41575708985328674,\n",
       "   -0.27478861808776855,\n",
       "   0.03417002782225609,\n",
       "   0.05346119403839111,\n",
       "   0.003058658679947257,\n",
       "   -0.08163110166788101,\n",
       "   -0.27619966864585876,\n",
       "   0.04011772572994232,\n",
       "   0.0772600769996643,\n",
       "   0.34324318170547485,\n",
       "   0.267156183719635,\n",
       "   0.26913291215896606,\n",
       "   0.3970060646533966,\n",
       "   0.08045633137226105,\n",
       "   0.12817557156085968,\n",
       "   -0.27313023805618286,\n",
       "   -0.07787783443927765,\n",
       "   -0.493949830532074,\n",
       "   -0.2617952227592468,\n",
       "   -0.45987287163734436,\n",
       "   -0.37602898478507996,\n",
       "   -0.2862066626548767,\n",
       "   0.37839415669441223,\n",
       "   0.10633856058120728,\n",
       "   -0.15312156081199646,\n",
       "   0.17783966660499573,\n",
       "   -0.050953276455402374,\n",
       "   -0.044771958142519,\n",
       "   -0.1803152710199356,\n",
       "   0.3877367675304413,\n",
       "   -0.04976784065365791,\n",
       "   0.029178354889154434,\n",
       "   0.055231764912605286,\n",
       "   -0.07215333729982376,\n",
       "   -0.2561942934989929,\n",
       "   0.05175529420375824,\n",
       "   -0.6389039158821106,\n",
       "   -0.1202056035399437,\n",
       "   -0.42154645919799805,\n",
       "   0.2870051860809326,\n",
       "   -0.016775866970419884,\n",
       "   0.43126165866851807,\n",
       "   0.12270736694335938,\n",
       "   -0.05267100781202316,\n",
       "   0.29488661885261536,\n",
       "   -0.13097336888313293,\n",
       "   0.0009658215567469597,\n",
       "   0.12286821752786636,\n",
       "   0.14946028590202332,\n",
       "   0.16690969467163086,\n",
       "   -0.05604232847690582,\n",
       "   0.3292648494243622,\n",
       "   0.01655629277229309,\n",
       "   0.20510411262512207,\n",
       "   -0.15346203744411469,\n",
       "   -0.1366799920797348,\n",
       "   -0.3815601170063019,\n",
       "   0.09475287795066833,\n",
       "   0.09593717008829117,\n",
       "   -0.09257370978593826,\n",
       "   -0.2515590190887451,\n",
       "   0.5767691135406494,\n",
       "   0.1887926310300827,\n",
       "   0.22871622443199158,\n",
       "   0.09541824460029602,\n",
       "   0.2692564129829407,\n",
       "   0.09543066471815109,\n",
       "   0.2632159888744354,\n",
       "   -0.13490691781044006,\n",
       "   0.4118649959564209,\n",
       "   0.16759978234767914,\n",
       "   -0.09127235412597656,\n",
       "   0.63633131980896,\n",
       "   0.01829805038869381,\n",
       "   0.2899843454360962,\n",
       "   -0.44282782077789307,\n",
       "   -0.18466968834400177,\n",
       "   -0.17261826992034912,\n",
       "   -0.0045245918445289135,\n",
       "   0.07285416126251221,\n",
       "   0.1767004430294037,\n",
       "   -0.37890830636024475,\n",
       "   -0.08752340823411942,\n",
       "   0.070140041410923,\n",
       "   0.6047359704971313,\n",
       "   -0.08484766632318497,\n",
       "   -0.11216255277395248,\n",
       "   -0.42635443806648254,\n",
       "   0.2407151609659195,\n",
       "   -0.21940773725509644,\n",
       "   0.1378462314605713,\n",
       "   0.10833149403333664,\n",
       "   -0.3378264605998993,\n",
       "   -0.04937777295708656,\n",
       "   0.2600060999393463,\n",
       "   -0.009205911308526993,\n",
       "   -0.09147582203149796,\n",
       "   -0.5635219812393188,\n",
       "   0.10002775490283966,\n",
       "   0.3946208655834198,\n",
       "   -0.04102335497736931,\n",
       "   -0.42000555992126465,\n",
       "   -0.12763768434524536,\n",
       "   0.2744685113430023,\n",
       "   0.07823309302330017,\n",
       "   0.28290414810180664,\n",
       "   -0.2729308009147644,\n",
       "   -0.581615686416626,\n",
       "   0.604445219039917,\n",
       "   0.09110791981220245,\n",
       "   0.4109429717063904,\n",
       "   -0.3419567346572876,\n",
       "   0.10724369436502457,\n",
       "   -0.35463443398475647,\n",
       "   -0.09115087240934372,\n",
       "   -0.02041189931333065,\n",
       "   -0.05970023572444916,\n",
       "   -0.2726476192474365,\n",
       "   0.19546394050121307,\n",
       "   0.031519051641225815,\n",
       "   0.12798859179019928,\n",
       "   -0.0035048644058406353,\n",
       "   0.18124213814735413,\n",
       "   0.20460203289985657,\n",
       "   -0.10051000863313675,\n",
       "   -0.4236724376678467,\n",
       "   0.13038834929466248,\n",
       "   -0.1466958373785019,\n",
       "   0.20964212715625763,\n",
       "   -0.06844837963581085,\n",
       "   -0.263739675283432,\n",
       "   0.023392019793391228,\n",
       "   -0.3223366439342499,\n",
       "   0.025356221944093704,\n",
       "   0.07607384026050568,\n",
       "   0.05684973672032356,\n",
       "   -0.5167763233184814,\n",
       "   0.3545910716056824,\n",
       "   -0.05172973498702049,\n",
       "   0.06382807344198227,\n",
       "   0.14279435575008392,\n",
       "   -0.0033904192969202995,\n",
       "   0.12436988949775696,\n",
       "   -0.15281590819358826,\n",
       "   -0.16003122925758362,\n",
       "   -0.2587360739707947,\n",
       "   0.07290735095739365,\n",
       "   -0.30474480986595154,\n",
       "   0.15506601333618164,\n",
       "   0.1669030338525772,\n",
       "   0.2012808918952942,\n",
       "   -0.07304378598928452,\n",
       "   0.09384965896606445,\n",
       "   -0.22091569006443024,\n",
       "   0.0654427632689476,\n",
       "   -0.40450721979141235,\n",
       "   0.32622626423835754,\n",
       "   0.2813292443752289,\n",
       "   -0.3125205934047699,\n",
       "   -0.1384180188179016,\n",
       "   -0.1394728571176529,\n",
       "   -0.33873626589775085,\n",
       "   -0.07362594455480576,\n",
       "   0.16972565650939941,\n",
       "   -0.06862654536962509,\n",
       "   -0.17628629505634308,\n",
       "   -0.21169526875019073,\n",
       "   0.18761388957500458,\n",
       "   0.23779858648777008,\n",
       "   0.30580323934555054,\n",
       "   -0.5417352318763733,\n",
       "   -0.19614149630069733,\n",
       "   0.08289557695388794,\n",
       "   0.1368594616651535,\n",
       "   -0.0935777798295021,\n",
       "   0.4364914000034332,\n",
       "   -0.12487801909446716,\n",
       "   0.28953662514686584,\n",
       "   -0.022410821169614792,\n",
       "   0.008827791549265385,\n",
       "   -0.4945319890975952,\n",
       "   0.1552853137254715,\n",
       "   -0.05711883679032326,\n",
       "   0.14097526669502258,\n",
       "   -0.05202452465891838,\n",
       "   0.09517747908830643,\n",
       "   0.5504316687583923,\n",
       "   -0.413796991109848,\n",
       "   0.3002834618091583,\n",
       "   0.12164207547903061,\n",
       "   0.5905415415763855,\n",
       "   0.01920662447810173,\n",
       "   0.13608111441135406,\n",
       "   -0.35681405663490295,\n",
       "   0.044689927250146866,\n",
       "   -0.2046840935945511,\n",
       "   0.020638903602957726,\n",
       "   -0.1324814110994339,\n",
       "   -0.3848804533481598,\n",
       "   -0.3477117121219635,\n",
       "   0.01702030561864376,\n",
       "   0.07902929186820984,\n",
       "   -0.07676669955253601,\n",
       "   -0.030578915029764175,\n",
       "   -0.16025769710540771,\n",
       "   0.04591907933354378,\n",
       "   -0.42163610458374023,\n",
       "   0.41087600588798523,\n",
       "   0.08581793308258057,\n",
       "   -0.06589177995920181,\n",
       "   0.048959601670503616,\n",
       "   -0.06127263978123665,\n",
       "   0.10846835374832153,\n",
       "   -0.2658383548259735,\n",
       "   0.3738953471183777,\n",
       "   -0.05418143421411514,\n",
       "   0.23127982020378113,\n",
       "   -0.5158993601799011,\n",
       "   0.18100138008594513,\n",
       "   -0.05507601052522659,\n",
       "   -0.048769377171993256,\n",
       "   -0.13609962165355682,\n",
       "   0.20625215768814087,\n",
       "   0.29409778118133545,\n",
       "   -0.28254297375679016,\n",
       "   -0.08931490033864975,\n",
       "   0.12599137425422668,\n",
       "   0.0371335968375206,\n",
       "   -0.004585402552038431,\n",
       "   0.1924716830253601,\n",
       "   0.21793250739574432,\n",
       "   0.13368743658065796,\n",
       "   -0.29485973715782166,\n",
       "   0.021680748090147972,\n",
       "   -0.3488123118877411,\n",
       "   -0.16656355559825897,\n",
       "   0.7536073327064514,\n",
       "   -0.04590808227658272,\n",
       "   -0.10698381811380386,\n",
       "   0.04415435716509819,\n",
       "   -0.14341244101524353,\n",
       "   0.3505246043205261,\n",
       "   0.3077349364757538,\n",
       "   0.0733332708477974,\n",
       "   -0.2863323390483856,\n",
       "   0.27741414308547974,\n",
       "   -0.15149639546871185,\n",
       "   0.011389222927391529,\n",
       "   0.19610454142093658,\n",
       "   0.020383812487125397,\n",
       "   0.15385369956493378,\n",
       "   0.2406240552663803,\n",
       "   0.32627013325691223,\n",
       "   0.19830924272537231,\n",
       "   -0.5969811081886292,\n",
       "   0.15447518229484558,\n",
       "   0.24537767469882965,\n",
       "   -0.05640247091650963,\n",
       "   -0.10178389400243759,\n",
       "   -9.281000137329102,\n",
       "   -0.008212209679186344,\n",
       "   -0.2224607616662979,\n",
       "   0.28255724906921387,\n",
       "   -0.13578464090824127,\n",
       "   0.022554099559783936,\n",
       "   -0.058959804475307465,\n",
       "   0.08270778506994247,\n",
       "   -0.34902697801589966,\n",
       "   0.1415698081254959,\n",
       "   0.051124997437000275,\n",
       "   0.03700349107384682,\n",
       "   0.5797575116157532,\n",
       "   -0.44498345255851746,\n",
       "   -0.07033484429121017,\n",
       "   -0.2966601848602295,\n",
       "   0.0930711179971695,\n",
       "   -0.25031688809394836,\n",
       "   0.04351357743144035,\n",
       "   0.0531453937292099,\n",
       "   0.04294003173708916,\n",
       "   0.10169769823551178,\n",
       "   -0.005132340360432863,\n",
       "   0.24255293607711792,\n",
       "   0.4041638970375061,\n",
       "   0.2817434072494507,\n",
       "   -0.36682605743408203,\n",
       "   -0.3072362542152405,\n",
       "   0.39874669909477234,\n",
       "   -0.266361266374588,\n",
       "   -0.14362168312072754,\n",
       "   0.3219505846500397,\n",
       "   -0.21467703580856323,\n",
       "   -0.012920171022415161,\n",
       "   0.5197749137878418,\n",
       "   -0.05032927915453911,\n",
       "   0.19049453735351562,\n",
       "   -0.013730786740779877,\n",
       "   0.021955057978630066,\n",
       "   -0.3818499445915222,\n",
       "   -0.4925760328769684,\n",
       "   -0.08942112326622009,\n",
       "   -0.3465171754360199,\n",
       "   0.25916239619255066,\n",
       "   -0.0643213614821434,\n",
       "   0.15702368319034576,\n",
       "   0.10262381285429001,\n",
       "   0.2639561891555786,\n",
       "   -0.1008627712726593,\n",
       "   -0.0517757311463356,\n",
       "   0.12811243534088135,\n",
       "   0.15974470973014832,\n",
       "   0.36122196912765503,\n",
       "   -0.04579729586839676,\n",
       "   0.15572403371334076,\n",
       "   0.14501705765724182,\n",
       "   0.06949774920940399,\n",
       "   0.42532190680503845,\n",
       "   -0.19818899035453796,\n",
       "   -0.07091119885444641,\n",
       "   0.08120512217283249,\n",
       "   -0.3069654405117035,\n",
       "   0.5813223719596863,\n",
       "   -0.3947488069534302,\n",
       "   -0.268024742603302,\n",
       "   0.13773803412914276,\n",
       "   0.21234406530857086,\n",
       "   0.07204270362854004,\n",
       "   0.17776191234588623,\n",
       "   -0.31464526057243347,\n",
       "   -0.22207914292812347,\n",
       "   0.38090553879737854,\n",
       "   -0.16518408060073853,\n",
       "   -0.10402152687311172,\n",
       "   -0.1415928155183792,\n",
       "   0.0006813299842178822,\n",
       "   -0.034856200218200684,\n",
       "   0.032254721969366074,\n",
       "   0.12432140856981277,\n",
       "   0.03602888807654381,\n",
       "   -0.09786783903837204,\n",
       "   0.1310724914073944,\n",
       "   0.7350970506668091,\n",
       "   -0.12630513310432434,\n",
       "   -0.04537222906947136,\n",
       "   0.07449883222579956,\n",
       "   0.02834036946296692,\n",
       "   0.44093140959739685,\n",
       "   0.04460557922720909,\n",
       "   0.14423221349716187,\n",
       "   0.007861076854169369,\n",
       "   -0.06530705094337463,\n",
       "   0.08805754035711288,\n",
       "   0.15228486061096191,\n",
       "   0.14748679101467133,\n",
       "   0.33132749795913696,\n",
       "   -0.21360784769058228,\n",
       "   0.37479105591773987,\n",
       "   0.03066692128777504,\n",
       "   0.3495238721370697,\n",
       "   -0.00016943899390753359,\n",
       "   0.05225911736488342,\n",
       "   0.1672937572002411,\n",
       "   -0.3643813133239746,\n",
       "   0.26414114236831665,\n",
       "   -0.16352218389511108,\n",
       "   0.20189180970191956,\n",
       "   -0.21488451957702637,\n",
       "   0.29292652010917664,\n",
       "   -0.020521042868494987,\n",
       "   0.0013361014425754547,\n",
       "   0.36190271377563477,\n",
       "   0.29784566164016724,\n",
       "   -0.08858596533536911,\n",
       "   0.29858410358428955,\n",
       "   0.1560281217098236,\n",
       "   -0.05518171936273575,\n",
       "   -0.10142097622156143,\n",
       "   0.45412150025367737,\n",
       "   -0.12852631509304047,\n",
       "   -0.012932500801980495,\n",
       "   -0.49733108282089233,\n",
       "   -0.15248019993305206,\n",
       "   -0.2064429074525833,\n",
       "   0.3394470810890198,\n",
       "   0.13662779331207275,\n",
       "   -0.3632945418357849,\n",
       "   0.013815130107104778,\n",
       "   0.06824716925621033,\n",
       "   -0.1540287733078003,\n",
       "   0.3162427842617035,\n",
       "   0.41283491253852844,\n",
       "   -0.47701653838157654,\n",
       "   0.4547557532787323,\n",
       "   0.19208866357803345,\n",
       "   -0.19714762270450592,\n",
       "   -0.18194876611232758,\n",
       "   0.30144086480140686,\n",
       "   -0.6145218014717102,\n",
       "   0.202785462141037,\n",
       "   0.01793803833425045,\n",
       "   -0.1407044231891632,\n",
       "   0.18681971728801727,\n",
       "   -0.39693060517311096,\n",
       "   0.5365517139434814,\n",
       "   -0.2080671489238739,\n",
       "   -0.35245969891548157,\n",
       "   -0.2564420998096466,\n",
       "   -0.06417828053236008,\n",
       "   -0.037686776369810104,\n",
       "   -0.09187256544828415,\n",
       "   0.47090381383895874,\n",
       "   0.45210954546928406,\n",
       "   -0.011662467382848263,\n",
       "   0.3087195158004761,\n",
       "   -0.05044393241405487,\n",
       "   0.14938917756080627,\n",
       "   -0.0889984518289566,\n",
       "   -0.08712980151176453,\n",
       "   0.30498218536376953,\n",
       "   -0.2803286910057068,\n",
       "   0.1358737200498581,\n",
       "   0.22019131481647491,\n",
       "   -0.2669585049152374,\n",
       "   -0.037611961364746094,\n",
       "   -0.32008033990859985,\n",
       "   -0.08830101788043976,\n",
       "   -0.10875867307186127,\n",
       "   0.16118454933166504,\n",
       "   -0.24312590062618256,\n",
       "   -0.21227873861789703,\n",
       "   0.14210690557956696,\n",
       "   -0.05903209373354912,\n",
       "   0.222696453332901,\n",
       "   -0.11649807542562485,\n",
       "   0.3954921066761017,\n",
       "   -0.3693070709705353,\n",
       "   -0.20106148719787598,\n",
       "   -0.06846927851438522,\n",
       "   0.2849244773387909,\n",
       "   0.2513014078140259,\n",
       "   0.10105570405721664,\n",
       "   -0.265840619802475,\n",
       "   -0.42318347096443176,\n",
       "   0.16404499113559723,\n",
       "   0.26750510931015015,\n",
       "   -0.10294166952371597,\n",
       "   0.22136950492858887,\n",
       "   -0.00226128613576293,\n",
       "   0.18123860657215118,\n",
       "   -0.11931966990232468,\n",
       "   0.0009458257700316608,\n",
       "   0.15420381724834442,\n",
       "   0.2142229676246643,\n",
       "   0.4453601837158203,\n",
       "   0.05038199946284294,\n",
       "   -0.02172957919538021,\n",
       "   -0.07024859637022018,\n",
       "   -0.05593553185462952,\n",
       "   -0.12208292633295059,\n",
       "   0.04233647882938385,\n",
       "   0.16644392907619476,\n",
       "   -0.020932039245963097,\n",
       "   -0.01906200498342514,\n",
       "   0.0380014069378376,\n",
       "   0.11045017838478088,\n",
       "   -0.03967363014817238,\n",
       "   0.6590482592582703,\n",
       "   0.19384023547172546,\n",
       "   0.17131587862968445,\n",
       "   -0.008038792759180069],\n",
       "  [-0.05362821742892265,\n",
       "   -0.20546190440654755,\n",
       "   -0.08306857198476791,\n",
       "   0.05380939692258835,\n",
       "   -0.007553505711257458,\n",
       "   -0.5140761733055115,\n",
       "   0.21959693729877472,\n",
       "   -0.02055172435939312,\n",
       "   0.094218909740448,\n",
       "   0.11344574391841888,\n",
       "   0.27188840508461,\n",
       "   0.14521972835063934,\n",
       "   -0.4942193329334259,\n",
       "   0.3581477105617523,\n",
       "   -0.5362507700920105,\n",
       "   -0.14989060163497925,\n",
       "   -0.36375507712364197,\n",
       "   -0.23759397864341736,\n",
       "   -0.01682344265282154,\n",
       "   -0.018630756065249443,\n",
       "   0.08365559577941895,\n",
       "   0.047601450234651566,\n",
       "   -0.16935178637504578,\n",
       "   0.2833704948425293,\n",
       "   0.12189844995737076,\n",
       "   -0.4917047917842865,\n",
       "   0.522419810295105,\n",
       "   0.08158346265554428,\n",
       "   0.07785168290138245,\n",
       "   0.890606701374054,\n",
       "   0.10127571225166321,\n",
       "   -0.40201762318611145,\n",
       "   0.17184646427631378,\n",
       "   0.3042065501213074,\n",
       "   -0.3309765160083771,\n",
       "   0.4277554750442505,\n",
       "   -0.286865770816803,\n",
       "   0.5523386597633362,\n",
       "   -0.2891199588775635,\n",
       "   0.3729105591773987,\n",
       "   -0.040679462254047394,\n",
       "   -0.05551305413246155,\n",
       "   -0.13687556982040405,\n",
       "   -0.2999417185783386,\n",
       "   0.248258575797081,\n",
       "   0.18742316961288452,\n",
       "   0.10128307342529297,\n",
       "   -0.261258989572525,\n",
       "   -0.3203575015068054,\n",
       "   -0.17901739478111267,\n",
       "   0.066442109644413,\n",
       "   0.23667454719543457,\n",
       "   -0.07292092591524124,\n",
       "   0.03712815046310425,\n",
       "   0.004288021475076675,\n",
       "   -0.13084165751934052,\n",
       "   0.2671528160572052,\n",
       "   -0.20510023832321167,\n",
       "   -0.33599451184272766,\n",
       "   0.6633272171020508,\n",
       "   -0.2897605895996094,\n",
       "   -0.16201451420783997,\n",
       "   -8.579963468946517e-05,\n",
       "   0.04405471682548523,\n",
       "   0.0368596576154232,\n",
       "   -0.017355239018797874,\n",
       "   0.03749536722898483,\n",
       "   -0.25034669041633606,\n",
       "   0.23044897615909576,\n",
       "   -0.19164234399795532,\n",
       "   0.2821591794490814,\n",
       "   0.12021829187870026,\n",
       "   -0.3389098346233368,\n",
       "   -0.15472260117530823,\n",
       "   -0.27563583850860596,\n",
       "   -0.07882452011108398,\n",
       "   0.18077734112739563,\n",
       "   0.0616760291159153,\n",
       "   -0.04388165846467018,\n",
       "   -0.42978331446647644,\n",
       "   -0.12242643535137177,\n",
       "   0.4867517054080963,\n",
       "   -0.0017870267620310187,\n",
       "   0.5329029560089111,\n",
       "   -0.2011437565088272,\n",
       "   0.19622425734996796,\n",
       "   0.1717786341905594,\n",
       "   0.6112406253814697,\n",
       "   -0.2386736422777176,\n",
       "   0.4171765148639679,\n",
       "   -0.36971718072891235,\n",
       "   0.037881411612033844,\n",
       "   0.23435965180397034,\n",
       "   -0.0038293038960546255,\n",
       "   0.09341989457607269,\n",
       "   0.25496265292167664,\n",
       "   -0.3454315662384033,\n",
       "   -0.1366925984621048,\n",
       "   -0.07532723248004913,\n",
       "   0.31328344345092773,\n",
       "   0.17452238500118256,\n",
       "   0.2923644185066223,\n",
       "   -0.4504217207431793,\n",
       "   -0.03181292489171028,\n",
       "   -0.1008078083395958,\n",
       "   0.053152844309806824,\n",
       "   0.2828296720981598,\n",
       "   -0.7349063754081726,\n",
       "   -0.10437425971031189,\n",
       "   0.517968475818634,\n",
       "   0.2839474678039551,\n",
       "   0.0442606583237648,\n",
       "   0.06968224048614502,\n",
       "   -0.254376620054245,\n",
       "   -0.2528296113014221,\n",
       "   -0.3213370442390442,\n",
       "   0.23128587007522583,\n",
       "   -0.2702181041240692,\n",
       "   -0.11190082132816315,\n",
       "   0.5673362016677856,\n",
       "   0.21817120909690857,\n",
       "   -0.6245011687278748,\n",
       "   -0.14326857030391693,\n",
       "   -0.2814626693725586,\n",
       "   0.019655223935842514,\n",
       "   0.08600945770740509,\n",
       "   -0.012269850820302963,\n",
       "   -0.3146883547306061,\n",
       "   0.08279598504304886,\n",
       "   0.48005208373069763,\n",
       "   -0.23965191841125488,\n",
       "   0.3956538140773773,\n",
       "   -0.008915149606764317,\n",
       "   -0.15879039466381073,\n",
       "   0.15312834084033966,\n",
       "   0.20323263108730316,\n",
       "   -0.32763272523880005,\n",
       "   0.722474217414856,\n",
       "   -0.593181848526001,\n",
       "   0.2183491736650467,\n",
       "   -0.08925028145313263,\n",
       "   0.12112472951412201,\n",
       "   0.4626756012439728,\n",
       "   0.11335822194814682,\n",
       "   0.45373421907424927,\n",
       "   -0.2741273045539856,\n",
       "   -0.17266829311847687,\n",
       "   -0.3696819543838501,\n",
       "   -0.38106709718704224,\n",
       "   0.014235199429094791,\n",
       "   -0.5636178255081177,\n",
       "   -0.04017140343785286,\n",
       "   -0.23603138327598572,\n",
       "   0.36006587743759155,\n",
       "   -0.5233237147331238,\n",
       "   -0.21323136985301971,\n",
       "   0.028269082307815552,\n",
       "   0.5501097440719604,\n",
       "   0.604794979095459,\n",
       "   0.2805608808994293,\n",
       "   0.15005289018154144,\n",
       "   -0.21077634394168854,\n",
       "   -0.48765796422958374,\n",
       "   0.28796952962875366,\n",
       "   -0.0944461077451706,\n",
       "   0.15870635211467743,\n",
       "   -0.35030215978622437,\n",
       "   0.17914979159832,\n",
       "   -0.17136497795581818,\n",
       "   0.31198036670684814,\n",
       "   0.4843765199184418,\n",
       "   0.12355896830558777,\n",
       "   0.19923916459083557,\n",
       "   0.10623819380998611,\n",
       "   -0.15028782188892365,\n",
       "   -0.49865952134132385,\n",
       "   0.21231697499752045,\n",
       "   -0.456020325422287,\n",
       "   -0.05247528478503227,\n",
       "   0.002666114130988717,\n",
       "   0.24121467769145966,\n",
       "   -0.382387638092041,\n",
       "   -0.3125015199184418,\n",
       "   -0.5896748900413513,\n",
       "   -0.2171632945537567,\n",
       "   -0.11549369990825653,\n",
       "   0.2169831395149231,\n",
       "   -0.2632341682910919,\n",
       "   -0.07301411777734756,\n",
       "   -0.06658957898616791,\n",
       "   -0.06399886310100555,\n",
       "   -0.09268242120742798,\n",
       "   -0.5809717178344727,\n",
       "   0.16412891447544098,\n",
       "   -0.08314068615436554,\n",
       "   0.03454311564564705,\n",
       "   -0.12032565474510193,\n",
       "   0.20796075463294983,\n",
       "   -0.02846035175025463,\n",
       "   -0.14473550021648407,\n",
       "   -0.09370690584182739,\n",
       "   -0.3057253658771515,\n",
       "   0.21686790883541107,\n",
       "   0.22809313237667084,\n",
       "   0.1578647792339325,\n",
       "   0.3060404658317566,\n",
       "   -0.2894028425216675,\n",
       "   0.26351505517959595,\n",
       "   -0.35048720240592957,\n",
       "   -0.04635307937860489,\n",
       "   0.18372449278831482,\n",
       "   -0.13899223506450653,\n",
       "   -0.31034770607948303,\n",
       "   0.22996298968791962,\n",
       "   -0.037417519837617874,\n",
       "   -0.19104550778865814,\n",
       "   -0.16201350092887878,\n",
       "   -0.0857795849442482,\n",
       "   -0.16318321228027344,\n",
       "   -0.08298202604055405,\n",
       "   -0.1488478034734726,\n",
       "   0.07608816772699356,\n",
       "   0.46172916889190674,\n",
       "   -0.26464104652404785,\n",
       "   -0.5016831159591675,\n",
       "   -0.36229825019836426,\n",
       "   0.2589702904224396,\n",
       "   0.2869439721107483,\n",
       "   0.5618931651115417,\n",
       "   -0.06565457582473755,\n",
       "   -0.18244290351867676,\n",
       "   -0.2258097529411316,\n",
       "   0.36543989181518555,\n",
       "   -0.006010051351040602,\n",
       "   0.13818825781345367,\n",
       "   0.4065086841583252,\n",
       "   0.023546947166323662,\n",
       "   -0.026606710627675056,\n",
       "   -0.14492639899253845,\n",
       "   -0.8568025231361389,\n",
       "   0.15001286566257477,\n",
       "   0.17898772656917572,\n",
       "   -0.13954387605190277,\n",
       "   0.493399441242218,\n",
       "   -0.09181012213230133,\n",
       "   -0.14665894210338593,\n",
       "   -0.23372364044189453,\n",
       "   0.3207129240036011,\n",
       "   0.3891829550266266,\n",
       "   -0.31812480092048645,\n",
       "   -0.17354345321655273,\n",
       "   -0.3614795207977295,\n",
       "   0.001277912873774767,\n",
       "   -0.41590186953544617,\n",
       "   0.5858234167098999,\n",
       "   -0.14128638803958893,\n",
       "   0.5271649360656738,\n",
       "   0.016721894964575768,\n",
       "   0.21780216693878174,\n",
       "   -0.6339848041534424,\n",
       "   0.20317165553569794,\n",
       "   -0.07165433466434479,\n",
       "   0.005372137762606144,\n",
       "   -0.04978417605161667,\n",
       "   -0.39731040596961975,\n",
       "   -0.270362913608551,\n",
       "   0.09139514714479446,\n",
       "   0.2877711057662964,\n",
       "   -0.4644916355609894,\n",
       "   0.34780794382095337,\n",
       "   -0.05130688473582268,\n",
       "   -0.22551803290843964,\n",
       "   -0.06966529786586761,\n",
       "   0.15498065948486328,\n",
       "   -0.19721880555152893,\n",
       "   0.07669822126626968,\n",
       "   -0.060481637716293335,\n",
       "   0.0004654106160160154,\n",
       "   -0.4507625699043274,\n",
       "   0.06659843027591705,\n",
       "   0.4352518320083618,\n",
       "   0.42048847675323486,\n",
       "   0.36917296051979065,\n",
       "   0.25010478496551514,\n",
       "   -0.5123941898345947,\n",
       "   0.3424646258354187,\n",
       "   0.337398499250412,\n",
       "   0.34797295928001404,\n",
       "   -0.2168131321668625,\n",
       "   -0.2651118338108063,\n",
       "   0.1633339375257492,\n",
       "   0.2580569088459015,\n",
       "   -0.7078709006309509,\n",
       "   0.16406838595867157,\n",
       "   -0.11145370453596115,\n",
       "   -0.010198196396231651,\n",
       "   -0.054935336112976074,\n",
       "   0.36400726437568665,\n",
       "   -0.34447652101516724,\n",
       "   -0.2534966468811035,\n",
       "   -0.3767426908016205,\n",
       "   0.08505488932132721,\n",
       "   0.25624600052833557,\n",
       "   0.1363922655582428,\n",
       "   -0.18084195256233215,\n",
       "   -0.8595181703567505,\n",
       "   0.1769564300775528,\n",
       "   -0.12898626923561096,\n",
       "   -0.06778421252965927,\n",
       "   0.5088981986045837,\n",
       "   0.4143112003803253,\n",
       "   0.22648920118808746,\n",
       "   0.19789600372314453,\n",
       "   -0.3591476082801819,\n",
       "   0.14089606702327728,\n",
       "   0.06850333511829376,\n",
       "   -0.38736051321029663,\n",
       "   -0.517837643623352,\n",
       "   -0.23305122554302216,\n",
       "   -0.14532196521759033,\n",
       "   0.49716779589653015,\n",
       "   0.5020577907562256,\n",
       "   -0.588623046875,\n",
       "   0.1967843621969223,\n",
       "   -0.06515862792730331,\n",
       "   -0.3492589592933655,\n",
       "   0.013919439166784286,\n",
       "   0.32985737919807434,\n",
       "   0.1400403380393982,\n",
       "   -0.1939304769039154,\n",
       "   0.017173437401652336,\n",
       "   0.18248070776462555,\n",
       "   -0.063407301902771,\n",
       "   -0.5034673810005188,\n",
       "   -0.3956153392791748,\n",
       "   -0.12905912101268768,\n",
       "   -0.27686211466789246,\n",
       "   0.24908874928951263,\n",
       "   -0.1140419989824295,\n",
       "   0.3225293457508087,\n",
       "   0.1821846067905426,\n",
       "   -0.008789479732513428,\n",
       "   0.2424105554819107,\n",
       "   -0.25295889377593994,\n",
       "   0.40485212206840515,\n",
       "   -0.17898967862129211,\n",
       "   -0.051805704832077026,\n",
       "   -0.22442446649074554,\n",
       "   -0.0644136518239975,\n",
       "   0.43866491317749023,\n",
       "   0.008010968565940857,\n",
       "   0.08875028043985367,\n",
       "   -0.02350183017551899,\n",
       "   0.07542316615581512,\n",
       "   0.2902791202068329,\n",
       "   0.25015076994895935,\n",
       "   0.02008146047592163,\n",
       "   0.05902715399861336,\n",
       "   0.09603887051343918,\n",
       "   0.21346555650234222,\n",
       "   0.14963963627815247,\n",
       "   -0.14202792942523956,\n",
       "   -0.5195455551147461,\n",
       "   -0.10715530067682266,\n",
       "   0.0389128252863884,\n",
       "   0.042770545929670334,\n",
       "   0.17188327014446259,\n",
       "   -0.001573159359395504,\n",
       "   0.390970915555954,\n",
       "   -0.2378130406141281,\n",
       "   0.7424635291099548,\n",
       "   -0.335528701543808,\n",
       "   0.25690025091171265,\n",
       "   -0.2982504367828369,\n",
       "   -0.06990701705217361,\n",
       "   -0.07204820960760117,\n",
       "   -0.29939892888069153,\n",
       "   0.006115855183452368,\n",
       "   0.027817532420158386,\n",
       "   -0.45587435364723206,\n",
       "   -0.09735441207885742,\n",
       "   0.21095137298107147,\n",
       "   0.6865816712379456,\n",
       "   -0.18912488222122192,\n",
       "   0.23749598860740662,\n",
       "   -0.47736242413520813,\n",
       "   0.17059704661369324,\n",
       "   -0.08132632076740265,\n",
       "   0.36546745896339417,\n",
       "   0.13374435901641846,\n",
       "   0.4204629361629486,\n",
       "   -0.023231549188494682,\n",
       "   0.3555746078491211,\n",
       "   0.07926122844219208,\n",
       "   -0.2681884169578552,\n",
       "   -0.1540035456418991,\n",
       "   -0.31098803877830505,\n",
       "   0.15976472198963165,\n",
       "   0.0796734169125557,\n",
       "   -0.19272734224796295,\n",
       "   -0.37937813997268677,\n",
       "   0.19909772276878357,\n",
       "   -0.06992662698030472,\n",
       "   -0.19074323773384094,\n",
       "   -0.12638024985790253,\n",
       "   -0.54676353931427,\n",
       "   0.013998298905789852,\n",
       "   0.1852332204580307,\n",
       "   0.18227800726890564,\n",
       "   0.03600424900650978,\n",
       "   -0.08682490885257721,\n",
       "   -0.31553682684898376,\n",
       "   -0.1881665587425232,\n",
       "   0.523568868637085,\n",
       "   0.3203893005847931,\n",
       "   -0.3959992825984955,\n",
       "   0.23779183626174927,\n",
       "   -0.12753738462924957,\n",
       "   0.10524722933769226,\n",
       "   0.0363314226269722,\n",
       "   0.0981217548251152,\n",
       "   -0.13765354454517365,\n",
       "   -0.1716836541891098,\n",
       "   -0.15374091267585754,\n",
       "   0.25866469740867615,\n",
       "   -0.16837771236896515,\n",
       "   0.08551948517560959,\n",
       "   -0.311588853597641,\n",
       "   -0.4288041591644287,\n",
       "   0.5075849294662476,\n",
       "   0.001755721285007894,\n",
       "   -0.30108827352523804,\n",
       "   0.009026257321238518,\n",
       "   0.2797882854938507,\n",
       "   -0.42398831248283386,\n",
       "   0.5470952391624451,\n",
       "   0.3156225085258484,\n",
       "   0.44326016306877136,\n",
       "   -0.09581666439771652,\n",
       "   0.16891753673553467,\n",
       "   0.17567507922649384,\n",
       "   -0.2544827461242676,\n",
       "   -0.04954412579536438,\n",
       "   -0.46157389879226685,\n",
       "   -0.23219497501850128,\n",
       "   -0.30620723962783813,\n",
       "   0.2784813940525055,\n",
       "   -0.3672809898853302,\n",
       "   0.2828706204891205,\n",
       "   -0.00917419046163559,\n",
       "   0.271889865398407,\n",
       "   -0.19910243153572083,\n",
       "   -0.05488830804824829,\n",
       "   -0.3975698947906494,\n",
       "   0.3716473877429962,\n",
       "   -0.12217611819505692,\n",
       "   -0.2717762589454651,\n",
       "   -0.24256277084350586,\n",
       "   -0.14728565514087677,\n",
       "   -0.20084887742996216,\n",
       "   0.07986071705818176,\n",
       "   0.24943824112415314,\n",
       "   0.3369888961315155,\n",
       "   0.47297921776771545,\n",
       "   -0.038900621235370636,\n",
       "   0.0003791743656620383,\n",
       "   0.23036199808120728,\n",
       "   0.37665215134620667,\n",
       "   0.08140791207551956,\n",
       "   -0.06075792759656906,\n",
       "   0.538266658782959,\n",
       "   0.0642189234495163,\n",
       "   0.2021331489086151,\n",
       "   0.4138665497303009,\n",
       "   -0.19301559031009674,\n",
       "   -0.2193896472454071,\n",
       "   -0.16473548114299774,\n",
       "   0.4899192154407501,\n",
       "   -0.5034283399581909,\n",
       "   -0.07250375300645828,\n",
       "   -0.1260080635547638,\n",
       "   -0.10426277667284012,\n",
       "   -0.134848952293396,\n",
       "   -0.17648692429065704,\n",
       "   0.5261843800544739,\n",
       "   -0.2905016541481018,\n",
       "   0.6408196687698364,\n",
       "   -0.05856447294354439,\n",
       "   0.06075062230229378,\n",
       "   0.3063848912715912,\n",
       "   -0.26711753010749817,\n",
       "   -0.27884551882743835,\n",
       "   0.24276202917099,\n",
       "   0.1125049740076065,\n",
       "   -0.036932870745658875,\n",
       "   0.021093547344207764,\n",
       "   -0.1577625274658203,\n",
       "   -0.14244268834590912,\n",
       "   -0.30425509810447693,\n",
       "   0.1286475658416748,\n",
       "   -0.5330514907836914,\n",
       "   -0.18862132728099823,\n",
       "   0.14282678067684174,\n",
       "   0.031086904928088188,\n",
       "   -0.22845809161663055,\n",
       "   0.8762878179550171,\n",
       "   0.5793572664260864,\n",
       "   -0.4325782358646393,\n",
       "   0.188038170337677,\n",
       "   -0.21602235734462738,\n",
       "   -0.2093735933303833,\n",
       "   -0.6137098670005798,\n",
       "   -0.02394270710647106,\n",
       "   0.037674665451049805,\n",
       "   -0.12546247243881226,\n",
       "   -0.20988576114177704,\n",
       "   0.3158893883228302,\n",
       "   -0.12539739906787872,\n",
       "   -0.009359655901789665,\n",
       "   0.20354591310024261,\n",
       "   0.6587973237037659,\n",
       "   0.242533341050148,\n",
       "   -0.21142533421516418,\n",
       "   -0.2503383159637451,\n",
       "   -0.04492383450269699,\n",
       "   0.27379652857780457,\n",
       "   0.7606788873672485,\n",
       "   0.09243757277727127,\n",
       "   0.08066083490848541,\n",
       "   0.33663469552993774,\n",
       "   -0.39806899428367615,\n",
       "   0.2919398844242096,\n",
       "   -0.0041012573055922985,\n",
       "   0.07624057680368423,\n",
       "   0.8717884421348572,\n",
       "   -0.049866631627082825,\n",
       "   -0.17173394560813904,\n",
       "   0.09651866555213928,\n",
       "   -0.41367822885513306,\n",
       "   0.1812722235918045,\n",
       "   0.05077095329761505,\n",
       "   -0.09530948102474213,\n",
       "   -0.045974284410476685,\n",
       "   0.12423870712518692,\n",
       "   -0.1661440134048462,\n",
       "   0.38909056782722473,\n",
       "   0.25796225666999817,\n",
       "   0.2816123366355896,\n",
       "   0.11072948575019836,\n",
       "   0.2618429958820343,\n",
       "   0.0008684497443027794,\n",
       "   -0.069534070789814,\n",
       "   -0.7481278777122498,\n",
       "   0.1528906375169754,\n",
       "   0.2246328592300415,\n",
       "   -0.0036385117564350367,\n",
       "   -0.12603417038917542,\n",
       "   -8.999354362487793,\n",
       "   0.32170745730400085,\n",
       "   -0.04464658722281456,\n",
       "   0.16836203634738922,\n",
       "   0.30167993903160095,\n",
       "   -0.03146403282880783,\n",
       "   0.02947564423084259,\n",
       "   -0.6308788657188416,\n",
       "   -0.7517760396003723,\n",
       "   0.18763864040374756,\n",
       "   0.01006321981549263,\n",
       "   0.09255549311637878,\n",
       "   0.4159988760948181,\n",
       "   -0.28053510189056396,\n",
       "   0.16165587306022644,\n",
       "   -0.26394736766815186,\n",
       "   0.22217828035354614,\n",
       "   -0.565394937992096,\n",
       "   0.3649221658706665,\n",
       "   -0.08855640143156052,\n",
       "   0.06130880489945412,\n",
       "   0.44148778915405273,\n",
       "   -0.22061225771903992,\n",
       "   0.43044978380203247,\n",
       "   0.12455383688211441,\n",
       "   0.34147828817367554,\n",
       "   -0.01208842545747757,\n",
       "   -0.060788605362176895,\n",
       "   0.27137136459350586,\n",
       "   -0.10275992751121521,\n",
       "   -0.1361445039510727,\n",
       "   0.3643706142902374,\n",
       "   0.0029646868351846933,\n",
       "   -0.23015053570270538,\n",
       "   0.47138267755508423,\n",
       "   -0.26556238532066345,\n",
       "   0.23824264109134674,\n",
       "   0.09812357276678085,\n",
       "   0.1551550030708313,\n",
       "   -0.5550961494445801,\n",
       "   -0.20755431056022644,\n",
       "   -0.041184015572071075,\n",
       "   -0.1797877997159958,\n",
       "   0.27235880494117737,\n",
       "   0.13414345681667328,\n",
       "   0.019539188593626022,\n",
       "   0.33020755648612976,\n",
       "   0.41794106364250183,\n",
       "   -0.14288075268268585,\n",
       "   -0.21366268396377563,\n",
       "   0.26190024614334106,\n",
       "   0.465075820684433,\n",
       "   0.08701527118682861,\n",
       "   0.1894678920507431,\n",
       "   0.34810757637023926,\n",
       "   0.23744039237499237,\n",
       "   -0.048567332327365875,\n",
       "   0.6342610716819763,\n",
       "   -0.27459731698036194,\n",
       "   -0.3747440576553345,\n",
       "   0.39568501710891724,\n",
       "   -0.1944483071565628,\n",
       "   0.8582138419151306,\n",
       "   0.016868242993950844,\n",
       "   0.0021724612452089787,\n",
       "   0.29701778292655945,\n",
       "   0.07636323571205139,\n",
       "   -0.08509030193090439,\n",
       "   0.20625145733356476,\n",
       "   -0.41840460896492004,\n",
       "   -0.44551923871040344,\n",
       "   -0.2013372778892517,\n",
       "   0.10678374767303467,\n",
       "   -0.5432321429252625,\n",
       "   -0.28603482246398926,\n",
       "   -0.2281777262687683,\n",
       "   -0.19513832032680511,\n",
       "   -0.007890265434980392,\n",
       "   -0.020244590938091278,\n",
       "   0.01720103994011879,\n",
       "   -0.4401368200778961,\n",
       "   0.6403221487998962,\n",
       "   0.7450942993164062,\n",
       "   -0.35215187072753906,\n",
       "   -0.13234254717826843,\n",
       "   -0.22960613667964935,\n",
       "   -0.06280352920293808,\n",
       "   0.5212649703025818,\n",
       "   0.2303433120250702,\n",
       "   -0.14702071249485016,\n",
       "   -0.08800296485424042,\n",
       "   0.24004042148590088,\n",
       "   0.25870034098625183,\n",
       "   0.24559539556503296,\n",
       "   -0.21311599016189575,\n",
       "   0.416113018989563,\n",
       "   -0.03294958919286728,\n",
       "   0.016953276470303535,\n",
       "   -0.07967506349086761,\n",
       "   0.007562057580798864,\n",
       "   -0.06319014728069305,\n",
       "   -0.14362981915473938,\n",
       "   -0.631452739238739,\n",
       "   -0.23479828238487244,\n",
       "   -0.030016746371984482,\n",
       "   -0.13884423673152924,\n",
       "   -0.06113097444176674,\n",
       "   -0.37784042954444885,\n",
       "   0.53810054063797,\n",
       "   0.3161105215549469,\n",
       "   -0.19398006796836853,\n",
       "   0.3004313111305237,\n",
       "   0.5870161056518555,\n",
       "   -0.01577373966574669,\n",
       "   0.27217185497283936,\n",
       "   0.39699405431747437,\n",
       "   -0.2065609246492386,\n",
       "   -0.35150623321533203,\n",
       "   -0.024800771847367287,\n",
       "   -0.07735385000705719,\n",
       "   -0.13374044001102448,\n",
       "   -0.29008424282073975,\n",
       "   -0.3205210864543915,\n",
       "   -0.26359909772872925,\n",
       "   0.24110843241214752,\n",
       "   0.10319586098194122,\n",
       "   -0.22554482519626617,\n",
       "   -0.5072118043899536,\n",
       "   0.017501449212431908,\n",
       "   -0.028519198298454285,\n",
       "   0.6414299011230469,\n",
       "   0.33137786388397217,\n",
       "   -0.336294561624527,\n",
       "   0.39463552832603455,\n",
       "   0.0075894128531217575,\n",
       "   -0.05294468253850937,\n",
       "   -0.2747686207294464,\n",
       "   0.16706408560276031,\n",
       "   -0.4558309316635132,\n",
       "   0.011317354626953602,\n",
       "   -0.2533467710018158,\n",
       "   -0.16745756566524506,\n",
       "   0.8323479294776917,\n",
       "   0.06274231523275375,\n",
       "   0.6852899789810181,\n",
       "   0.03830617293715477,\n",
       "   -0.03053821250796318,\n",
       "   -0.2138511836528778,\n",
       "   -0.05297904089093208,\n",
       "   -0.12857586145401,\n",
       "   -0.07907035201787949,\n",
       "   0.519373893737793,\n",
       "   0.36844322085380554,\n",
       "   0.08162862807512283,\n",
       "   0.20541223883628845,\n",
       "   -0.46867427229881287,\n",
       "   3.110960460617207e-05,\n",
       "   0.12128904461860657,\n",
       "   -0.6480856537818909,\n",
       "   0.03944782167673111,\n",
       "   0.1722259372472763,\n",
       "   -0.012855365872383118,\n",
       "   0.03632038086652756,\n",
       "   0.011875063180923462,\n",
       "   -0.33758243918418884,\n",
       "   -0.12635044753551483,\n",
       "   -0.11699515581130981,\n",
       "   0.09132622927427292,\n",
       "   0.03185679763555527,\n",
       "   -0.18540233373641968,\n",
       "   -0.17000289261341095,\n",
       "   -0.03367273136973381,\n",
       "   0.11955329775810242,\n",
       "   0.19259712100028992,\n",
       "   0.06709836423397064,\n",
       "   -0.04844522476196289,\n",
       "   0.2534991502761841,\n",
       "   -0.22531701624393463,\n",
       "   0.3690544366836548,\n",
       "   0.2651667296886444,\n",
       "   0.569176971912384,\n",
       "   0.2886291742324829,\n",
       "   -0.1527675986289978,\n",
       "   -0.780421257019043,\n",
       "   -0.27488112449645996,\n",
       "   0.49056097865104675,\n",
       "   0.4545229375362396,\n",
       "   0.05469173938035965,\n",
       "   -0.13031667470932007,\n",
       "   0.3847530782222748,\n",
       "   0.019470511004328728,\n",
       "   0.05145934224128723,\n",
       "   -0.032504670321941376,\n",
       "   0.22893458604812622,\n",
       "   -0.04318099096417427,\n",
       "   0.43717026710510254,\n",
       "   0.0868506208062172,\n",
       "   0.31041622161865234,\n",
       "   -0.3181533217430115,\n",
       "   -0.11609168350696564,\n",
       "   0.19027099013328552,\n",
       "   0.2252909541130066,\n",
       "   -0.022953331470489502,\n",
       "   -0.4304545521736145,\n",
       "   0.2519226670265198,\n",
       "   -0.03491334244608879,\n",
       "   -0.08118058741092682,\n",
       "   0.23829421401023865,\n",
       "   0.3828681409358978,\n",
       "   -0.0854487419128418,\n",
       "   0.21378961205482483],\n",
       "  [0.28046631813049316,\n",
       "   -0.19070617854595184,\n",
       "   -0.07161179184913635,\n",
       "   0.13770972192287445,\n",
       "   0.04684202000498772,\n",
       "   -0.1880185753107071,\n",
       "   0.24970142543315887,\n",
       "   -0.01801096834242344,\n",
       "   -0.41722923517227173,\n",
       "   0.21666152775287628,\n",
       "   0.2436591535806656,\n",
       "   0.1905343234539032,\n",
       "   -0.5529885292053223,\n",
       "   0.41170942783355713,\n",
       "   -0.3722990155220032,\n",
       "   0.27769654989242554,\n",
       "   0.2829175591468811,\n",
       "   -0.2040744572877884,\n",
       "   0.04024979844689369,\n",
       "   0.11470441520214081,\n",
       "   0.0803893432021141,\n",
       "   0.05077841877937317,\n",
       "   -0.30934014916419983,\n",
       "   -0.004049472976475954,\n",
       "   -0.08218864351511002,\n",
       "   -0.5025867223739624,\n",
       "   0.21201428771018982,\n",
       "   0.15799196064472198,\n",
       "   0.5911697745323181,\n",
       "   0.43270108103752136,\n",
       "   0.1536911129951477,\n",
       "   -0.27464941143989563,\n",
       "   0.4741092622280121,\n",
       "   0.04739484563469887,\n",
       "   -0.2523052394390106,\n",
       "   0.39995384216308594,\n",
       "   -0.08065532147884369,\n",
       "   0.41212326288223267,\n",
       "   -0.13912668824195862,\n",
       "   0.14276157319545746,\n",
       "   0.21543680131435394,\n",
       "   -0.2175288200378418,\n",
       "   -0.15143707394599915,\n",
       "   -0.5182114243507385,\n",
       "   0.2860046625137329,\n",
       "   -0.4104136824607849,\n",
       "   0.1471448838710785,\n",
       "   -0.3139074146747589,\n",
       "   -0.20179708302021027,\n",
       "   0.03571143001317978,\n",
       "   -0.0059696887619793415,\n",
       "   0.3744499683380127,\n",
       "   -0.6001506447792053,\n",
       "   -0.1680605262517929,\n",
       "   -0.09315241128206253,\n",
       "   -0.32471176981925964,\n",
       "   0.15698325634002686,\n",
       "   -0.0006217930349521339,\n",
       "   -0.42996689677238464,\n",
       "   0.66495281457901,\n",
       "   -0.38317084312438965,\n",
       "   0.4111855924129486,\n",
       "   -0.018294362351298332,\n",
       "   0.25540632009506226,\n",
       "   0.14531436562538147,\n",
       "   0.34802812337875366,\n",
       "   -0.09926307946443558,\n",
       "   0.005301497410982847,\n",
       "   0.24564127624034882,\n",
       "   -0.4722098112106323,\n",
       "   0.12551580369472504,\n",
       "   0.2110878974199295,\n",
       "   -0.3383445739746094,\n",
       "   -0.045976389199495316,\n",
       "   0.2200976014137268,\n",
       "   -0.40419337153434753,\n",
       "   0.08835484087467194,\n",
       "   -0.13787707686424255,\n",
       "   0.36143815517425537,\n",
       "   -0.25795960426330566,\n",
       "   0.19070222973823547,\n",
       "   0.609140932559967,\n",
       "   -0.04646351933479309,\n",
       "   0.547934353351593,\n",
       "   0.08290418237447739,\n",
       "   0.03386383131146431,\n",
       "   -0.05032029375433922,\n",
       "   0.38688984513282776,\n",
       "   0.1379457712173462,\n",
       "   0.3616695702075958,\n",
       "   -0.2446182817220688,\n",
       "   -0.15714047849178314,\n",
       "   0.23653210699558258,\n",
       "   -0.21548381447792053,\n",
       "   -0.21891900897026062,\n",
       "   0.30615919828414917,\n",
       "   -0.2593451738357544,\n",
       "   -0.3885423243045807,\n",
       "   -0.46611765027046204,\n",
       "   0.238413468003273,\n",
       "   -0.05474349483847618,\n",
       "   0.08159185945987701,\n",
       "   -0.3059577941894531,\n",
       "   -0.13717298209667206,\n",
       "   -0.03643718734383583,\n",
       "   0.1479605734348297,\n",
       "   -0.20264123380184174,\n",
       "   -0.579948365688324,\n",
       "   -0.25558751821517944,\n",
       "   0.5630895495414734,\n",
       "   0.02539312094449997,\n",
       "   0.1153126060962677,\n",
       "   -0.16514132916927338,\n",
       "   0.07766485214233398,\n",
       "   -0.022659000009298325,\n",
       "   -0.05984211340546608,\n",
       "   -0.1553928554058075,\n",
       "   -0.4249820411205292,\n",
       "   -0.35624679923057556,\n",
       "   0.2075740545988083,\n",
       "   0.1738974153995514,\n",
       "   0.14418762922286987,\n",
       "   -0.07639706879854202,\n",
       "   -0.22204691171646118,\n",
       "   0.10295363515615463,\n",
       "   -0.09547174721956253,\n",
       "   -0.19198818504810333,\n",
       "   -0.3428857624530792,\n",
       "   -0.086879663169384,\n",
       "   0.45756417512893677,\n",
       "   -0.2195005565881729,\n",
       "   -0.18789006769657135,\n",
       "   0.06836922466754913,\n",
       "   -0.13431084156036377,\n",
       "   0.13328927755355835,\n",
       "   -0.07013269513845444,\n",
       "   -0.3154734671115875,\n",
       "   0.26853233575820923,\n",
       "   -0.9699930548667908,\n",
       "   0.11136522144079208,\n",
       "   -0.2754770815372467,\n",
       "   -0.3714471757411957,\n",
       "   0.4972290098667145,\n",
       "   0.16314615309238434,\n",
       "   0.24034343659877777,\n",
       "   -0.1815781146287918,\n",
       "   -0.27930787205696106,\n",
       "   -0.18836426734924316,\n",
       "   -0.05748427286744118,\n",
       "   0.2994615435600281,\n",
       "   -0.6719810962677002,\n",
       "   -0.04477459564805031,\n",
       "   0.07126793265342712,\n",
       "   0.5851690769195557,\n",
       "   -0.24970443546772003,\n",
       "   -0.12436947971582413,\n",
       "   -0.061887264251708984,\n",
       "   0.4425712525844574,\n",
       "   0.6064181923866272,\n",
       "   0.39309513568878174,\n",
       "   0.22473177313804626,\n",
       "   -0.13751578330993652,\n",
       "   -0.30458927154541016,\n",
       "   0.007434740662574768,\n",
       "   0.019765600562095642,\n",
       "   0.04383612796664238,\n",
       "   -0.36319828033447266,\n",
       "   0.20249824225902557,\n",
       "   0.10180595517158508,\n",
       "   0.0680360272526741,\n",
       "   0.33169546723365784,\n",
       "   0.005490010138601065,\n",
       "   -0.029876234009861946,\n",
       "   0.2276054322719574,\n",
       "   -0.05782042071223259,\n",
       "   -0.07238619774580002,\n",
       "   0.03959322348237038,\n",
       "   -0.11480821669101715,\n",
       "   0.1302613914012909,\n",
       "   0.11777333915233612,\n",
       "   0.11285427212715149,\n",
       "   -0.07290874421596527,\n",
       "   -0.22361591458320618,\n",
       "   -0.41470852494239807,\n",
       "   0.17711980640888214,\n",
       "   -0.0029926220886409283,\n",
       "   0.4152749180793762,\n",
       "   -0.29806074500083923,\n",
       "   0.6583307981491089,\n",
       "   -0.28136563301086426,\n",
       "   -0.06928713619709015,\n",
       "   -0.1353730708360672,\n",
       "   -0.6296660304069519,\n",
       "   0.20903357863426208,\n",
       "   -0.19580085575580597,\n",
       "   -0.22447079420089722,\n",
       "   0.25079870223999023,\n",
       "   0.041551221162080765,\n",
       "   0.2038203328847885,\n",
       "   0.020759670063853264,\n",
       "   -0.3248453140258789,\n",
       "   0.12609754502773285,\n",
       "   0.0527631901204586,\n",
       "   -0.05727819353342056,\n",
       "   -0.020863767713308334,\n",
       "   0.20032258331775665,\n",
       "   -0.31260061264038086,\n",
       "   0.21712735295295715,\n",
       "   -0.1652584820985794,\n",
       "   -0.29021814465522766,\n",
       "   0.3714308440685272,\n",
       "   -0.48044171929359436,\n",
       "   -0.3374655246734619,\n",
       "   -0.18354374170303345,\n",
       "   -0.24972514808177948,\n",
       "   -0.2881585359573364,\n",
       "   0.3424365818500519,\n",
       "   -0.059604447335004807,\n",
       "   -0.08653879910707474,\n",
       "   -0.0216554943472147,\n",
       "   0.028078949078917503,\n",
       "   -0.07577236741781235,\n",
       "   0.39770108461380005,\n",
       "   -0.4505547881126404,\n",
       "   -0.1306668072938919,\n",
       "   -0.22321338951587677,\n",
       "   0.15359941124916077,\n",
       "   0.24810053408145905,\n",
       "   0.24921482801437378,\n",
       "   -0.1708303689956665,\n",
       "   -0.19797484576702118,\n",
       "   -0.11478996276855469,\n",
       "   0.17906655371189117,\n",
       "   -0.2998790740966797,\n",
       "   -0.012274338863790035,\n",
       "   0.21376831829547882,\n",
       "   0.12966439127922058,\n",
       "   0.12815701961517334,\n",
       "   -0.16880644857883453,\n",
       "   -0.8358525037765503,\n",
       "   0.03342065215110779,\n",
       "   0.12065484374761581,\n",
       "   0.07747907936573029,\n",
       "   0.07218096405267715,\n",
       "   -0.06992555409669876,\n",
       "   0.06738486140966415,\n",
       "   -0.2633574306964874,\n",
       "   0.37408748269081116,\n",
       "   0.33649942278862,\n",
       "   -0.08286435157060623,\n",
       "   0.18065722286701202,\n",
       "   -0.16770948469638824,\n",
       "   0.10610935837030411,\n",
       "   -0.2790919542312622,\n",
       "   0.5945287346839905,\n",
       "   -0.5345039963722229,\n",
       "   0.002979596145451069,\n",
       "   0.2213110327720642,\n",
       "   -0.005305150058120489,\n",
       "   -0.47401857376098633,\n",
       "   0.10756277292966843,\n",
       "   0.13850994408130646,\n",
       "   0.16551069915294647,\n",
       "   0.10023949295282364,\n",
       "   -0.22550195455551147,\n",
       "   -0.20865733921527863,\n",
       "   0.09439331293106079,\n",
       "   0.35153618454933167,\n",
       "   -0.3605828285217285,\n",
       "   0.6094035506248474,\n",
       "   0.20584912598133087,\n",
       "   -0.3989737629890442,\n",
       "   -0.26624825596809387,\n",
       "   0.416972815990448,\n",
       "   -0.1262473464012146,\n",
       "   0.17250430583953857,\n",
       "   -0.04548443853855133,\n",
       "   -0.35756009817123413,\n",
       "   -0.2762342691421509,\n",
       "   -0.045149900019168854,\n",
       "   0.6578981876373291,\n",
       "   0.207273930311203,\n",
       "   -0.021165460348129272,\n",
       "   0.1530090570449829,\n",
       "   -0.4990735948085785,\n",
       "   -0.22494064271450043,\n",
       "   -0.14443103969097137,\n",
       "   0.3264307677745819,\n",
       "   -0.40263450145721436,\n",
       "   -0.3188915252685547,\n",
       "   -0.18746088445186615,\n",
       "   0.23551811277866364,\n",
       "   -0.23759403824806213,\n",
       "   -0.08646433800458908,\n",
       "   -0.14811842143535614,\n",
       "   -0.047545209527015686,\n",
       "   -0.3354330360889435,\n",
       "   0.3778958022594452,\n",
       "   -0.39460548758506775,\n",
       "   -0.12479124963283539,\n",
       "   -0.10522512346506119,\n",
       "   0.18605634570121765,\n",
       "   0.28010791540145874,\n",
       "   0.30427929759025574,\n",
       "   -0.1993744671344757,\n",
       "   -0.473113089799881,\n",
       "   0.28181347250938416,\n",
       "   0.4218594431877136,\n",
       "   0.5846205949783325,\n",
       "   0.37713491916656494,\n",
       "   0.5932141542434692,\n",
       "   0.19461505115032196,\n",
       "   -0.35964035987854004,\n",
       "   -0.39208951592445374,\n",
       "   0.1504465788602829,\n",
       "   0.06536222249269485,\n",
       "   -0.07896704971790314,\n",
       "   -0.44647377729415894,\n",
       "   -0.33356279134750366,\n",
       "   0.3704763650894165,\n",
       "   0.44458678364753723,\n",
       "   0.2321304976940155,\n",
       "   -0.16872277855873108,\n",
       "   0.21552984416484833,\n",
       "   0.1279928833246231,\n",
       "   -0.09816626459360123,\n",
       "   0.06661908328533173,\n",
       "   0.10992468148469925,\n",
       "   -0.1855519711971283,\n",
       "   -0.04074059799313545,\n",
       "   0.04666143283247948,\n",
       "   -0.27623000741004944,\n",
       "   0.06632167845964432,\n",
       "   -0.46238064765930176,\n",
       "   -0.19453829526901245,\n",
       "   0.12559457123279572,\n",
       "   -0.37924331426620483,\n",
       "   0.5554717779159546,\n",
       "   -0.368296355009079,\n",
       "   0.35640978813171387,\n",
       "   0.1262054145336151,\n",
       "   0.13493049144744873,\n",
       "   0.24288202822208405,\n",
       "   -0.372560054063797,\n",
       "   0.2883254885673523,\n",
       "   0.04158511757850647,\n",
       "   -0.12451504170894623,\n",
       "   0.06824341416358948,\n",
       "   0.0007360278395935893,\n",
       "   -0.03811485320329666,\n",
       "   -0.23182746767997742,\n",
       "   -0.11581229418516159,\n",
       "   -0.20385126769542694,\n",
       "   -0.13817709684371948,\n",
       "   0.24272219836711884,\n",
       "   0.3383960425853729,\n",
       "   0.2618468105792999,\n",
       "   0.056791141629219055,\n",
       "   0.2653263807296753,\n",
       "   0.4210492670536041,\n",
       "   0.25107118487358093,\n",
       "   0.18245741724967957,\n",
       "   -0.24122318625450134,\n",
       "   -0.1829271912574768,\n",
       "   0.11523260176181793,\n",
       "   -0.11466900259256363,\n",
       "   0.09393585473299026,\n",
       "   0.35631611943244934,\n",
       "   -0.17196157574653625,\n",
       "   0.05633034557104111,\n",
       "   0.724998414516449,\n",
       "   -0.3890085518360138,\n",
       "   0.5116389393806458,\n",
       "   0.006959077436476946,\n",
       "   0.07264720648527145,\n",
       "   0.11919643729925156,\n",
       "   -0.19112752377986908,\n",
       "   0.01564393751323223,\n",
       "   0.1742987483739853,\n",
       "   -0.6224803328514099,\n",
       "   0.2133445143699646,\n",
       "   0.15482625365257263,\n",
       "   0.7324672341346741,\n",
       "   -0.33075591921806335,\n",
       "   0.09260207414627075,\n",
       "   -0.06254163384437561,\n",
       "   -0.2569940984249115,\n",
       "   -0.09689125418663025,\n",
       "   0.06716356426477432,\n",
       "   0.055104002356529236,\n",
       "   0.5388466119766235,\n",
       "   -0.1582476794719696,\n",
       "   0.22277657687664032,\n",
       "   -0.32308128476142883,\n",
       "   -0.16120529174804688,\n",
       "   -0.2692984342575073,\n",
       "   -0.4310673475265503,\n",
       "   0.0934714823961258,\n",
       "   0.015274087898433208,\n",
       "   -0.36194950342178345,\n",
       "   -0.1724274754524231,\n",
       "   -0.09507358074188232,\n",
       "   -0.16932213306427002,\n",
       "   -0.1311129480600357,\n",
       "   -0.39338332414627075,\n",
       "   -0.5529777407646179,\n",
       "   -0.02529301308095455,\n",
       "   0.4064950942993164,\n",
       "   0.09032589197158813,\n",
       "   0.1717410534620285,\n",
       "   0.1395077407360077,\n",
       "   -0.3407818377017975,\n",
       "   -0.08669886738061905,\n",
       "   0.13900107145309448,\n",
       "   0.42649558186531067,\n",
       "   -0.6357011795043945,\n",
       "   0.2565883994102478,\n",
       "   -0.23800025880336761,\n",
       "   0.16183270514011383,\n",
       "   0.24388465285301208,\n",
       "   -0.0398721806704998,\n",
       "   0.5732666850090027,\n",
       "   -0.17869284749031067,\n",
       "   -0.30883878469467163,\n",
       "   0.16188138723373413,\n",
       "   0.26147031784057617,\n",
       "   0.08322934806346893,\n",
       "   -0.3634513318538666,\n",
       "   -0.44759172201156616,\n",
       "   -0.003007570281624794,\n",
       "   -0.2224699854850769,\n",
       "   0.043031852692365646,\n",
       "   0.18311002850532532,\n",
       "   0.5187612175941467,\n",
       "   -0.6175909638404846,\n",
       "   0.3594644367694855,\n",
       "   0.24510163068771362,\n",
       "   0.23091046512126923,\n",
       "   0.08362874388694763,\n",
       "   0.06908118724822998,\n",
       "   -0.13061070442199707,\n",
       "   -0.24879084527492523,\n",
       "   0.29967355728149414,\n",
       "   -0.41831740736961365,\n",
       "   -0.19879502058029175,\n",
       "   -0.07655695080757141,\n",
       "   0.3115042448043823,\n",
       "   0.05329841375350952,\n",
       "   -0.14320306479930878,\n",
       "   -0.4544607400894165,\n",
       "   0.449973464012146,\n",
       "   -0.12363819777965546,\n",
       "   -0.11600105464458466,\n",
       "   -0.4902776777744293,\n",
       "   0.49134403467178345,\n",
       "   -0.48790913820266724,\n",
       "   -0.1719866544008255,\n",
       "   -0.02073822356760502,\n",
       "   -0.09735173732042313,\n",
       "   -0.06010041385889053,\n",
       "   -0.20680323243141174,\n",
       "   -0.161550372838974,\n",
       "   0.32984766364097595,\n",
       "   0.37927764654159546,\n",
       "   -0.02678459882736206,\n",
       "   -0.02874666079878807,\n",
       "   0.5901305079460144,\n",
       "   0.11200882494449615,\n",
       "   -0.20354269444942474,\n",
       "   0.01851874589920044,\n",
       "   0.4563835561275482,\n",
       "   0.3196360468864441,\n",
       "   0.2732475697994232,\n",
       "   0.7404157519340515,\n",
       "   0.22739019989967346,\n",
       "   -0.22645585238933563,\n",
       "   0.11364760249853134,\n",
       "   0.19661599397659302,\n",
       "   -0.5145405530929565,\n",
       "   -0.02569957822561264,\n",
       "   -0.28512048721313477,\n",
       "   -0.33010125160217285,\n",
       "   -0.3218332827091217,\n",
       "   -0.32021746039390564,\n",
       "   0.3539379835128784,\n",
       "   0.061068251729011536,\n",
       "   0.8587266802787781,\n",
       "   -0.07265784591436386,\n",
       "   -0.2960425019264221,\n",
       "   0.2978825271129608,\n",
       "   0.07465622574090958,\n",
       "   0.13059572875499725,\n",
       "   0.09960153698921204,\n",
       "   0.2530166208744049,\n",
       "   -0.3091277480125427,\n",
       "   -0.07268279790878296,\n",
       "   -0.013109100982546806,\n",
       "   0.02692479081451893,\n",
       "   -0.43132519721984863,\n",
       "   -0.09348892420530319,\n",
       "   -0.08733408898115158,\n",
       "   -0.13201624155044556,\n",
       "   -0.171132430434227,\n",
       "   0.22280286252498627,\n",
       "   -0.13218143582344055,\n",
       "   0.6625158786773682,\n",
       "   0.41879981756210327,\n",
       "   0.057638850063085556,\n",
       "   -0.09143882244825363,\n",
       "   -0.04850113391876221,\n",
       "   -0.0488370805978775,\n",
       "   -0.352274090051651,\n",
       "   0.011072870343923569,\n",
       "   0.37983042001724243,\n",
       "   -0.21283921599388123,\n",
       "   -0.0162483099848032,\n",
       "   0.3226918578147888,\n",
       "   0.1365685760974884,\n",
       "   0.0290700551122427,\n",
       "   0.18946808576583862,\n",
       "   0.17845800518989563,\n",
       "   0.17018622159957886,\n",
       "   -0.6106261014938354,\n",
       "   -0.529874324798584,\n",
       "   -0.04239792749285698,\n",
       "   -0.140700101852417,\n",
       "   0.8901746273040771,\n",
       "   0.10680091381072998,\n",
       "   -0.20574630796909332,\n",
       "   0.49747246503829956,\n",
       "   -0.08822879195213318,\n",
       "   0.04627586901187897,\n",
       "   0.3583202064037323,\n",
       "   -0.1629447489976883,\n",
       "   0.6583921313285828,\n",
       "   -0.1615629643201828,\n",
       "   -0.011414403095841408,\n",
       "   -0.16684794425964355,\n",
       "   -0.39053037762641907,\n",
       "   0.15429717302322388,\n",
       "   0.004722838755697012,\n",
       "   0.14956748485565186,\n",
       "   0.138554647564888,\n",
       "   0.17286626994609833,\n",
       "   -0.297616571187973,\n",
       "   0.3166879713535309,\n",
       "   0.020758865401148796,\n",
       "   0.39757609367370605,\n",
       "   0.2575247883796692,\n",
       "   0.43607839941978455,\n",
       "   0.18686163425445557,\n",
       "   -0.18917429447174072,\n",
       "   -0.9979385137557983,\n",
       "   0.2724825441837311,\n",
       "   -0.23106616735458374,\n",
       "   -0.13643895089626312,\n",
       "   -0.11473577469587326,\n",
       "   -9.012743949890137,\n",
       "   0.53187096118927,\n",
       "   -0.07489809393882751,\n",
       "   0.12643803656101227,\n",
       "   -0.31927117705345154,\n",
       "   0.15777598321437836,\n",
       "   -0.3658771514892578,\n",
       "   -0.28250083327293396,\n",
       "   -0.435909241437912,\n",
       "   0.23797722160816193,\n",
       "   -0.09316721558570862,\n",
       "   -0.015111702494323254,\n",
       "   0.18104355037212372,\n",
       "   -0.24381497502326965,\n",
       "   0.19390161335468292,\n",
       "   -0.21703237295150757,\n",
       "   0.15240095555782318,\n",
       "   -0.2233324646949768,\n",
       "   0.3002903461456299,\n",
       "   -0.20284926891326904,\n",
       "   0.32404038310050964,\n",
       "   0.44773972034454346,\n",
       "   0.1705523431301117,\n",
       "   0.3797566592693329,\n",
       "   -0.1764664202928543,\n",
       "   0.3713659942150116,\n",
       "   -0.17747735977172852,\n",
       "   0.04994630813598633,\n",
       "   0.10881504416465759,\n",
       "   -0.3875495493412018,\n",
       "   -0.3850945234298706,\n",
       "   0.2316746562719345,\n",
       "   -0.09221145510673523,\n",
       "   0.09799423068761826,\n",
       "   0.22706200182437897,\n",
       "   -0.13233314454555511,\n",
       "   0.05773262307047844,\n",
       "   -0.4753905236721039,\n",
       "   0.06873917579650879,\n",
       "   -0.5873454809188843,\n",
       "   -0.48588693141937256,\n",
       "   -0.3095930814743042,\n",
       "   -0.12415563315153122,\n",
       "   0.24487264454364777,\n",
       "   0.453855037689209,\n",
       "   0.17694704234600067,\n",
       "   -0.04016968235373497,\n",
       "   0.45367830991744995,\n",
       "   0.04983258619904518,\n",
       "   0.2608380615711212,\n",
       "   0.3254074156284332,\n",
       "   0.2442058026790619,\n",
       "   0.3181002736091614,\n",
       "   -0.17165255546569824,\n",
       "   0.0767003670334816,\n",
       "   0.06019166111946106,\n",
       "   0.07876361161470413,\n",
       "   0.351084440946579,\n",
       "   -0.0628800168633461,\n",
       "   -0.177109032869339,\n",
       "   0.08651461452245712,\n",
       "   0.2034107744693756,\n",
       "   0.8572867512702942,\n",
       "   0.10273825377225876,\n",
       "   -0.2722935378551483,\n",
       "   0.5857033729553223,\n",
       "   0.05783207714557648,\n",
       "   0.23818227648735046,\n",
       "   0.12721626460552216,\n",
       "   -0.3882468640804291,\n",
       "   -0.3170216679573059,\n",
       "   -0.2569204568862915,\n",
       "   0.12753358483314514,\n",
       "   -0.452625572681427,\n",
       "   0.34988507628440857,\n",
       "   -0.6254688501358032,\n",
       "   0.08023452013731003,\n",
       "   0.08813590556383133,\n",
       "   -0.1765987128019333,\n",
       "   0.08983975648880005,\n",
       "   -0.45864689350128174,\n",
       "   0.6088526248931885,\n",
       "   0.5694794058799744,\n",
       "   -0.03240623697638512,\n",
       "   0.21027269959449768,\n",
       "   -0.19903375208377838,\n",
       "   -0.21990317106246948,\n",
       "   0.33829498291015625,\n",
       "   0.3215998709201813,\n",
       "   -0.21131345629692078,\n",
       "   -0.3494700789451599,\n",
       "   0.3773781657218933,\n",
       "   -0.05590296909213066,\n",
       "   0.26431402564048767,\n",
       "   0.12503020465373993,\n",
       "   0.5956054329872131,\n",
       "   -0.15501467883586884,\n",
       "   0.11176438629627228,\n",
       "   0.001548607717268169,\n",
       "   0.008603225462138653,\n",
       "   -0.09948696196079254,\n",
       "   0.002256953390315175,\n",
       "   -0.381985604763031,\n",
       "   -0.2612495720386505,\n",
       "   -0.28980666399002075,\n",
       "   -0.06655319780111313,\n",
       "   -0.07927396148443222,\n",
       "   -0.24030552804470062,\n",
       "   0.24676413834095,\n",
       "   -0.09016148746013641,\n",
       "   -0.2637752890586853,\n",
       "   0.2654886543750763,\n",
       "   0.171249121427536,\n",
       "   -0.4030735492706299,\n",
       "   0.014071539975702763,\n",
       "   0.17717064917087555,\n",
       "   -0.11043940484523773,\n",
       "   -0.11119478195905685,\n",
       "   0.25318101048469543,\n",
       "   0.02209489978849888,\n",
       "   -0.1721774786710739,\n",
       "   -0.2351246029138565,\n",
       "   -0.012981140986084938,\n",
       "   -0.2636476159095764,\n",
       "   0.17397859692573547,\n",
       "   0.22605329751968384,\n",
       "   0.16334904730319977,\n",
       "   -0.17419709265232086,\n",
       "   -0.022375429049134254,\n",
       "   0.014007623307406902,\n",
       "   0.2192266881465912,\n",
       "   0.5423415303230286,\n",
       "   -0.4357753098011017,\n",
       "   0.4426116645336151,\n",
       "   -0.12476193159818649,\n",
       "   -0.10771064460277557,\n",
       "   0.05258097127079964,\n",
       "   -0.13532258570194244,\n",
       "   -0.32154279947280884,\n",
       "   0.3903005123138428,\n",
       "   -0.9367335438728333,\n",
       "   0.0256519615650177,\n",
       "   0.3990069627761841,\n",
       "   0.29873043298721313,\n",
       "   0.4146905839443207,\n",
       "   0.13804160058498383,\n",
       "   -0.2722961902618408,\n",
       "   0.20985621213912964,\n",
       "   0.12815585732460022,\n",
       "   0.12096062302589417,\n",
       "   -0.40189871191978455,\n",
       "   0.3790358603000641,\n",
       "   0.17520968616008759,\n",
       "   0.1611010730266571,\n",
       "   -0.13688088953495026,\n",
       "   -0.43790337443351746,\n",
       "   -0.008803186938166618,\n",
       "   0.16840684413909912,\n",
       "   -0.28789326548576355,\n",
       "   0.07405615597963333,\n",
       "   0.002367469947785139,\n",
       "   0.11198671907186508,\n",
       "   0.1742240935564041,\n",
       "   -0.20434711873531342,\n",
       "   -0.2733086049556732,\n",
       "   -0.2594846785068512,\n",
       "   -0.12906941771507263,\n",
       "   -0.18747228384017944,\n",
       "   0.1734137088060379,\n",
       "   -0.05481618642807007,\n",
       "   -0.23183199763298035,\n",
       "   -0.0014102768618613482,\n",
       "   -0.18889862298965454,\n",
       "   0.30897805094718933,\n",
       "   0.07258874177932739,\n",
       "   -0.14138078689575195,\n",
       "   -0.28124067187309265,\n",
       "   -0.01857021078467369,\n",
       "   1.0896401405334473,\n",
       "   -0.10443148761987686,\n",
       "   0.21196840703487396,\n",
       "   0.2341570407152176,\n",
       "   0.12423437088727951,\n",
       "   -0.47476786375045776,\n",
       "   -0.15677380561828613,\n",
       "   0.25994014739990234,\n",
       "   0.6076132655143738,\n",
       "   0.3590207099914551,\n",
       "   -0.38546833395957947,\n",
       "   0.1943417638540268,\n",
       "   0.516679048538208,\n",
       "   0.027859551832079887,\n",
       "   -0.2610571086406708,\n",
       "   0.43198859691619873,\n",
       "   -0.2141464352607727,\n",
       "   0.4434349238872528,\n",
       "   0.08585664629936218,\n",
       "   0.3925272226333618,\n",
       "   -0.21910890936851501,\n",
       "   -0.14944569766521454,\n",
       "   -0.06552690267562866,\n",
       "   0.272533118724823,\n",
       "   0.2136155217885971,\n",
       "   -0.1630031317472458,\n",
       "   -0.03665317967534065,\n",
       "   0.1664859503507614,\n",
       "   -0.14872559905052185,\n",
       "   0.21673911809921265,\n",
       "   -0.03554848954081535,\n",
       "   0.3411966562271118,\n",
       "   -0.03323633596301079],\n",
       "  [0.20413941144943237,\n",
       "   -0.3568560779094696,\n",
       "   0.12449616938829422,\n",
       "   0.05401140823960304,\n",
       "   0.06376826018095016,\n",
       "   -0.24773813784122467,\n",
       "   0.5897760391235352,\n",
       "   -0.41930800676345825,\n",
       "   -0.3490208387374878,\n",
       "   0.5029515624046326,\n",
       "   -0.16014346480369568,\n",
       "   -0.06938182562589645,\n",
       "   -0.24661824107170105,\n",
       "   0.38921603560447693,\n",
       "   -0.5427655577659607,\n",
       "   0.12053650617599487,\n",
       "   -0.06470140814781189,\n",
       "   0.2772042453289032,\n",
       "   0.16415224969387054,\n",
       "   0.15801699459552765,\n",
       "   -0.5916617512702942,\n",
       "   -0.08048611879348755,\n",
       "   -0.13380785286426544,\n",
       "   0.08694970607757568,\n",
       "   0.07232601940631866,\n",
       "   0.09370168298482895,\n",
       "   -0.04333193600177765,\n",
       "   0.19702063500881195,\n",
       "   0.32434800267219543,\n",
       "   0.859807014465332,\n",
       "   0.17782725393772125,\n",
       "   -0.013364698737859726,\n",
       "   -0.13868451118469238,\n",
       "   0.13171257078647614,\n",
       "   -0.12296146154403687,\n",
       "   0.3470029830932617,\n",
       "   -0.1082531288266182,\n",
       "   0.24402378499507904,\n",
       "   -0.25672608613967896,\n",
       "   0.3196110129356384,\n",
       "   -0.08875982463359833,\n",
       "   -0.1807582974433899,\n",
       "   0.2191537618637085,\n",
       "   -0.48362886905670166,\n",
       "   0.10042267292737961,\n",
       "   -0.2928456664085388,\n",
       "   0.4135136604309082,\n",
       "   -0.006402736529707909,\n",
       "   -0.12705887854099274,\n",
       "   0.37664294242858887,\n",
       "   -0.1964423507452011,\n",
       "   0.33143681287765503,\n",
       "   -0.27682945132255554,\n",
       "   -0.2258145809173584,\n",
       "   0.15392769873142242,\n",
       "   -0.1420401632785797,\n",
       "   0.13676081597805023,\n",
       "   -0.132113516330719,\n",
       "   -0.22413644194602966,\n",
       "   0.3188752830028534,\n",
       "   -0.5182229280471802,\n",
       "   -0.05533121898770332,\n",
       "   0.229742169380188,\n",
       "   0.09438161551952362,\n",
       "   -0.029835786670446396,\n",
       "   0.08631904423236847,\n",
       "   -0.12409447878599167,\n",
       "   -0.03973880410194397,\n",
       "   -0.08609948307275772,\n",
       "   0.039437610656023026,\n",
       "   -0.04575240612030029,\n",
       "   0.02687549777328968,\n",
       "   -0.4478410482406616,\n",
       "   -0.3651159107685089,\n",
       "   -0.3423393666744232,\n",
       "   -0.2857626676559448,\n",
       "   -0.4527844190597534,\n",
       "   0.16042779386043549,\n",
       "   -0.03481662645936012,\n",
       "   -0.15802183747291565,\n",
       "   -0.003913749009370804,\n",
       "   0.06728502362966537,\n",
       "   0.20450006425380707,\n",
       "   0.14259439706802368,\n",
       "   -0.0248490609228611,\n",
       "   0.156051367521286,\n",
       "   0.02093096822500229,\n",
       "   0.30117878317832947,\n",
       "   0.09652334451675415,\n",
       "   0.4251944124698639,\n",
       "   0.08147688210010529,\n",
       "   -0.14234137535095215,\n",
       "   0.1765812784433365,\n",
       "   0.18009090423583984,\n",
       "   -0.38452619314193726,\n",
       "   -0.025153540074825287,\n",
       "   -0.11571852117776871,\n",
       "   0.03066408447921276,\n",
       "   -0.892703652381897,\n",
       "   0.13132022321224213,\n",
       "   -0.21450108289718628,\n",
       "   0.41365742683410645,\n",
       "   -0.10435262322425842,\n",
       "   0.40742507576942444,\n",
       "   -0.2116006314754486,\n",
       "   0.34140002727508545,\n",
       "   0.21361492574214935,\n",
       "   -0.2016954869031906,\n",
       "   -0.19992481172084808,\n",
       "   0.5722950100898743,\n",
       "   -0.008428975008428097,\n",
       "   0.3151063621044159,\n",
       "   0.38656380772590637,\n",
       "   -0.07625578343868256,\n",
       "   -0.24153487384319305,\n",
       "   -0.011609483510255814,\n",
       "   -0.24754618108272552,\n",
       "   -0.16888128221035004,\n",
       "   -0.13505151867866516,\n",
       "   0.5064759254455566,\n",
       "   0.276621013879776,\n",
       "   0.0792897418141365,\n",
       "   0.22615915536880493,\n",
       "   -0.4253872334957123,\n",
       "   0.2295912653207779,\n",
       "   0.3486517667770386,\n",
       "   -0.18475112318992615,\n",
       "   -0.02507731504738331,\n",
       "   -0.2305079698562622,\n",
       "   0.16532596945762634,\n",
       "   -0.20433539152145386,\n",
       "   0.25575903058052063,\n",
       "   0.5871418714523315,\n",
       "   0.10397350043058395,\n",
       "   0.032402340322732925,\n",
       "   0.24689722061157227,\n",
       "   -0.41678059101104736,\n",
       "   0.7314226627349854,\n",
       "   -0.2583540081977844,\n",
       "   -0.3693712651729584,\n",
       "   -0.3044547736644745,\n",
       "   0.1684078723192215,\n",
       "   0.3246765434741974,\n",
       "   0.10354012995958328,\n",
       "   0.11477282643318176,\n",
       "   -0.28440338373184204,\n",
       "   -0.393137127161026,\n",
       "   0.011565268971025944,\n",
       "   -0.04105687886476517,\n",
       "   0.1618088036775589,\n",
       "   -0.08739492297172546,\n",
       "   0.18896184861660004,\n",
       "   -0.34952613711357117,\n",
       "   0.05521809309720993,\n",
       "   -0.176116481423378,\n",
       "   -0.32698503136634827,\n",
       "   -0.16051147878170013,\n",
       "   0.3765232264995575,\n",
       "   0.5480057597160339,\n",
       "   0.2831480801105499,\n",
       "   0.17984311282634735,\n",
       "   -0.16101036965847015,\n",
       "   -0.5059975981712341,\n",
       "   -0.23875489830970764,\n",
       "   0.25182193517684937,\n",
       "   -0.034148767590522766,\n",
       "   -0.25865644216537476,\n",
       "   0.4247254431247711,\n",
       "   -0.07105764746665955,\n",
       "   0.49106037616729736,\n",
       "   0.057456016540527344,\n",
       "   0.058250125497579575,\n",
       "   0.020327476784586906,\n",
       "   0.1255105584859848,\n",
       "   -0.29210370779037476,\n",
       "   0.35960569977760315,\n",
       "   0.13779863715171814,\n",
       "   -0.19401314854621887,\n",
       "   0.045493751764297485,\n",
       "   0.1334557980298996,\n",
       "   -0.00953626912087202,\n",
       "   0.21172499656677246,\n",
       "   -0.34051206707954407,\n",
       "   -0.6508031487464905,\n",
       "   -0.24383185803890228,\n",
       "   0.11267884075641632,\n",
       "   0.1043790876865387,\n",
       "   -0.32603996992111206,\n",
       "   0.04888714849948883,\n",
       "   -0.42781856656074524,\n",
       "   -0.0832994133234024,\n",
       "   -0.023325009271502495,\n",
       "   -0.030784940347075462,\n",
       "   0.042941026389598846,\n",
       "   -0.47533461451530457,\n",
       "   0.2134474366903305,\n",
       "   -0.12406444549560547,\n",
       "   0.3151215612888336,\n",
       "   0.17109335958957672,\n",
       "   -0.04064946621656418,\n",
       "   0.16870823502540588,\n",
       "   -0.19998516142368317,\n",
       "   0.20386146008968353,\n",
       "   -0.007402495015412569,\n",
       "   0.06613180041313171,\n",
       "   0.6220700740814209,\n",
       "   -0.22548887133598328,\n",
       "   0.010340462438762188,\n",
       "   -0.02764330990612507,\n",
       "   -0.31194815039634705,\n",
       "   -0.04112841561436653,\n",
       "   -0.4198859930038452,\n",
       "   -0.3629027009010315,\n",
       "   -0.02689492143690586,\n",
       "   0.04736214876174927,\n",
       "   -0.3396609127521515,\n",
       "   -0.03498250991106033,\n",
       "   0.054008517414331436,\n",
       "   -0.4679020047187805,\n",
       "   0.03891938179731369,\n",
       "   0.16825568675994873,\n",
       "   0.0652276799082756,\n",
       "   0.03315475955605507,\n",
       "   -0.02389068901538849,\n",
       "   -0.8163591027259827,\n",
       "   0.1024080440402031,\n",
       "   0.05948970839381218,\n",
       "   0.14238454401493073,\n",
       "   0.23671014606952667,\n",
       "   0.10433457046747208,\n",
       "   -0.3486044108867645,\n",
       "   0.10580960661172867,\n",
       "   0.03212348744273186,\n",
       "   -0.035594161599874496,\n",
       "   0.022767402231693268,\n",
       "   -0.277334988117218,\n",
       "   0.03033822402358055,\n",
       "   -0.08208557218313217,\n",
       "   -0.4185677766799927,\n",
       "   -0.34359124302864075,\n",
       "   0.4628032445907593,\n",
       "   0.4831521809101105,\n",
       "   0.11337704956531525,\n",
       "   0.02893654629588127,\n",
       "   0.1454240083694458,\n",
       "   0.004311618860810995,\n",
       "   -0.38771045207977295,\n",
       "   0.29990047216415405,\n",
       "   0.3584800958633423,\n",
       "   -0.2841596305370331,\n",
       "   0.015583035536110401,\n",
       "   -0.2590486705303192,\n",
       "   -0.004991533234715462,\n",
       "   -0.2889528274536133,\n",
       "   -0.04581118002533913,\n",
       "   -0.061145879328250885,\n",
       "   0.15516087412834167,\n",
       "   0.14080394804477692,\n",
       "   0.2753959000110626,\n",
       "   -0.5157513618469238,\n",
       "   0.4342171549797058,\n",
       "   0.377025306224823,\n",
       "   0.015593207441270351,\n",
       "   -0.19382329285144806,\n",
       "   -0.06328605860471725,\n",
       "   -0.1542847901582718,\n",
       "   -0.23986735939979553,\n",
       "   0.42759597301483154,\n",
       "   -0.4105145335197449,\n",
       "   0.20167842507362366,\n",
       "   -0.05898991972208023,\n",
       "   0.08890870958566666,\n",
       "   -0.07808155566453934,\n",
       "   0.25283440947532654,\n",
       "   0.2716306746006012,\n",
       "   0.5206972360610962,\n",
       "   0.18808725476264954,\n",
       "   -0.45106470584869385,\n",
       "   0.06647640466690063,\n",
       "   -0.14869724214076996,\n",
       "   0.03736886382102966,\n",
       "   0.3471032977104187,\n",
       "   0.0906175822019577,\n",
       "   0.6289488077163696,\n",
       "   -0.5643739700317383,\n",
       "   -0.19217689335346222,\n",
       "   0.11113683134317398,\n",
       "   0.5242506265640259,\n",
       "   -0.1074260026216507,\n",
       "   -0.20804749429225922,\n",
       "   -0.1420203596353531,\n",
       "   0.051321789622306824,\n",
       "   -0.20669196546077728,\n",
       "   0.24216553568840027,\n",
       "   0.5033589601516724,\n",
       "   0.2384849488735199,\n",
       "   -0.08631020039319992,\n",
       "   0.38121986389160156,\n",
       "   -0.4680091142654419,\n",
       "   -0.08760605752468109,\n",
       "   0.11819528043270111,\n",
       "   0.31410467624664307,\n",
       "   -0.015903493389487267,\n",
       "   -0.09388595819473267,\n",
       "   -0.22886985540390015,\n",
       "   0.147019624710083,\n",
       "   0.20584861934185028,\n",
       "   0.2829587459564209,\n",
       "   -0.24028393626213074,\n",
       "   -0.08887512236833572,\n",
       "   0.3137507140636444,\n",
       "   0.026497337967157364,\n",
       "   -0.22250550985336304,\n",
       "   0.12060476839542389,\n",
       "   0.19975902140140533,\n",
       "   -0.18518730998039246,\n",
       "   -0.1601897031068802,\n",
       "   -0.41504791378974915,\n",
       "   0.26232191920280457,\n",
       "   0.055130790919065475,\n",
       "   0.17070035636425018,\n",
       "   0.12081270664930344,\n",
       "   -0.4014417827129364,\n",
       "   0.5467467308044434,\n",
       "   0.1870342642068863,\n",
       "   0.3563249409198761,\n",
       "   -0.023669445887207985,\n",
       "   0.1042703166604042,\n",
       "   -0.1753581464290619,\n",
       "   0.08704520761966705,\n",
       "   0.19808968901634216,\n",
       "   0.22477808594703674,\n",
       "   0.16756165027618408,\n",
       "   -0.5818096995353699,\n",
       "   -0.4519619643688202,\n",
       "   -0.3038361966609955,\n",
       "   0.08357914537191391,\n",
       "   0.644950270652771,\n",
       "   -0.8334813117980957,\n",
       "   -0.1882988065481186,\n",
       "   0.03740137442946434,\n",
       "   0.12920226156711578,\n",
       "   -0.005194329656660557,\n",
       "   -0.4262647032737732,\n",
       "   0.12813669443130493,\n",
       "   0.21312624216079712,\n",
       "   0.11222299188375473,\n",
       "   0.0790332481265068,\n",
       "   0.18491582572460175,\n",
       "   0.24816204607486725,\n",
       "   -0.31952711939811707,\n",
       "   0.08504962921142578,\n",
       "   0.09663683921098709,\n",
       "   -0.11551781743764877,\n",
       "   0.6483935713768005,\n",
       "   0.21044166386127472,\n",
       "   -0.2650130093097687,\n",
       "   0.05628197640180588,\n",
       "   0.37427252531051636,\n",
       "   -0.0900411605834961,\n",
       "   -0.13114236295223236,\n",
       "   0.03401816636323929,\n",
       "   -0.0014100692933425307,\n",
       "   -0.6454712748527527,\n",
       "   -0.49064400792121887,\n",
       "   0.0394519679248333,\n",
       "   -0.2663961946964264,\n",
       "   0.16466128826141357,\n",
       "   -0.021850736811757088,\n",
       "   0.04728470742702484,\n",
       "   0.7948008179664612,\n",
       "   -0.17563804984092712,\n",
       "   -0.13631948828697205,\n",
       "   -0.4299534559249878,\n",
       "   -0.1680593192577362,\n",
       "   -0.09924183785915375,\n",
       "   -0.19902756810188293,\n",
       "   -0.23036892712116241,\n",
       "   0.22935184836387634,\n",
       "   -0.2725224494934082,\n",
       "   0.3932367265224457,\n",
       "   0.23371781408786774,\n",
       "   0.3429059684276581,\n",
       "   -0.32414665818214417,\n",
       "   0.19830119609832764,\n",
       "   -0.19419030845165253,\n",
       "   0.15269285440444946,\n",
       "   0.14078134298324585,\n",
       "   0.4601118564605713,\n",
       "   -0.009904446080327034,\n",
       "   0.5689184665679932,\n",
       "   0.013102096505463123,\n",
       "   0.36025482416152954,\n",
       "   0.3600282073020935,\n",
       "   0.009258713573217392,\n",
       "   0.027540650218725204,\n",
       "   -0.4194749891757965,\n",
       "   0.4250412583351135,\n",
       "   0.2421971708536148,\n",
       "   -0.3798334002494812,\n",
       "   -0.14216254651546478,\n",
       "   -0.17126905918121338,\n",
       "   0.30802297592163086,\n",
       "   -0.39227625727653503,\n",
       "   -0.45599493384361267,\n",
       "   -0.021177833899855614,\n",
       "   -0.3390550911426544,\n",
       "   0.2278284877538681,\n",
       "   0.17317569255828857,\n",
       "   -0.05403801053762436,\n",
       "   0.01192003209143877,\n",
       "   -0.27222761511802673,\n",
       "   -0.16716361045837402,\n",
       "   -0.10849043726921082,\n",
       "   0.17502062022686005,\n",
       "   -0.2726094722747803,\n",
       "   -0.12625864148139954,\n",
       "   -0.28978589177131653,\n",
       "   -0.16771255433559418,\n",
       "   0.3339795768260956,\n",
       "   0.24075177311897278,\n",
       "   -0.2273174375295639,\n",
       "   -0.3267613351345062,\n",
       "   0.06564608216285706,\n",
       "   -0.09753507375717163,\n",
       "   0.3558506667613983,\n",
       "   -0.005295571405440569,\n",
       "   -0.20506501197814941,\n",
       "   -0.331449031829834,\n",
       "   0.4007315933704376,\n",
       "   0.40134233236312866,\n",
       "   -0.19581469893455505,\n",
       "   0.050530072301626205,\n",
       "   0.08564864099025726,\n",
       "   -0.19177661836147308,\n",
       "   0.0412958487868309,\n",
       "   -0.017527559772133827,\n",
       "   0.22203421592712402,\n",
       "   0.24723514914512634,\n",
       "   -0.14993669092655182,\n",
       "   0.122634656727314,\n",
       "   -0.35518157482147217,\n",
       "   -0.19171178340911865,\n",
       "   -0.4368215501308441,\n",
       "   -0.2659625709056854,\n",
       "   0.20161356031894684,\n",
       "   0.30641353130340576,\n",
       "   -0.25068631768226624,\n",
       "   -0.23162661492824554,\n",
       "   0.07353164255619049,\n",
       "   0.38967975974082947,\n",
       "   -0.24919964373111725,\n",
       "   0.1291109323501587,\n",
       "   -0.15384966135025024,\n",
       "   0.18101705610752106,\n",
       "   0.42398178577423096,\n",
       "   -0.305717408657074,\n",
       "   -0.31334400177001953,\n",
       "   -0.037434037774801254,\n",
       "   -0.20662258565425873,\n",
       "   -0.36608049273490906,\n",
       "   0.3636649250984192,\n",
       "   0.20085978507995605,\n",
       "   0.11704520136117935,\n",
       "   0.005279103294014931,\n",
       "   -0.3777419924736023,\n",
       "   0.2642150819301605,\n",
       "   0.5350056290626526,\n",
       "   -0.051678091287612915,\n",
       "   0.07734835147857666,\n",
       "   0.4077048897743225,\n",
       "   0.17531907558441162,\n",
       "   0.5437820553779602,\n",
       "   0.46091175079345703,\n",
       "   0.11989130079746246,\n",
       "   -0.035992708057165146,\n",
       "   -0.21057865023612976,\n",
       "   0.37365204095840454,\n",
       "   -0.6677941679954529,\n",
       "   0.1632324606180191,\n",
       "   0.07735435664653778,\n",
       "   -0.18887194991111755,\n",
       "   -0.11793297529220581,\n",
       "   -0.2714844346046448,\n",
       "   0.19119514524936676,\n",
       "   0.12069341540336609,\n",
       "   0.25791892409324646,\n",
       "   -0.001898844726383686,\n",
       "   0.09523706138134003,\n",
       "   0.1053151786327362,\n",
       "   -0.2882351279258728,\n",
       "   -0.12673963606357574,\n",
       "   0.21780334413051605,\n",
       "   0.3007877469062805,\n",
       "   -0.14732541143894196,\n",
       "   -0.0955396518111229,\n",
       "   0.08822419494390488,\n",
       "   0.23567669093608856,\n",
       "   -0.2652350962162018,\n",
       "   -0.04557659849524498,\n",
       "   -0.29732581973075867,\n",
       "   0.026095906272530556,\n",
       "   -0.007501984015107155,\n",
       "   -0.10835926234722137,\n",
       "   -0.2948361933231354,\n",
       "   0.6132366061210632,\n",
       "   0.07140572369098663,\n",
       "   -0.2508571445941925,\n",
       "   0.07564736157655716,\n",
       "   -0.1265711486339569,\n",
       "   -0.046423058956861496,\n",
       "   -0.3950931131839752,\n",
       "   0.0316442996263504,\n",
       "   -0.46707797050476074,\n",
       "   0.10601810365915298,\n",
       "   -0.21391114592552185,\n",
       "   -0.2483634054660797,\n",
       "   0.09532598406076431,\n",
       "   -0.04805510491132736,\n",
       "   0.5193482041358948,\n",
       "   0.5089400410652161,\n",
       "   -0.05451721325516701,\n",
       "   0.06415525823831558,\n",
       "   -0.16058501601219177,\n",
       "   0.6895288825035095,\n",
       "   -0.3064870834350586,\n",
       "   0.20508398115634918,\n",
       "   0.15800777077674866,\n",
       "   -0.0715644359588623,\n",
       "   -0.03594721481204033,\n",
       "   -0.0648900493979454,\n",
       "   0.12770305573940277,\n",
       "   0.1449722796678543,\n",
       "   -0.03592345863580704,\n",
       "   0.5531870126724243,\n",
       "   0.006970671005547047,\n",
       "   -0.18992458283901215,\n",
       "   -0.05781819671392441,\n",
       "   -0.7433051466941833,\n",
       "   -0.07452146708965302,\n",
       "   0.16472244262695312,\n",
       "   0.008281945250928402,\n",
       "   0.05042371153831482,\n",
       "   0.22148437798023224,\n",
       "   -0.012744612991809845,\n",
       "   -0.03967199847102165,\n",
       "   0.013196469284594059,\n",
       "   0.16274109482765198,\n",
       "   0.37912020087242126,\n",
       "   0.06910312920808792,\n",
       "   0.16025978326797485,\n",
       "   -0.00771908275783062,\n",
       "   -0.17578276991844177,\n",
       "   -0.014500479213893414,\n",
       "   -0.07659498602151871,\n",
       "   0.20302964746952057,\n",
       "   -0.5195913910865784,\n",
       "   -9.118025779724121,\n",
       "   0.2834787964820862,\n",
       "   -0.30704668164253235,\n",
       "   0.4747934937477112,\n",
       "   -0.35564303398132324,\n",
       "   0.13476471602916718,\n",
       "   0.3029322028160095,\n",
       "   -0.04679166153073311,\n",
       "   -0.16644926369190216,\n",
       "   -0.09476208686828613,\n",
       "   0.12792891263961792,\n",
       "   -0.0725255161523819,\n",
       "   -0.02597508206963539,\n",
       "   -0.006741605699062347,\n",
       "   0.1786550134420395,\n",
       "   -0.09961652755737305,\n",
       "   0.19130922853946686,\n",
       "   -0.13887104392051697,\n",
       "   -0.27167418599128723,\n",
       "   -0.00967749860137701,\n",
       "   -0.045785002410411835,\n",
       "   -0.3350381851196289,\n",
       "   -0.20709805190563202,\n",
       "   0.830843985080719,\n",
       "   0.0309867225587368,\n",
       "   0.2872360646724701,\n",
       "   0.07659103721380234,\n",
       "   -0.2223944216966629,\n",
       "   -0.11557994037866592,\n",
       "   -0.07058997452259064,\n",
       "   -0.052358679473400116,\n",
       "   0.15311327576637268,\n",
       "   -0.14616015553474426,\n",
       "   0.2315065860748291,\n",
       "   0.06975055485963821,\n",
       "   -0.16529960930347443,\n",
       "   -0.09772960096597672,\n",
       "   0.1797567456960678,\n",
       "   -0.01980629190802574,\n",
       "   -0.3082617223262787,\n",
       "   0.20657813549041748,\n",
       "   0.03182326629757881,\n",
       "   -0.07340353727340698,\n",
       "   0.4481484889984131,\n",
       "   -0.06679195910692215,\n",
       "   0.058871302753686905,\n",
       "   0.09893351793289185,\n",
       "   -0.10483076423406601,\n",
       "   0.014865518547594547,\n",
       "   0.03466968610882759,\n",
       "   0.5009471774101257,\n",
       "   0.2217055708169937,\n",
       "   0.6296201944351196,\n",
       "   0.35476377606391907,\n",
       "   0.10546618700027466,\n",
       "   0.20796845853328705,\n",
       "   0.16451574862003326,\n",
       "   0.43887415528297424,\n",
       "   -0.2757894694805145,\n",
       "   -0.6501739621162415,\n",
       "   0.031138572841882706,\n",
       "   -0.010248025879263878,\n",
       "   0.39518705010414124,\n",
       "   0.05085255950689316,\n",
       "   -0.2547388970851898,\n",
       "   0.22206717729568481,\n",
       "   -0.36587101221084595,\n",
       "   -0.0798880085349083,\n",
       "   -0.00716976635158062,\n",
       "   -0.25560325384140015,\n",
       "   -0.5023272037506104,\n",
       "   -0.394113153219223,\n",
       "   0.26623237133026123,\n",
       "   -0.24248850345611572,\n",
       "   0.07840696722269058,\n",
       "   -0.11499214172363281,\n",
       "   -0.045509107410907745,\n",
       "   0.27304497361183167,\n",
       "   0.11502497643232346,\n",
       "   0.3397876024246216,\n",
       "   -0.6479330658912659,\n",
       "   0.22305575013160706,\n",
       "   0.4331167936325073,\n",
       "   0.03172299638390541,\n",
       "   0.33025696873664856,\n",
       "   -0.20069432258605957,\n",
       "   -0.2277631014585495,\n",
       "   0.030992913991212845,\n",
       "   -0.05551433190703392,\n",
       "   0.3387663960456848,\n",
       "   -0.3440079987049103,\n",
       "   0.18899260461330414,\n",
       "   -0.056187476962804794,\n",
       "   0.22342577576637268,\n",
       "   -0.3041534721851349,\n",
       "   0.38474133610725403,\n",
       "   -0.09401191771030426,\n",
       "   -0.08709023892879486,\n",
       "   -0.012767883948981762,\n",
       "   0.13414330780506134,\n",
       "   0.2561575472354889,\n",
       "   -0.07115154713392258,\n",
       "   0.0955146923661232,\n",
       "   -0.520342230796814,\n",
       "   -0.4847775399684906,\n",
       "   -0.03024502471089363,\n",
       "   0.022204119712114334,\n",
       "   -0.052354421466588974,\n",
       "   0.24888299405574799,\n",
       "   0.3535902798175812,\n",
       "   -0.18211613595485687,\n",
       "   0.060221560299396515,\n",
       "   0.052520204335451126,\n",
       "   0.0021436656825244427,\n",
       "   0.18971757590770721,\n",
       "   0.28272247314453125,\n",
       "   -0.008366134949028492,\n",
       "   -0.4214584529399872,\n",
       "   -0.006511058658361435,\n",
       "   -0.23788097500801086,\n",
       "   0.09491853415966034,\n",
       "   -0.7180008292198181,\n",
       "   0.11016760766506195,\n",
       "   -0.3432792127132416,\n",
       "   -0.056520748883485794,\n",
       "   0.04703870788216591,\n",
       "   0.0064995321445167065,\n",
       "   0.08746594190597534,\n",
       "   0.1079571545124054,\n",
       "   0.036016445606946945,\n",
       "   0.31577378511428833,\n",
       "   0.3419632613658905,\n",
       "   0.556753933429718,\n",
       "   -0.15163272619247437,\n",
       "   0.1298999786376953,\n",
       "   -0.07308980822563171,\n",
       "   0.1593717485666275,\n",
       "   -0.08595287799835205,\n",
       "   -0.19974102079868317,\n",
       "   0.16645187139511108,\n",
       "   -0.5543858408927917,\n",
       "   -0.040412381291389465,\n",
       "   0.16396953165531158,\n",
       "   0.19336749613285065,\n",
       "   0.3577405512332916,\n",
       "   -0.17118948698043823,\n",
       "   -0.34950441122055054,\n",
       "   0.35263165831565857,\n",
       "   0.03847851604223251,\n",
       "   -0.16705146431922913,\n",
       "   -0.25239256024360657,\n",
       "   0.11041714251041412,\n",
       "   0.2670477330684662,\n",
       "   6.822514114901423e-05,\n",
       "   0.2974708676338196,\n",
       "   -0.346294641494751,\n",
       "   -0.1728086620569229,\n",
       "   0.34860366582870483,\n",
       "   -0.04497027024626732,\n",
       "   0.06309552490711212,\n",
       "   -0.3987106382846832,\n",
       "   -0.08995211124420166,\n",
       "   0.055362652987241745,\n",
       "   -0.5251456499099731,\n",
       "   -0.21295469999313354,\n",
       "   -0.07076621055603027,\n",
       "   -0.28760653734207153,\n",
       "   -0.4250190854072571,\n",
       "   0.13233064115047455,\n",
       "   0.179702028632164,\n",
       "   0.3321031928062439,\n",
       "   0.01939276047050953,\n",
       "   0.3076454997062683,\n",
       "   -0.049908433109521866,\n",
       "   0.18875771760940552,\n",
       "   -0.05908414348959923,\n",
       "   -0.0632721483707428,\n",
       "   -0.32187172770500183,\n",
       "   0.31719163060188293,\n",
       "   -0.06633512675762177,\n",
       "   0.09714178740978241,\n",
       "   0.6615350246429443,\n",
       "   0.07924534380435944,\n",
       "   -0.4919222891330719,\n",
       "   -0.33060479164123535,\n",
       "   0.2516065537929535,\n",
       "   0.5288068652153015,\n",
       "   0.19661372900009155,\n",
       "   -0.10171951353549957,\n",
       "   0.005397482309490442,\n",
       "   0.3191368281841278,\n",
       "   0.2212258279323578,\n",
       "   -0.08569305390119553,\n",
       "   -0.0534527413547039,\n",
       "   0.17569567263126373,\n",
       "   0.1680649071931839,\n",
       "   0.11552277952432632,\n",
       "   0.47911950945854187,\n",
       "   -0.2773286700248718,\n",
       "   -0.20398227870464325,\n",
       "   -0.24475005269050598,\n",
       "   0.4472593069076538,\n",
       "   0.0599195770919323,\n",
       "   0.14667078852653503,\n",
       "   -0.022956402972340584,\n",
       "   -0.015726929530501366,\n",
       "   -0.11855368316173553,\n",
       "   0.0561174675822258,\n",
       "   -0.025354726240038872,\n",
       "   0.16087903082370758,\n",
       "   0.2354309856891632],\n",
       "  [0.43803054094314575,\n",
       "   0.21090012788772583,\n",
       "   -0.052670300006866455,\n",
       "   -0.12622497975826263,\n",
       "   -0.0587453730404377,\n",
       "   0.05492731183767319,\n",
       "   0.3202914595603943,\n",
       "   0.19914403557777405,\n",
       "   -0.17336879670619965,\n",
       "   -0.0004859263135585934,\n",
       "   -0.1617426574230194,\n",
       "   -0.09440388530492783,\n",
       "   -0.1904529631137848,\n",
       "   0.22495293617248535,\n",
       "   -0.36981359124183655,\n",
       "   -0.02301810309290886,\n",
       "   0.31124648451805115,\n",
       "   -0.04847889766097069,\n",
       "   0.05093804746866226,\n",
       "   -0.03317893296480179,\n",
       "   -0.246631920337677,\n",
       "   -0.1801820993423462,\n",
       "   -0.36802783608436584,\n",
       "   0.19830165803432465,\n",
       "   -0.24989865720272064,\n",
       "   -0.03530983626842499,\n",
       "   0.25435951352119446,\n",
       "   0.24462004005908966,\n",
       "   -0.0007477484759874642,\n",
       "   0.5119883418083191,\n",
       "   -0.12161318212747574,\n",
       "   -0.345014750957489,\n",
       "   0.23305505514144897,\n",
       "   0.14886035025119781,\n",
       "   0.2391655147075653,\n",
       "   -0.049969594925642014,\n",
       "   -0.07122362405061722,\n",
       "   0.07648015022277832,\n",
       "   -0.20264527201652527,\n",
       "   0.236930251121521,\n",
       "   0.01270201150327921,\n",
       "   -0.021763933822512627,\n",
       "   0.10153070837259293,\n",
       "   0.06759725511074066,\n",
       "   0.6030603051185608,\n",
       "   -0.29588207602500916,\n",
       "   -0.10586987435817719,\n",
       "   -0.007552333176136017,\n",
       "   -0.18611659109592438,\n",
       "   0.19917286932468414,\n",
       "   -0.2550404369831085,\n",
       "   0.07514561712741852,\n",
       "   -0.36273837089538574,\n",
       "   -0.009144925512373447,\n",
       "   0.17082470655441284,\n",
       "   -0.17435845732688904,\n",
       "   0.1461924910545349,\n",
       "   -0.04109581559896469,\n",
       "   -0.2135714292526245,\n",
       "   0.054613083600997925,\n",
       "   -0.27208852767944336,\n",
       "   0.13409702479839325,\n",
       "   0.2232133448123932,\n",
       "   0.031715940684080124,\n",
       "   -0.2791729271411896,\n",
       "   -0.2093934714794159,\n",
       "   -0.007777530234307051,\n",
       "   0.20247025787830353,\n",
       "   -0.1975710093975067,\n",
       "   -0.04267430678009987,\n",
       "   -0.11777600646018982,\n",
       "   -0.1332426816225052,\n",
       "   -0.032353345304727554,\n",
       "   0.01839892752468586,\n",
       "   0.21759504079818726,\n",
       "   -0.30084365606307983,\n",
       "   0.1371459811925888,\n",
       "   -0.19882197678089142,\n",
       "   0.050969257950782776,\n",
       "   0.09462490677833557,\n",
       "   0.3047657907009125,\n",
       "   0.3393491804599762,\n",
       "   -0.019479941576719284,\n",
       "   -0.07364912331104279,\n",
       "   -0.05286870151758194,\n",
       "   -0.12930628657341003,\n",
       "   -0.004092266317456961,\n",
       "   0.13941454887390137,\n",
       "   0.21452583372592926,\n",
       "   0.17876935005187988,\n",
       "   0.20739364624023438,\n",
       "   -0.2784980833530426,\n",
       "   0.01780707761645317,\n",
       "   -0.15293125808238983,\n",
       "   -0.41025349497795105,\n",
       "   -0.3874669075012207,\n",
       "   -0.04002249240875244,\n",
       "   -0.10330251604318619,\n",
       "   -0.08789043128490448,\n",
       "   0.2192864567041397,\n",
       "   0.16402190923690796,\n",
       "   -0.0674460157752037,\n",
       "   -0.036652203649282455,\n",
       "   -0.032389335334300995,\n",
       "   0.0648437887430191,\n",
       "   0.37366151809692383,\n",
       "   0.01925053820014,\n",
       "   -0.4348295032978058,\n",
       "   -0.27180027961730957,\n",
       "   0.48058047890663147,\n",
       "   0.29454243183135986,\n",
       "   -0.15215329825878143,\n",
       "   0.25192198157310486,\n",
       "   0.07214260846376419,\n",
       "   -0.06667071580886841,\n",
       "   -0.10644876956939697,\n",
       "   -0.2267446666955948,\n",
       "   -0.04689245671033859,\n",
       "   -0.16510498523712158,\n",
       "   0.006545408628880978,\n",
       "   0.4208675026893616,\n",
       "   0.14052598178386688,\n",
       "   0.3808513283729553,\n",
       "   0.045475032180547714,\n",
       "   0.28689002990722656,\n",
       "   0.1973133087158203,\n",
       "   0.0045743524096906185,\n",
       "   -0.1515762060880661,\n",
       "   -0.13122980296611786,\n",
       "   0.16861288249492645,\n",
       "   -0.39454543590545654,\n",
       "   -0.08477791398763657,\n",
       "   0.23227310180664062,\n",
       "   0.02044890634715557,\n",
       "   0.039276912808418274,\n",
       "   0.06817251443862915,\n",
       "   -0.5179181694984436,\n",
       "   0.332127183675766,\n",
       "   -0.8057743906974792,\n",
       "   -0.015311635099351406,\n",
       "   -0.05798092111945152,\n",
       "   0.15777936577796936,\n",
       "   0.22208169102668762,\n",
       "   -0.008002516813576221,\n",
       "   -0.00823051854968071,\n",
       "   -0.16372476518154144,\n",
       "   -0.4570913016796112,\n",
       "   -0.2643117606639862,\n",
       "   0.3087524473667145,\n",
       "   0.18788672983646393,\n",
       "   -0.4048819839954376,\n",
       "   -0.06350365281105042,\n",
       "   -0.03999936208128929,\n",
       "   0.19185827672481537,\n",
       "   -0.0395023338496685,\n",
       "   0.031954552978277206,\n",
       "   -0.08923612534999847,\n",
       "   0.16681410372257233,\n",
       "   0.3545747995376587,\n",
       "   0.18125887215137482,\n",
       "   -0.142375648021698,\n",
       "   -0.43860089778900146,\n",
       "   0.027753328904509544,\n",
       "   -0.06387415528297424,\n",
       "   0.05645051971077919,\n",
       "   0.3692861497402191,\n",
       "   -0.27953463792800903,\n",
       "   0.06912173330783844,\n",
       "   0.1424751579761505,\n",
       "   0.19478251039981842,\n",
       "   0.30899348855018616,\n",
       "   -0.0511724129319191,\n",
       "   -0.16798652708530426,\n",
       "   0.18936994671821594,\n",
       "   -0.10407883673906326,\n",
       "   0.20873069763183594,\n",
       "   0.02473302371799946,\n",
       "   -0.3882734775543213,\n",
       "   -0.035601213574409485,\n",
       "   0.15831774473190308,\n",
       "   0.1632518321275711,\n",
       "   -0.01150454580783844,\n",
       "   -0.055208515375852585,\n",
       "   -0.36992210149765015,\n",
       "   -0.10951171070337296,\n",
       "   -0.36167246103286743,\n",
       "   0.20348672568798065,\n",
       "   -0.3994659185409546,\n",
       "   -0.0022199617233127356,\n",
       "   -0.20102640986442566,\n",
       "   0.032750777900218964,\n",
       "   -0.0298460740596056,\n",
       "   -0.11224480718374252,\n",
       "   0.03374485298991203,\n",
       "   -0.35441216826438904,\n",
       "   -0.03676128387451172,\n",
       "   0.06937776505947113,\n",
       "   0.3444732427597046,\n",
       "   0.2585935890674591,\n",
       "   0.1503911316394806,\n",
       "   -0.34424495697021484,\n",
       "   -0.08351191133260727,\n",
       "   0.17500165104866028,\n",
       "   -0.007753719110041857,\n",
       "   0.11199816316366196,\n",
       "   0.2038855105638504,\n",
       "   0.01010403223335743,\n",
       "   -0.05796494334936142,\n",
       "   -0.08703465759754181,\n",
       "   -0.11421127617359161,\n",
       "   0.29567885398864746,\n",
       "   -0.1011892557144165,\n",
       "   0.09944598376750946,\n",
       "   0.2533285915851593,\n",
       "   -0.11213875561952591,\n",
       "   -0.4262564778327942,\n",
       "   0.1972171664237976,\n",
       "   -0.25105929374694824,\n",
       "   -0.3816451132297516,\n",
       "   -0.20430408418178558,\n",
       "   0.24328714609146118,\n",
       "   -0.11683887243270874,\n",
       "   0.4173732101917267,\n",
       "   -0.15499423444271088,\n",
       "   -0.37581467628479004,\n",
       "   0.21921193599700928,\n",
       "   0.31887757778167725,\n",
       "   0.4073413014411926,\n",
       "   0.17026251554489136,\n",
       "   -0.061883628368377686,\n",
       "   0.18801653385162354,\n",
       "   0.1330161690711975,\n",
       "   0.08371787518262863,\n",
       "   -0.31158584356307983,\n",
       "   0.15769708156585693,\n",
       "   -0.39442694187164307,\n",
       "   -0.18282149732112885,\n",
       "   0.10773361474275589,\n",
       "   -0.30379268527030945,\n",
       "   -0.17745748162269592,\n",
       "   0.25191447138786316,\n",
       "   0.20117564499378204,\n",
       "   0.0611024834215641,\n",
       "   -0.022105103358626366,\n",
       "   -0.18423794209957123,\n",
       "   -0.010329377837479115,\n",
       "   -0.2145979404449463,\n",
       "   0.4501461982727051,\n",
       "   0.779100239276886,\n",
       "   0.04754435643553734,\n",
       "   -0.037920158356428146,\n",
       "   -0.4102310538291931,\n",
       "   0.28360632061958313,\n",
       "   -0.30398592352867126,\n",
       "   0.12932896614074707,\n",
       "   -0.08981708437204361,\n",
       "   0.29589203000068665,\n",
       "   -0.050627779215574265,\n",
       "   0.10319355130195618,\n",
       "   -0.12104983627796173,\n",
       "   0.2321396768093109,\n",
       "   0.16863131523132324,\n",
       "   0.21620626747608185,\n",
       "   0.23430496454238892,\n",
       "   -0.14859162271022797,\n",
       "   -0.3467663526535034,\n",
       "   -0.004561343230307102,\n",
       "   0.08379576355218887,\n",
       "   -0.35075628757476807,\n",
       "   -0.19479340314865112,\n",
       "   -0.06932701170444489,\n",
       "   -0.11023768037557602,\n",
       "   0.18135792016983032,\n",
       "   0.4403136074542999,\n",
       "   0.321720153093338,\n",
       "   0.008397803641855717,\n",
       "   0.30616506934165955,\n",
       "   -0.4940996468067169,\n",
       "   0.035513460636138916,\n",
       "   -0.3165452480316162,\n",
       "   0.11276310682296753,\n",
       "   0.03652314469218254,\n",
       "   0.1899537742137909,\n",
       "   0.4397031366825104,\n",
       "   -0.23830081522464752,\n",
       "   -0.20870964229106903,\n",
       "   0.15968313813209534,\n",
       "   0.11831551790237427,\n",
       "   0.04039251059293747,\n",
       "   -0.21522095799446106,\n",
       "   0.02847954072058201,\n",
       "   0.18114115297794342,\n",
       "   -0.24297644197940826,\n",
       "   0.031362757086753845,\n",
       "   0.29554328322410583,\n",
       "   0.16462883353233337,\n",
       "   -0.11995438486337662,\n",
       "   0.09964790940284729,\n",
       "   -0.025733578950166702,\n",
       "   -0.1690925657749176,\n",
       "   0.037822239100933075,\n",
       "   0.005222296807914972,\n",
       "   0.026718564331531525,\n",
       "   -0.05419595539569855,\n",
       "   -0.26980113983154297,\n",
       "   -0.0639929473400116,\n",
       "   0.0062893289141356945,\n",
       "   0.3596094250679016,\n",
       "   0.1883702576160431,\n",
       "   0.1677594929933548,\n",
       "   0.048630233854055405,\n",
       "   -0.09324594587087631,\n",
       "   -0.07806335389614105,\n",
       "   -0.05535050481557846,\n",
       "   -0.2879193127155304,\n",
       "   -0.33686789870262146,\n",
       "   -0.42951855063438416,\n",
       "   -0.14738121628761292,\n",
       "   -0.14600512385368347,\n",
       "   -0.01882578618824482,\n",
       "   0.29057371616363525,\n",
       "   0.10377653688192368,\n",
       "   -0.41385868191719055,\n",
       "   0.4599873423576355,\n",
       "   0.015807855874300003,\n",
       "   0.425769567489624,\n",
       "   -0.22173815965652466,\n",
       "   0.2576161324977875,\n",
       "   0.1432325839996338,\n",
       "   -0.16359524428844452,\n",
       "   -0.3165925145149231,\n",
       "   -0.1656169593334198,\n",
       "   0.16619223356246948,\n",
       "   0.04196161776781082,\n",
       "   -0.41187939047813416,\n",
       "   -0.2760026752948761,\n",
       "   0.17035026848316193,\n",
       "   0.6534948348999023,\n",
       "   -0.6824848651885986,\n",
       "   -0.1354246288537979,\n",
       "   0.08721146732568741,\n",
       "   0.2712564766407013,\n",
       "   0.20926228165626526,\n",
       "   -0.12083926796913147,\n",
       "   -0.15765975415706635,\n",
       "   0.032328806817531586,\n",
       "   0.2601531445980072,\n",
       "   0.0507502555847168,\n",
       "   0.08390211313962936,\n",
       "   0.01432864647358656,\n",
       "   -0.21115648746490479,\n",
       "   -0.1329633593559265,\n",
       "   0.02965456061065197,\n",
       "   -0.3067527115345001,\n",
       "   0.35032132267951965,\n",
       "   -0.13680601119995117,\n",
       "   -0.003611211432144046,\n",
       "   -0.12180398404598236,\n",
       "   0.1505785435438156,\n",
       "   0.11526479572057724,\n",
       "   0.2967049181461334,\n",
       "   -0.15221065282821655,\n",
       "   -0.20553642511367798,\n",
       "   -0.06789252161979675,\n",
       "   -0.11760518699884415,\n",
       "   0.0012069896329194307,\n",
       "   0.026814835146069527,\n",
       "   0.6970223784446716,\n",
       "   -0.1929619163274765,\n",
       "   -0.20108982920646667,\n",
       "   0.5711534023284912,\n",
       "   0.3741534650325775,\n",
       "   -0.035538200289011,\n",
       "   -0.2838602066040039,\n",
       "   0.13142073154449463,\n",
       "   -0.22025074064731598,\n",
       "   -0.06158115714788437,\n",
       "   0.007960991002619267,\n",
       "   -0.2794680893421173,\n",
       "   -0.27277326583862305,\n",
       "   -0.001176923862658441,\n",
       "   0.24786236882209778,\n",
       "   0.2142656445503235,\n",
       "   -0.18941833078861237,\n",
       "   0.05922739580273628,\n",
       "   0.10297969728708267,\n",
       "   -0.07828820496797562,\n",
       "   -0.06953851133584976,\n",
       "   0.07679564505815506,\n",
       "   -0.0024041703436523676,\n",
       "   -0.44323423504829407,\n",
       "   0.08208072185516357,\n",
       "   0.3659496605396271,\n",
       "   -0.05342380702495575,\n",
       "   -0.13028523325920105,\n",
       "   -0.05336294695734978,\n",
       "   -0.40387293696403503,\n",
       "   -0.0548386387526989,\n",
       "   0.18836116790771484,\n",
       "   -0.3929038643836975,\n",
       "   0.05019310489296913,\n",
       "   0.05341631919145584,\n",
       "   0.02955438941717148,\n",
       "   -0.11488299816846848,\n",
       "   -0.2406916618347168,\n",
       "   -0.4219628572463989,\n",
       "   0.04130982235074043,\n",
       "   -0.04805644601583481,\n",
       "   0.07495483756065369,\n",
       "   0.2477075159549713,\n",
       "   -0.18263272941112518,\n",
       "   -0.24363777041435242,\n",
       "   -0.13797718286514282,\n",
       "   -0.5324180126190186,\n",
       "   -0.0017110061598941684,\n",
       "   -0.4782509207725525,\n",
       "   0.0704684630036354,\n",
       "   -0.1792392134666443,\n",
       "   0.13301286101341248,\n",
       "   0.14417530596256256,\n",
       "   0.19596053659915924,\n",
       "   0.28581398725509644,\n",
       "   0.25897926092147827,\n",
       "   -0.40216025710105896,\n",
       "   -0.12183146178722382,\n",
       "   0.3362339437007904,\n",
       "   0.17743811011314392,\n",
       "   -0.10306461900472641,\n",
       "   -0.04938973858952522,\n",
       "   0.12972894310951233,\n",
       "   -0.0729832872748375,\n",
       "   0.01808268018066883,\n",
       "   -0.12281989306211472,\n",
       "   0.05954286828637123,\n",
       "   -0.2724786102771759,\n",
       "   0.15533792972564697,\n",
       "   0.13148336112499237,\n",
       "   0.07626082003116608,\n",
       "   0.06830299645662308,\n",
       "   0.14919281005859375,\n",
       "   -0.08339209109544754,\n",
       "   -0.3364960551261902,\n",
       "   0.03880219906568527,\n",
       "   -0.15943774580955505,\n",
       "   0.1588001400232315,\n",
       "   -0.04620666801929474,\n",
       "   0.24676701426506042,\n",
       "   0.07561302185058594,\n",
       "   0.1830638349056244,\n",
       "   -0.0384584479033947,\n",
       "   0.07522769272327423,\n",
       "   -0.4264799654483795,\n",
       "   0.5815995335578918,\n",
       "   -0.2399565577507019,\n",
       "   0.48084309697151184,\n",
       "   -0.2913372814655304,\n",
       "   -0.20187871158123016,\n",
       "   -0.12091854214668274,\n",
       "   -0.22188164293766022,\n",
       "   -0.033800024539232254,\n",
       "   -0.329906702041626,\n",
       "   0.11025439947843552,\n",
       "   0.11386315524578094,\n",
       "   0.03980275243520737,\n",
       "   0.005319567862898111,\n",
       "   0.13600914180278778,\n",
       "   0.6356148719787598,\n",
       "   0.09758821129798889,\n",
       "   0.00984966941177845,\n",
       "   0.055007003247737885,\n",
       "   0.05220886319875717,\n",
       "   0.24203166365623474,\n",
       "   0.30488109588623047,\n",
       "   0.48102524876594543,\n",
       "   0.3098188638687134,\n",
       "   -0.3901365399360657,\n",
       "   -0.12614569067955017,\n",
       "   0.13722240924835205,\n",
       "   -0.32691049575805664,\n",
       "   0.18290270864963531,\n",
       "   0.17920419573783875,\n",
       "   0.08139214664697647,\n",
       "   -0.3141725957393646,\n",
       "   0.09027041494846344,\n",
       "   0.3150785267353058,\n",
       "   0.132659450173378,\n",
       "   0.17703309655189514,\n",
       "   0.07696738839149475,\n",
       "   0.08245833963155746,\n",
       "   -0.023447396233677864,\n",
       "   -0.19062840938568115,\n",
       "   0.23024237155914307,\n",
       "   0.32595133781433105,\n",
       "   0.132023423910141,\n",
       "   0.25641119480133057,\n",
       "   -0.011144150979816914,\n",
       "   -0.08377422392368317,\n",
       "   -0.0597517304122448,\n",
       "   0.04745246097445488,\n",
       "   0.2101922333240509,\n",
       "   0.07621148973703384,\n",
       "   0.17983275651931763,\n",
       "   0.11683444678783417,\n",
       "   0.2646828889846802,\n",
       "   -0.08653188496828079,\n",
       "   0.45000118017196655,\n",
       "   0.2586764097213745,\n",
       "   0.001734959427267313,\n",
       "   0.07030867785215378,\n",
       "   -0.08616568148136139,\n",
       "   0.08902283012866974,\n",
       "   -0.4497082829475403,\n",
       "   -0.03960373252630234,\n",
       "   -0.29388684034347534,\n",
       "   -0.05764014646410942,\n",
       "   -0.278935968875885,\n",
       "   0.17882350087165833,\n",
       "   0.11600153893232346,\n",
       "   -0.15016455948352814,\n",
       "   0.3212963938713074,\n",
       "   0.03059200756251812,\n",
       "   0.320110023021698,\n",
       "   -0.2542576789855957,\n",
       "   -0.32755714654922485,\n",
       "   0.1803494691848755,\n",
       "   -0.3381635844707489,\n",
       "   0.2838672995567322,\n",
       "   -0.23182281851768494,\n",
       "   -0.19019348919391632,\n",
       "   0.2431991845369339,\n",
       "   0.0692070871591568,\n",
       "   0.24478957056999207,\n",
       "   0.44464626908302307,\n",
       "   -0.251756489276886,\n",
       "   0.2790307402610779,\n",
       "   -0.07325560599565506,\n",
       "   -0.02892381139099598,\n",
       "   0.09787856787443161,\n",
       "   -0.10459639132022858,\n",
       "   -0.19441735744476318,\n",
       "   0.2514323890209198,\n",
       "   -0.12311820685863495,\n",
       "   0.08852311968803406,\n",
       "   -0.030952494591474533,\n",
       "   -0.39934101700782776,\n",
       "   0.4047296643257141,\n",
       "   0.12288521230220795,\n",
       "   0.4595267176628113,\n",
       "   0.447619765996933,\n",
       "   0.2081274688243866,\n",
       "   0.046648528426885605,\n",
       "   -0.19744464755058289,\n",
       "   -0.4105342626571655,\n",
       "   0.1221255287528038,\n",
       "   0.16474933922290802,\n",
       "   0.14503292739391327,\n",
       "   -0.3445427417755127,\n",
       "   -9.322747230529785,\n",
       "   0.24078287184238434,\n",
       "   -0.4447128176689148,\n",
       "   0.395641952753067,\n",
       "   -0.38408342003822327,\n",
       "   0.03649864345788956,\n",
       "   -0.05595579370856285,\n",
       "   -0.03218700736761093,\n",
       "   -0.03099435195326805,\n",
       "   0.10308792442083359,\n",
       "   0.09320300817489624,\n",
       "   0.1578974574804306,\n",
       "   0.20670728385448456,\n",
       "   0.01684349961578846,\n",
       "   -0.12958000600337982,\n",
       "   -0.10823725163936615,\n",
       "   0.29996126890182495,\n",
       "   -0.4848330020904541,\n",
       "   0.11482233554124832,\n",
       "   0.021416140720248222,\n",
       "   0.029622720554471016,\n",
       "   -0.28744208812713623,\n",
       "   -0.0999445766210556,\n",
       "   0.2839825451374054,\n",
       "   -0.17824193835258484,\n",
       "   -0.0195840522646904,\n",
       "   0.1400885134935379,\n",
       "   -0.017837120220065117,\n",
       "   0.44972535967826843,\n",
       "   0.016170727089047432,\n",
       "   0.14662963151931763,\n",
       "   -0.13026845455169678,\n",
       "   -0.3358595073223114,\n",
       "   0.14714747667312622,\n",
       "   0.11855409294366837,\n",
       "   -0.1868990808725357,\n",
       "   0.11877867579460144,\n",
       "   -0.6414977312088013,\n",
       "   0.6867644786834717,\n",
       "   -0.3936838209629059,\n",
       "   0.046809710562229156,\n",
       "   -0.1770571917295456,\n",
       "   0.005523039028048515,\n",
       "   0.05874712020158768,\n",
       "   0.20094285905361176,\n",
       "   0.06209663674235344,\n",
       "   -0.024835441261529922,\n",
       "   0.051789652556180954,\n",
       "   -0.048071108758449554,\n",
       "   0.43067389726638794,\n",
       "   0.13396841287612915,\n",
       "   -0.1548430174589157,\n",
       "   0.014185836538672447,\n",
       "   -0.13235287368297577,\n",
       "   -0.24366748332977295,\n",
       "   0.0006665935507044196,\n",
       "   -0.0028249374590814114,\n",
       "   0.09867909550666809,\n",
       "   0.25335168838500977,\n",
       "   -0.3747820556163788,\n",
       "   -0.00952585693448782,\n",
       "   0.5556346774101257,\n",
       "   0.6756361126899719,\n",
       "   -0.005514427553862333,\n",
       "   -0.16983288526535034,\n",
       "   0.5373854041099548,\n",
       "   -0.23973266780376434,\n",
       "   0.04505354166030884,\n",
       "   0.007625010330229998,\n",
       "   0.2808559536933899,\n",
       "   -0.21621109545230865,\n",
       "   -0.15523956716060638,\n",
       "   0.16535800695419312,\n",
       "   -0.1876126378774643,\n",
       "   -0.07248137146234512,\n",
       "   -0.3388787508010864,\n",
       "   0.03757470101118088,\n",
       "   0.11687026172876358,\n",
       "   -0.025731902569532394,\n",
       "   0.352354496717453,\n",
       "   -0.11610665917396545,\n",
       "   0.13035628199577332,\n",
       "   0.47674742341041565,\n",
       "   -0.5442944169044495,\n",
       "   0.01888236217200756,\n",
       "   0.20235459506511688,\n",
       "   -0.158257856965065,\n",
       "   0.04982101544737816,\n",
       "   -0.35557398200035095,\n",
       "   0.20818153023719788,\n",
       "   -0.4992436170578003,\n",
       "   0.12082959711551666,\n",
       "   0.1167195737361908,\n",
       "   -0.04799878969788551,\n",
       "   -0.008167019113898277,\n",
       "   0.4135652482509613,\n",
       "   0.004664451815187931,\n",
       "   -0.08934294432401657,\n",
       "   0.1742320954799652,\n",
       "   0.14946265518665314,\n",
       "   -0.16347002983093262,\n",
       "   0.2082754671573639,\n",
       "   0.059473711997270584,\n",
       "   -0.4080435335636139,\n",
       "   0.27301025390625,\n",
       "   0.2885674238204956,\n",
       "   0.219059020280838,\n",
       "   -0.027915500104427338,\n",
       "   0.03552790358662605,\n",
       "   -0.34404459595680237,\n",
       "   -0.15635651350021362,\n",
       "   0.03787945955991745,\n",
       "   0.2769808769226074,\n",
       "   0.12806914746761322,\n",
       "   -0.07923812419176102,\n",
       "   0.22679205238819122,\n",
       "   0.15942807495594025,\n",
       "   0.23874200880527496,\n",
       "   0.18281638622283936,\n",
       "   -0.1755514144897461,\n",
       "   0.25527051091194153,\n",
       "   -0.49288418889045715,\n",
       "   0.23916201293468475,\n",
       "   -0.015249599702656269,\n",
       "   -0.04801684990525246,\n",
       "   0.23036545515060425,\n",
       "   -0.027865879237651825,\n",
       "   0.26922157406806946,\n",
       "   0.0889362022280693,\n",
       "   -0.13743793964385986,\n",
       "   0.09915444254875183,\n",
       "   0.43327194452285767,\n",
       "   -0.21602065861225128,\n",
       "   0.10483331233263016,\n",
       "   -0.04306714981794357,\n",
       "   0.10380172729492188,\n",
       "   -0.4468038082122803,\n",
       "   0.019531844183802605,\n",
       "   0.1452455371618271,\n",
       "   0.4038347601890564,\n",
       "   -0.2994631826877594,\n",
       "   0.09826626628637314,\n",
       "   0.07292862236499786,\n",
       "   -0.10378306359052658,\n",
       "   0.38374054431915283,\n",
       "   -0.1672814041376114,\n",
       "   -0.1662830412387848,\n",
       "   0.10470834374427795,\n",
       "   0.3505885601043701,\n",
       "   0.029159920290112495,\n",
       "   -0.13187573850154877,\n",
       "   0.3131963312625885,\n",
       "   0.4171101450920105,\n",
       "   -0.11177966743707657,\n",
       "   0.13979636132717133,\n",
       "   -0.317671000957489,\n",
       "   0.07381869107484818,\n",
       "   -0.19556035101413727,\n",
       "   0.015734797343611717,\n",
       "   0.19369155168533325,\n",
       "   -0.2224443405866623,\n",
       "   0.04331866651773453,\n",
       "   -0.24917849898338318,\n",
       "   0.047787632793188095,\n",
       "   0.018834112212061882,\n",
       "   -0.0852213203907013,\n",
       "   0.12228254228830338,\n",
       "   -0.2836296558380127,\n",
       "   0.040887121111154556,\n",
       "   -0.32678529620170593,\n",
       "   -0.16632528603076935,\n",
       "   0.20252424478530884,\n",
       "   -0.03806793689727783,\n",
       "   -0.0773313120007515,\n",
       "   -0.1420142501592636,\n",
       "   -0.16003619134426117,\n",
       "   -0.1513933390378952,\n",
       "   -0.18066738545894623,\n",
       "   0.3577122092247009,\n",
       "   -0.06347175687551498,\n",
       "   0.18099811673164368,\n",
       "   0.3026464879512787,\n",
       "   -0.051015231758356094,\n",
       "   -0.44160810112953186,\n",
       "   -0.2733083963394165,\n",
       "   0.0661417543888092,\n",
       "   0.10400618612766266,\n",
       "   0.0020875786431133747,\n",
       "   0.19865725934505463,\n",
       "   0.07123582810163498,\n",
       "   0.10624594241380692,\n",
       "   -0.45586541295051575,\n",
       "   -0.13405239582061768,\n",
       "   0.056323882192373276,\n",
       "   0.052114445716142654,\n",
       "   0.4160921275615692,\n",
       "   0.11906813830137253,\n",
       "   0.37769925594329834,\n",
       "   -0.038814183324575424,\n",
       "   0.08613438159227371,\n",
       "   -0.003188295988366008,\n",
       "   0.1740262508392334,\n",
       "   0.00782065000385046,\n",
       "   0.07826584577560425,\n",
       "   -0.10115719586610794,\n",
       "   0.03784384950995445,\n",
       "   -0.1557053029537201,\n",
       "   -0.4175284504890442,\n",
       "   0.07057540118694305,\n",
       "   0.035553496330976486,\n",
       "   0.1198817640542984],\n",
       "  [-0.16118036210536957,\n",
       "   0.2548774182796478,\n",
       "   -0.3390737473964691,\n",
       "   -0.12996628880500793,\n",
       "   0.5241680145263672,\n",
       "   -0.23995763063430786,\n",
       "   0.3960505723953247,\n",
       "   -0.08184182643890381,\n",
       "   0.0904616042971611,\n",
       "   0.4768114387989044,\n",
       "   0.3645857274532318,\n",
       "   0.08496446162462234,\n",
       "   -0.21909424662590027,\n",
       "   0.20740506052970886,\n",
       "   -0.670903205871582,\n",
       "   -0.5020628571510315,\n",
       "   0.0706392228603363,\n",
       "   -0.4079698920249939,\n",
       "   -0.3414822220802307,\n",
       "   -0.04170198738574982,\n",
       "   0.28540635108947754,\n",
       "   0.30541524291038513,\n",
       "   -0.09589706361293793,\n",
       "   -0.027225378900766373,\n",
       "   0.16270072758197784,\n",
       "   -0.07065805047750473,\n",
       "   0.32896316051483154,\n",
       "   0.7368724942207336,\n",
       "   0.105153389275074,\n",
       "   0.398468941450119,\n",
       "   0.3337153494358063,\n",
       "   0.05020992457866669,\n",
       "   0.019615059718489647,\n",
       "   0.037761569023132324,\n",
       "   0.5269657373428345,\n",
       "   0.577706515789032,\n",
       "   -0.6188894510269165,\n",
       "   0.4102226197719574,\n",
       "   0.022481562569737434,\n",
       "   0.565750002861023,\n",
       "   -0.015380033291876316,\n",
       "   -0.1056632548570633,\n",
       "   -0.009172332473099232,\n",
       "   -0.034488495439291,\n",
       "   0.3608178496360779,\n",
       "   0.030257878825068474,\n",
       "   -0.03941452130675316,\n",
       "   -0.12640228867530823,\n",
       "   -0.3816390335559845,\n",
       "   -0.016927015036344528,\n",
       "   0.017095765098929405,\n",
       "   -0.3749007284641266,\n",
       "   -0.2622559070587158,\n",
       "   -0.2754167914390564,\n",
       "   0.05244005471467972,\n",
       "   -0.18996988236904144,\n",
       "   0.18090727925300598,\n",
       "   -0.08110463619232178,\n",
       "   -0.20975971221923828,\n",
       "   0.8003359436988831,\n",
       "   -0.1325109750032425,\n",
       "   0.2743014395236969,\n",
       "   -0.056207939982414246,\n",
       "   -0.4608655869960785,\n",
       "   0.28373637795448303,\n",
       "   0.20740878582000732,\n",
       "   0.07655016332864761,\n",
       "   -0.13816799223423004,\n",
       "   0.4950923025608063,\n",
       "   0.07816627621650696,\n",
       "   -0.2305746078491211,\n",
       "   0.2954876720905304,\n",
       "   -0.16200470924377441,\n",
       "   -0.2595367431640625,\n",
       "   -0.26889291405677795,\n",
       "   -0.3849635124206543,\n",
       "   0.24702493846416473,\n",
       "   0.05513192340731621,\n",
       "   0.024578362703323364,\n",
       "   -0.03938755393028259,\n",
       "   0.14451810717582703,\n",
       "   0.6435462236404419,\n",
       "   -0.10290657728910446,\n",
       "   0.3885155916213989,\n",
       "   -0.5587705373764038,\n",
       "   0.016720660030841827,\n",
       "   -0.561145007610321,\n",
       "   0.035533443093299866,\n",
       "   0.4389629364013672,\n",
       "   0.7349928021430969,\n",
       "   0.19315187633037567,\n",
       "   -0.5062928199768066,\n",
       "   0.19690994918346405,\n",
       "   -0.3151557743549347,\n",
       "   -0.22122791409492493,\n",
       "   -0.0012899942230433226,\n",
       "   -0.37908509373664856,\n",
       "   0.08574897050857544,\n",
       "   -0.569525957107544,\n",
       "   -0.09390725195407867,\n",
       "   -0.3369569778442383,\n",
       "   -0.01994023658335209,\n",
       "   -0.06315162777900696,\n",
       "   -0.17143525183200836,\n",
       "   -0.15488475561141968,\n",
       "   0.12075058370828629,\n",
       "   -0.12998196482658386,\n",
       "   -0.7910171151161194,\n",
       "   -0.3719601631164551,\n",
       "   0.3715520203113556,\n",
       "   0.19428645074367523,\n",
       "   0.18164291977882385,\n",
       "   0.31810688972473145,\n",
       "   0.31228864192962646,\n",
       "   -0.3338969349861145,\n",
       "   -0.29936638474464417,\n",
       "   -0.1441807746887207,\n",
       "   0.30526530742645264,\n",
       "   0.2649693787097931,\n",
       "   -0.27686336636543274,\n",
       "   0.18579046428203583,\n",
       "   0.1535528600215912,\n",
       "   -0.2371605783700943,\n",
       "   -0.17795740067958832,\n",
       "   0.1036645844578743,\n",
       "   -0.35587015748023987,\n",
       "   -0.06384342163801193,\n",
       "   -0.3636842966079712,\n",
       "   0.20966067910194397,\n",
       "   -0.17850007116794586,\n",
       "   -0.26084548234939575,\n",
       "   -0.11780280619859695,\n",
       "   -0.18398040533065796,\n",
       "   0.2641684114933014,\n",
       "   0.1877296268939972,\n",
       "   0.27085012197494507,\n",
       "   -0.8909936547279358,\n",
       "   0.5056071281433105,\n",
       "   -0.9116324782371521,\n",
       "   -0.2875380218029022,\n",
       "   0.03411038964986801,\n",
       "   0.4675026834011078,\n",
       "   0.5237365365028381,\n",
       "   -0.3223752975463867,\n",
       "   -0.04780382663011551,\n",
       "   -0.5050094127655029,\n",
       "   -0.3783011734485626,\n",
       "   -0.06776946783065796,\n",
       "   0.35863015055656433,\n",
       "   0.15622833371162415,\n",
       "   -0.4706546664237976,\n",
       "   0.1139548197388649,\n",
       "   0.1874784231185913,\n",
       "   -0.6924290060997009,\n",
       "   -0.12418021261692047,\n",
       "   -0.13461719453334808,\n",
       "   -0.3887905776500702,\n",
       "   0.03179128095507622,\n",
       "   0.6316360831260681,\n",
       "   0.5841689109802246,\n",
       "   -0.04956052079796791,\n",
       "   -0.5320766568183899,\n",
       "   -0.38924551010131836,\n",
       "   0.2933290898799896,\n",
       "   0.16334937512874603,\n",
       "   0.09418615698814392,\n",
       "   -0.011521749198436737,\n",
       "   0.1768777072429657,\n",
       "   -0.5471398234367371,\n",
       "   0.4997970163822174,\n",
       "   0.5758666396141052,\n",
       "   -0.008185239508748055,\n",
       "   -0.010203116573393345,\n",
       "   0.1273200809955597,\n",
       "   -0.04549131542444229,\n",
       "   -0.054816801100969315,\n",
       "   0.7858956456184387,\n",
       "   0.03676290065050125,\n",
       "   -0.2523181736469269,\n",
       "   -0.0520772747695446,\n",
       "   0.40173864364624023,\n",
       "   0.015422468073666096,\n",
       "   -0.432502806186676,\n",
       "   -0.6307576894760132,\n",
       "   0.10409329831600189,\n",
       "   -0.5508580803871155,\n",
       "   0.25061193108558655,\n",
       "   -0.7649791240692139,\n",
       "   0.5634007453918457,\n",
       "   0.25909027457237244,\n",
       "   0.04744113236665726,\n",
       "   0.025842633098363876,\n",
       "   -0.4543457329273224,\n",
       "   0.1576547622680664,\n",
       "   -0.5229416489601135,\n",
       "   0.3572026491165161,\n",
       "   0.08482217788696289,\n",
       "   0.10434696078300476,\n",
       "   -0.17223124206066132,\n",
       "   0.3263723850250244,\n",
       "   -0.07457452267408371,\n",
       "   0.25614985823631287,\n",
       "   -0.20128631591796875,\n",
       "   -0.057447172701358795,\n",
       "   0.18718843162059784,\n",
       "   0.32431185245513916,\n",
       "   -0.2923354506492615,\n",
       "   0.24213199317455292,\n",
       "   -0.32563650608062744,\n",
       "   -0.5171029567718506,\n",
       "   0.5864726305007935,\n",
       "   -0.9867948889732361,\n",
       "   -0.29782453179359436,\n",
       "   -0.22253233194351196,\n",
       "   0.4641247093677521,\n",
       "   -0.3671762943267822,\n",
       "   0.18604405224323273,\n",
       "   -0.07757483422756195,\n",
       "   0.04644864797592163,\n",
       "   0.08029426634311676,\n",
       "   -0.37335240840911865,\n",
       "   -0.08275200426578522,\n",
       "   0.046821556985378265,\n",
       "   -0.4816427528858185,\n",
       "   -0.6080594062805176,\n",
       "   0.7808122634887695,\n",
       "   0.004602903500199318,\n",
       "   0.4872892200946808,\n",
       "   0.07176906615495682,\n",
       "   -0.20548975467681885,\n",
       "   0.23054541647434235,\n",
       "   -0.08261480927467346,\n",
       "   0.26670894026756287,\n",
       "   -0.3686211109161377,\n",
       "   0.2079067975282669,\n",
       "   0.41685035824775696,\n",
       "   -0.12099827080965042,\n",
       "   -0.005755011923611164,\n",
       "   -0.4716528654098511,\n",
       "   -0.6004133820533752,\n",
       "   0.14185206592082977,\n",
       "   0.24783481657505035,\n",
       "   -0.17491179704666138,\n",
       "   0.09398318827152252,\n",
       "   -0.07574395835399628,\n",
       "   -0.18352971971035004,\n",
       "   -0.24794046580791473,\n",
       "   0.1985965520143509,\n",
       "   -0.04839090257883072,\n",
       "   -0.20087794959545135,\n",
       "   -0.2899753153324127,\n",
       "   -0.292089581489563,\n",
       "   -0.15763358771800995,\n",
       "   -0.17393776774406433,\n",
       "   0.5212517976760864,\n",
       "   0.0027991647366434336,\n",
       "   -0.11521764099597931,\n",
       "   0.5686092376708984,\n",
       "   0.7254937887191772,\n",
       "   -1.0454336404800415,\n",
       "   0.18580150604248047,\n",
       "   0.5087225437164307,\n",
       "   0.2887498140335083,\n",
       "   -0.12014181911945343,\n",
       "   -0.33374279737472534,\n",
       "   -0.2789803445339203,\n",
       "   -0.3310821056365967,\n",
       "   -0.4359247088432312,\n",
       "   -0.8061231374740601,\n",
       "   0.2967895567417145,\n",
       "   0.17564520239830017,\n",
       "   -0.30804768204689026,\n",
       "   0.10638786852359772,\n",
       "   0.2888069748878479,\n",
       "   -0.037984538823366165,\n",
       "   -0.11371517181396484,\n",
       "   0.18010732531547546,\n",
       "   -0.2171565145254135,\n",
       "   -0.8490076661109924,\n",
       "   -0.5200297832489014,\n",
       "   0.639251708984375,\n",
       "   0.3721681535243988,\n",
       "   0.4477895498275757,\n",
       "   0.004984184168279171,\n",
       "   -0.5795627236366272,\n",
       "   -0.4607104957103729,\n",
       "   -0.050184816122055054,\n",
       "   0.36190301179885864,\n",
       "   -0.03871120512485504,\n",
       "   -0.5729483962059021,\n",
       "   -0.3277946412563324,\n",
       "   0.015909269452095032,\n",
       "   -0.6231564283370972,\n",
       "   -0.11371505260467529,\n",
       "   0.1300821155309677,\n",
       "   0.12161818146705627,\n",
       "   -0.05741734057664871,\n",
       "   0.24719037115573883,\n",
       "   -0.45256301760673523,\n",
       "   -0.17439676821231842,\n",
       "   -0.2690065801143646,\n",
       "   -0.09504737704992294,\n",
       "   0.4732087254524231,\n",
       "   -0.07921551167964935,\n",
       "   -0.17598043382167816,\n",
       "   -0.004227802157402039,\n",
       "   0.39032885432243347,\n",
       "   0.368396133184433,\n",
       "   0.4590757489204407,\n",
       "   0.37255924940109253,\n",
       "   0.4673812985420227,\n",
       "   -0.09202815592288971,\n",
       "   -0.047139935195446014,\n",
       "   -0.021669868379831314,\n",
       "   -0.010674797929823399,\n",
       "   -0.26285597681999207,\n",
       "   -0.047015272080898285,\n",
       "   -0.19194284081459045,\n",
       "   -0.21011389791965485,\n",
       "   -0.08524845540523529,\n",
       "   -0.32133039832115173,\n",
       "   0.2805407643318176,\n",
       "   -0.3578856885433197,\n",
       "   0.4713604152202606,\n",
       "   0.02095157280564308,\n",
       "   0.21679703891277313,\n",
       "   0.11991196125745773,\n",
       "   0.48704004287719727,\n",
       "   0.015723370015621185,\n",
       "   -0.0747404396533966,\n",
       "   0.17614011466503143,\n",
       "   -0.561481237411499,\n",
       "   -0.22983333468437195,\n",
       "   0.17573697865009308,\n",
       "   -0.6686018705368042,\n",
       "   -0.25735020637512207,\n",
       "   -0.28017330169677734,\n",
       "   0.32306617498397827,\n",
       "   -0.866256833076477,\n",
       "   0.6142313480377197,\n",
       "   0.20499271154403687,\n",
       "   0.4196791350841522,\n",
       "   0.25658705830574036,\n",
       "   -0.3012484908103943,\n",
       "   0.5664305686950684,\n",
       "   0.42797181010246277,\n",
       "   -0.062226615846157074,\n",
       "   0.4013035297393799,\n",
       "   -0.11381084471940994,\n",
       "   0.0321735255420208,\n",
       "   -0.47323039174079895,\n",
       "   -0.30588504672050476,\n",
       "   -0.03218212351202965,\n",
       "   -0.38531193137168884,\n",
       "   0.6459419131278992,\n",
       "   0.04508526250720024,\n",
       "   -0.06170400604605675,\n",
       "   0.38781121373176575,\n",
       "   0.09355349093675613,\n",
       "   0.6372923851013184,\n",
       "   0.24056586623191833,\n",
       "   -0.0035382756032049656,\n",
       "   0.47376179695129395,\n",
       "   0.04744107276201248,\n",
       "   0.28234800696372986,\n",
       "   -0.1681351363658905,\n",
       "   -0.8622677326202393,\n",
       "   -0.20753362774848938,\n",
       "   -0.860603392124176,\n",
       "   -0.27916470170021057,\n",
       "   1.1827244758605957,\n",
       "   -0.23245492577552795,\n",
       "   0.22582422196865082,\n",
       "   -0.19172145426273346,\n",
       "   0.45858699083328247,\n",
       "   -0.38793954253196716,\n",
       "   -0.6489843726158142,\n",
       "   0.05028460919857025,\n",
       "   0.0007890952983871102,\n",
       "   -0.6255221366882324,\n",
       "   -0.2706288695335388,\n",
       "   0.32252413034439087,\n",
       "   0.2522125244140625,\n",
       "   -0.1672646701335907,\n",
       "   0.03014996089041233,\n",
       "   -0.3043545186519623,\n",
       "   0.019072605296969414,\n",
       "   0.43793439865112305,\n",
       "   -0.03912633657455444,\n",
       "   -0.6419830322265625,\n",
       "   0.36825400590896606,\n",
       "   0.32277265191078186,\n",
       "   0.39554762840270996,\n",
       "   -0.4164430797100067,\n",
       "   -0.13892915844917297,\n",
       "   0.3417983055114746,\n",
       "   -0.464706689119339,\n",
       "   0.4481538236141205,\n",
       "   0.16648200154304504,\n",
       "   -0.12956523895263672,\n",
       "   -0.3028044104576111,\n",
       "   0.2549423277378082,\n",
       "   -0.04078936576843262,\n",
       "   -0.32036492228507996,\n",
       "   -0.6121866106987,\n",
       "   -0.3908574879169464,\n",
       "   -0.3095930814743042,\n",
       "   0.2779303193092346,\n",
       "   0.0860973596572876,\n",
       "   0.10871486365795135,\n",
       "   -0.26142895221710205,\n",
       "   -0.4056853950023651,\n",
       "   0.014902186580002308,\n",
       "   0.2827581763267517,\n",
       "   0.47607773542404175,\n",
       "   -0.4082109332084656,\n",
       "   0.06047036498785019,\n",
       "   -0.09930211305618286,\n",
       "   -0.00533907487988472,\n",
       "   0.040562063455581665,\n",
       "   -0.020959531888365746,\n",
       "   0.042253464460372925,\n",
       "   0.1940392702817917,\n",
       "   -0.165509894490242,\n",
       "   -0.026312435045838356,\n",
       "   0.2763359248638153,\n",
       "   0.0408070906996727,\n",
       "   -0.034036193042993546,\n",
       "   -0.4164751172065735,\n",
       "   0.0921064019203186,\n",
       "   -0.11870799213647842,\n",
       "   0.7137219905853271,\n",
       "   -0.018477167934179306,\n",
       "   0.8696333169937134,\n",
       "   -0.06789664179086685,\n",
       "   0.15488125383853912,\n",
       "   -0.19119751453399658,\n",
       "   0.1361706703901291,\n",
       "   -0.2600696086883545,\n",
       "   0.24030598998069763,\n",
       "   -0.2703896760940552,\n",
       "   -0.4293617308139801,\n",
       "   -0.6342848539352417,\n",
       "   -0.79497891664505,\n",
       "   -0.033998310565948486,\n",
       "   -0.44016894698143005,\n",
       "   -0.4236289858818054,\n",
       "   -0.5722865462303162,\n",
       "   0.028717827051877975,\n",
       "   -0.355166494846344,\n",
       "   0.23839157819747925,\n",
       "   0.05603964999318123,\n",
       "   -0.358344167470932,\n",
       "   0.08771714568138123,\n",
       "   0.29782646894454956,\n",
       "   -0.4206430912017822,\n",
       "   -0.6134313941001892,\n",
       "   -0.3106149137020111,\n",
       "   -0.3681926429271698,\n",
       "   0.008005992509424686,\n",
       "   -1.1116646528244019,\n",
       "   0.3165920376777649,\n",
       "   0.04180499538779259,\n",
       "   0.5331408381462097,\n",
       "   -0.1373254805803299,\n",
       "   -0.117766372859478,\n",
       "   0.5778921842575073,\n",
       "   0.4515663981437683,\n",
       "   0.8493826389312744,\n",
       "   0.12393443286418915,\n",
       "   0.7081281542778015,\n",
       "   0.6008553504943848,\n",
       "   0.16986607015132904,\n",
       "   1.0446959733963013,\n",
       "   0.04611143842339516,\n",
       "   -0.9326906204223633,\n",
       "   0.028310084715485573,\n",
       "   0.3845920264720917,\n",
       "   -0.17224480211734772,\n",
       "   -0.1473870575428009,\n",
       "   -0.6794405579566956,\n",
       "   0.05287427082657814,\n",
       "   -0.3958606421947479,\n",
       "   -0.23576238751411438,\n",
       "   0.03321145102381706,\n",
       "   -0.2599148750305176,\n",
       "   1.1132664680480957,\n",
       "   0.15778306126594543,\n",
       "   0.4002273678779602,\n",
       "   0.39104291796684265,\n",
       "   0.05080564692616463,\n",
       "   -0.2302640974521637,\n",
       "   -0.023122508078813553,\n",
       "   0.1438983678817749,\n",
       "   -0.6486464142799377,\n",
       "   0.13170547783374786,\n",
       "   -0.03208722174167633,\n",
       "   0.132053405046463,\n",
       "   0.17821818590164185,\n",
       "   -0.47252368927001953,\n",
       "   -0.06972049921751022,\n",
       "   0.1837293803691864,\n",
       "   0.24692562222480774,\n",
       "   0.31285417079925537,\n",
       "   -0.6673080325126648,\n",
       "   1.0399270057678223,\n",
       "   0.327576607465744,\n",
       "   0.054062288254499435,\n",
       "   0.43577155470848083,\n",
       "   0.39679887890815735,\n",
       "   -0.07823702692985535,\n",
       "   -0.2924978733062744,\n",
       "   0.038145799189805984,\n",
       "   -0.02392362803220749,\n",
       "   -0.01219446025788784,\n",
       "   -0.10499182343482971,\n",
       "   0.41431158781051636,\n",
       "   0.11188945919275284,\n",
       "   0.15597642958164215,\n",
       "   -0.22246021032333374,\n",
       "   0.4373754560947418,\n",
       "   0.24591389298439026,\n",
       "   0.2015192210674286,\n",
       "   0.36282816529273987,\n",
       "   0.06501882523298264,\n",
       "   -0.05484336242079735,\n",
       "   0.751182496547699,\n",
       "   0.6218438148498535,\n",
       "   0.08290048688650131,\n",
       "   0.007583197671920061,\n",
       "   -0.16646942496299744,\n",
       "   -0.020261431112885475,\n",
       "   0.26508629322052,\n",
       "   -0.3000650703907013,\n",
       "   1.0192822217941284,\n",
       "   -0.2523294985294342,\n",
       "   0.549095869064331,\n",
       "   -0.19170352816581726,\n",
       "   -0.30338194966316223,\n",
       "   -0.2464972734451294,\n",
       "   -0.08461539447307587,\n",
       "   0.022817738354206085,\n",
       "   0.18356779217720032,\n",
       "   0.09961245208978653,\n",
       "   0.15479299426078796,\n",
       "   0.38340210914611816,\n",
       "   -0.20922940969467163,\n",
       "   0.3950158357620239,\n",
       "   0.501717746257782,\n",
       "   -0.14048175513744354,\n",
       "   0.3479819893836975,\n",
       "   -0.006775234825909138,\n",
       "   -1.2905402183532715,\n",
       "   0.25725090503692627,\n",
       "   0.307098925113678,\n",
       "   -0.26462480425834656,\n",
       "   -0.1025044322013855,\n",
       "   -8.461233139038086,\n",
       "   0.4207412004470825,\n",
       "   -0.06890023499727249,\n",
       "   0.35919246077537537,\n",
       "   0.34371161460876465,\n",
       "   0.6763906478881836,\n",
       "   0.25267907977104187,\n",
       "   -0.6744294166564941,\n",
       "   0.016844656318426132,\n",
       "   -0.038937993347644806,\n",
       "   -0.3312195837497711,\n",
       "   -0.33403703570365906,\n",
       "   0.5931390523910522,\n",
       "   -0.06325100362300873,\n",
       "   0.35472142696380615,\n",
       "   0.11524182558059692,\n",
       "   -0.08524574339389801,\n",
       "   0.23076733946800232,\n",
       "   0.6242288947105408,\n",
       "   -0.37884894013404846,\n",
       "   -0.07490301132202148,\n",
       "   0.456752210855484,\n",
       "   0.250792533159256,\n",
       "   0.14842379093170166,\n",
       "   -0.08614388108253479,\n",
       "   0.1364707201719284,\n",
       "   0.3224850296974182,\n",
       "   0.019159337505698204,\n",
       "   0.12716467678546906,\n",
       "   -0.0871860459446907,\n",
       "   -0.14094455540180206,\n",
       "   0.0700392946600914,\n",
       "   -0.14966754615306854,\n",
       "   -0.1668170988559723,\n",
       "   0.38829922676086426,\n",
       "   -0.33281004428863525,\n",
       "   0.050803039222955704,\n",
       "   -0.1896197497844696,\n",
       "   0.8266006112098694,\n",
       "   -0.4771983027458191,\n",
       "   -0.055155299603939056,\n",
       "   -0.2259303629398346,\n",
       "   -0.32767999172210693,\n",
       "   -0.22564662992954254,\n",
       "   0.26996666193008423,\n",
       "   0.11049818992614746,\n",
       "   0.20235441625118256,\n",
       "   -0.1865726262331009,\n",
       "   -0.6951892971992493,\n",
       "   0.3950577974319458,\n",
       "   0.5411190986633301,\n",
       "   0.2780722677707672,\n",
       "   0.43997713923454285,\n",
       "   0.2338045835494995,\n",
       "   -0.6291667819023132,\n",
       "   -0.2125924527645111,\n",
       "   -0.20704704523086548,\n",
       "   0.781332790851593,\n",
       "   0.06425851583480835,\n",
       "   -0.24195243418216705,\n",
       "   0.19828495383262634,\n",
       "   -0.3871179223060608,\n",
       "   1.0662950277328491,\n",
       "   -0.010818406008183956,\n",
       "   -0.1649172604084015,\n",
       "   0.4206255376338959,\n",
       "   0.008915499784052372,\n",
       "   0.3621164560317993,\n",
       "   0.06667633354663849,\n",
       "   -0.2655062973499298,\n",
       "   -0.1279831826686859,\n",
       "   -0.40621647238731384,\n",
       "   0.15574197471141815,\n",
       "   -0.8797316551208496,\n",
       "   0.10944870859384537,\n",
       "   -1.0463298559188843,\n",
       "   -0.6048266887664795,\n",
       "   -0.05775974690914154,\n",
       "   -0.3227023482322693,\n",
       "   0.4113863706588745,\n",
       "   -0.2712942659854889,\n",
       "   0.5715924501419067,\n",
       "   0.2749098837375641,\n",
       "   -0.6586939692497253,\n",
       "   0.031068310141563416,\n",
       "   0.11090243607759476,\n",
       "   -0.245825856924057,\n",
       "   0.24054710566997528,\n",
       "   -0.14132273197174072,\n",
       "   0.061777252703905106,\n",
       "   0.08935746550559998,\n",
       "   0.4066148102283478,\n",
       "   -0.011214441619813442,\n",
       "   -0.021838895976543427,\n",
       "   0.14420372247695923,\n",
       "   0.2681196630001068,\n",
       "   -0.20947489142417908,\n",
       "   -0.2278681844472885,\n",
       "   0.06600211560726166,\n",
       "   0.0002634738339111209,\n",
       "   -0.6900691390037537,\n",
       "   -0.08256657421588898,\n",
       "   -0.46704399585723877,\n",
       "   0.4112975001335144,\n",
       "   -0.12397714704275131,\n",
       "   -0.30216076970100403,\n",
       "   0.46779993176460266,\n",
       "   -0.6343556046485901,\n",
       "   0.475957453250885,\n",
       "   0.3509064316749573,\n",
       "   -0.41110438108444214,\n",
       "   0.4805920720100403,\n",
       "   0.9691587686538696,\n",
       "   -0.6198944449424744,\n",
       "   0.17560695111751556,\n",
       "   0.46370160579681396,\n",
       "   0.6311012506484985,\n",
       "   -0.11954224109649658,\n",
       "   0.2885425388813019,\n",
       "   -0.618180513381958,\n",
       "   -0.26434823870658875,\n",
       "   -0.3110060691833496,\n",
       "   0.20582890510559082,\n",
       "   -0.08262571692466736,\n",
       "   0.5847300887107849,\n",
       "   -0.34600457549095154,\n",
       "   0.10258402675390244,\n",
       "   -0.08332246541976929,\n",
       "   0.38459163904190063,\n",
       "   0.016180410981178284,\n",
       "   0.20843768119812012,\n",
       "   -0.018626609817147255,\n",
       "   -0.2424992173910141,\n",
       "   0.7938214540481567,\n",
       "   0.2790283262729645,\n",
       "   0.019204458221793175,\n",
       "   -0.1727563738822937,\n",
       "   -0.4892141819000244,\n",
       "   -0.8415645956993103,\n",
       "   -0.30211398005485535,\n",
       "   -0.5301533937454224,\n",
       "   -0.35059642791748047,\n",
       "   0.8121254444122314,\n",
       "   0.46847692131996155,\n",
       "   0.7174314260482788,\n",
       "   -0.4090859591960907,\n",
       "   0.15861041843891144,\n",
       "   0.2529904842376709,\n",
       "   0.03455597534775734,\n",
       "   -0.3481205105781555,\n",
       "   -0.3595319986343384,\n",
       "   0.12215254455804825,\n",
       "   0.8088840842247009,\n",
       "   0.6441751718521118,\n",
       "   0.5998246073722839,\n",
       "   -0.7174039483070374,\n",
       "   0.5150076150894165,\n",
       "   0.5524468421936035,\n",
       "   0.03858539089560509,\n",
       "   0.09241282939910889,\n",
       "   0.17091454565525055,\n",
       "   -0.08983217924833298,\n",
       "   0.2286573350429535,\n",
       "   1.0073153972625732,\n",
       "   -0.15557825565338135,\n",
       "   0.3272560238838196,\n",
       "   -0.2570045590400696,\n",
       "   0.5124146342277527,\n",
       "   0.129707470536232,\n",
       "   0.41511523723602295,\n",
       "   -0.060493823140859604,\n",
       "   -0.6515084505081177,\n",
       "   0.45937466621398926,\n",
       "   0.3152359426021576,\n",
       "   -0.062345679849386215,\n",
       "   -0.4168175160884857,\n",
       "   0.08525603264570236,\n",
       "   -0.31925538182258606,\n",
       "   0.46335864067077637,\n",
       "   -0.06699099391698837,\n",
       "   0.32003405690193176,\n",
       "   0.48793962597846985,\n",
       "   -0.3741367757320404,\n",
       "   -0.18712787330150604,\n",
       "   -0.8024080991744995,\n",
       "   -0.12542976438999176,\n",
       "   0.29424265027046204,\n",
       "   0.20694321393966675,\n",
       "   0.08666428178548813,\n",
       "   -0.29756519198417664,\n",
       "   0.07736829668283463,\n",
       "   0.2699865698814392,\n",
       "   -0.09897147119045258,\n",
       "   0.15428216755390167,\n",
       "   0.06912262737751007,\n",
       "   0.21941202878952026,\n",
       "   -0.023837057873606682,\n",
       "   0.3267635405063629,\n",
       "   -0.0658392384648323,\n",
       "   -0.2202189564704895,\n",
       "   0.10840199142694473,\n",
       "   0.258113831281662,\n",
       "   -0.33948907256126404,\n",
       "   -0.3111613392829895,\n",
       "   -0.3627331554889679,\n",
       "   -0.14684954285621643,\n",
       "   0.10231925547122955,\n",
       "   0.6670399308204651,\n",
       "   -0.07142055779695511,\n",
       "   0.23693832755088806,\n",
       "   0.03896544501185417],\n",
       "  [-0.25803691148757935,\n",
       "   -0.026581164449453354,\n",
       "   -0.0002084632433252409,\n",
       "   0.09458558261394501,\n",
       "   0.16868850588798523,\n",
       "   -0.07082992792129517,\n",
       "   0.13221138715744019,\n",
       "   -0.08993542939424515,\n",
       "   -0.31156906485557556,\n",
       "   0.3077642321586609,\n",
       "   -0.09806609898805618,\n",
       "   0.041831180453300476,\n",
       "   -0.3141549825668335,\n",
       "   -0.154927596449852,\n",
       "   -0.5019873976707458,\n",
       "   -0.6071435809135437,\n",
       "   0.13161244988441467,\n",
       "   -0.5406479835510254,\n",
       "   -0.05289672687649727,\n",
       "   -0.07065163552761078,\n",
       "   0.05446320399641991,\n",
       "   -0.1414726823568344,\n",
       "   -0.23325762152671814,\n",
       "   -0.15816324949264526,\n",
       "   0.14160680770874023,\n",
       "   -0.5551360845565796,\n",
       "   -0.1259886920452118,\n",
       "   0.7447525262832642,\n",
       "   -0.20748008787631989,\n",
       "   0.458329439163208,\n",
       "   0.2830254137516022,\n",
       "   -0.225203737616539,\n",
       "   0.4948604106903076,\n",
       "   0.02336782217025757,\n",
       "   0.038054242730140686,\n",
       "   0.24787338078022003,\n",
       "   -0.289493203163147,\n",
       "   0.5485414266586304,\n",
       "   0.07099597901105881,\n",
       "   0.3927100598812103,\n",
       "   0.16960199177265167,\n",
       "   0.2640778124332428,\n",
       "   0.05051117762923241,\n",
       "   0.31199297308921814,\n",
       "   -0.710211455821991,\n",
       "   -0.17547501623630524,\n",
       "   -0.06917247176170349,\n",
       "   -0.5312818884849548,\n",
       "   -0.2797756791114807,\n",
       "   0.10280565172433853,\n",
       "   0.5880894064903259,\n",
       "   -0.13363216817378998,\n",
       "   -0.20380626618862152,\n",
       "   -0.10658535361289978,\n",
       "   0.11379930377006531,\n",
       "   -0.48533183336257935,\n",
       "   0.15005181729793549,\n",
       "   0.12578906118869781,\n",
       "   -0.24781066179275513,\n",
       "   0.3740174472332001,\n",
       "   -0.06073332205414772,\n",
       "   -0.15053196251392365,\n",
       "   0.009288348257541656,\n",
       "   -0.053447823971509933,\n",
       "   0.05637048929929733,\n",
       "   0.04451453685760498,\n",
       "   0.3206358253955841,\n",
       "   -0.4170416295528412,\n",
       "   0.20449040830135345,\n",
       "   0.20016196370124817,\n",
       "   -0.1975083351135254,\n",
       "   -0.10165706276893616,\n",
       "   -0.27798154950141907,\n",
       "   -0.2658195495605469,\n",
       "   0.0653318241238594,\n",
       "   -0.08551584929227829,\n",
       "   -0.3567050099372864,\n",
       "   0.06505730003118515,\n",
       "   0.14205969870090485,\n",
       "   0.1788642853498459,\n",
       "   0.2696695625782013,\n",
       "   0.1409100443124771,\n",
       "   -0.2091677188873291,\n",
       "   0.5363064408302307,\n",
       "   0.10246148705482483,\n",
       "   0.1515156775712967,\n",
       "   -0.2999197542667389,\n",
       "   -0.013456829823553562,\n",
       "   -0.11866404116153717,\n",
       "   0.229743093252182,\n",
       "   0.33061885833740234,\n",
       "   -0.519523024559021,\n",
       "   0.5238829255104065,\n",
       "   -0.05531240627169609,\n",
       "   -0.12541675567626953,\n",
       "   -0.14465753734111786,\n",
       "   -0.48311617970466614,\n",
       "   -0.36931151151657104,\n",
       "   -0.9509907960891724,\n",
       "   0.08079399913549423,\n",
       "   -0.11169755458831787,\n",
       "   0.46738123893737793,\n",
       "   -0.04734358564019203,\n",
       "   -0.20623527467250824,\n",
       "   0.06429463624954224,\n",
       "   0.16789163649082184,\n",
       "   -0.5912734866142273,\n",
       "   -0.8214201331138611,\n",
       "   -0.3872552216053009,\n",
       "   0.6589658856391907,\n",
       "   0.03676404803991318,\n",
       "   0.33561837673187256,\n",
       "   -0.3125501275062561,\n",
       "   0.2828826904296875,\n",
       "   -0.41041603684425354,\n",
       "   -0.7497535943984985,\n",
       "   -0.0038993111811578274,\n",
       "   0.14202924072742462,\n",
       "   0.11027384549379349,\n",
       "   0.16176854074001312,\n",
       "   0.250623494386673,\n",
       "   -0.05004998669028282,\n",
       "   -0.15158043801784515,\n",
       "   0.10352303087711334,\n",
       "   0.0014177518896758556,\n",
       "   -0.1292148232460022,\n",
       "   -0.21233101189136505,\n",
       "   -0.4723093807697296,\n",
       "   0.26506927609443665,\n",
       "   -0.16920647025108337,\n",
       "   -0.21612714231014252,\n",
       "   0.11385982483625412,\n",
       "   -0.10639433562755585,\n",
       "   -0.23700059950351715,\n",
       "   0.23395107686519623,\n",
       "   0.1365886628627777,\n",
       "   -0.5958521366119385,\n",
       "   -0.09193074703216553,\n",
       "   -1.1826316118240356,\n",
       "   -0.4507286250591278,\n",
       "   -0.28068357706069946,\n",
       "   0.3938458561897278,\n",
       "   0.6745367050170898,\n",
       "   -0.21526089310646057,\n",
       "   0.14130260050296783,\n",
       "   -0.1315600872039795,\n",
       "   -0.5191888213157654,\n",
       "   0.00027618667809292674,\n",
       "   0.29900801181793213,\n",
       "   0.11295842379331589,\n",
       "   -0.16458846628665924,\n",
       "   -0.25166913866996765,\n",
       "   0.2129112333059311,\n",
       "   -0.03473484516143799,\n",
       "   -0.3240460753440857,\n",
       "   -0.03754853829741478,\n",
       "   -0.16651789844036102,\n",
       "   0.006056374404579401,\n",
       "   0.5024867653846741,\n",
       "   0.6025905609130859,\n",
       "   0.17680202424526215,\n",
       "   -0.8164891600608826,\n",
       "   -0.13730980455875397,\n",
       "   0.42325738072395325,\n",
       "   0.17150601744651794,\n",
       "   -0.2441878467798233,\n",
       "   -0.06814207136631012,\n",
       "   0.4653148353099823,\n",
       "   -0.3865961730480194,\n",
       "   0.11799431592226028,\n",
       "   0.3327935039997101,\n",
       "   -0.23555730283260345,\n",
       "   0.5027729272842407,\n",
       "   0.35157060623168945,\n",
       "   0.10051771253347397,\n",
       "   -0.0970325767993927,\n",
       "   0.3153649866580963,\n",
       "   -0.028004761785268784,\n",
       "   -0.04500806704163551,\n",
       "   -0.24231839179992676,\n",
       "   0.23741890490055084,\n",
       "   -0.49472934007644653,\n",
       "   0.17074546217918396,\n",
       "   -0.5892711281776428,\n",
       "   0.3051777184009552,\n",
       "   -0.43087026476860046,\n",
       "   -0.02956852875649929,\n",
       "   -0.6974024772644043,\n",
       "   0.161199152469635,\n",
       "   0.0651339739561081,\n",
       "   0.07700598984956741,\n",
       "   -0.32202595472335815,\n",
       "   -0.5001997351646423,\n",
       "   -0.06798647344112396,\n",
       "   -0.9889787435531616,\n",
       "   0.36374861001968384,\n",
       "   -0.05773237347602844,\n",
       "   0.29524582624435425,\n",
       "   0.07275548577308655,\n",
       "   0.20161965489387512,\n",
       "   -0.005035771057009697,\n",
       "   0.1508270651102066,\n",
       "   -0.3018660545349121,\n",
       "   -0.03103608824312687,\n",
       "   -0.13230684399604797,\n",
       "   0.21461232006549835,\n",
       "   -0.7408343553543091,\n",
       "   0.42254722118377686,\n",
       "   -0.17302940785884857,\n",
       "   -0.6659637689590454,\n",
       "   -0.15661169588565826,\n",
       "   -0.6492679715156555,\n",
       "   0.09099490195512772,\n",
       "   -0.012517577968537807,\n",
       "   -0.5251414179801941,\n",
       "   -0.28792253136634827,\n",
       "   -0.22010579705238342,\n",
       "   0.14300182461738586,\n",
       "   -0.04812394455075264,\n",
       "   -0.13265137374401093,\n",
       "   0.48282507061958313,\n",
       "   0.22073163092136383,\n",
       "   0.3868749737739563,\n",
       "   -0.009591063484549522,\n",
       "   -0.3682824671268463,\n",
       "   0.3337438702583313,\n",
       "   0.34181058406829834,\n",
       "   0.5150434970855713,\n",
       "   0.47151345014572144,\n",
       "   -0.17704138159751892,\n",
       "   -0.16076114773750305,\n",
       "   0.13847392797470093,\n",
       "   -0.008947880007326603,\n",
       "   -0.4163866937160492,\n",
       "   0.2652185559272766,\n",
       "   0.06732241809368134,\n",
       "   -0.06145765632390976,\n",
       "   0.11447279155254364,\n",
       "   -0.7358323931694031,\n",
       "   -0.584770917892456,\n",
       "   -0.042866844683885574,\n",
       "   -0.20643937587738037,\n",
       "   0.29114991426467896,\n",
       "   0.446331650018692,\n",
       "   -0.10909530520439148,\n",
       "   0.03071616403758526,\n",
       "   -0.4020671546459198,\n",
       "   0.701820433139801,\n",
       "   0.17239852249622345,\n",
       "   0.17631091177463531,\n",
       "   -0.35870322585105896,\n",
       "   -0.17862924933433533,\n",
       "   -0.25562822818756104,\n",
       "   0.19948841631412506,\n",
       "   -0.3929453194141388,\n",
       "   -0.4331338405609131,\n",
       "   -0.43034443259239197,\n",
       "   0.5622181296348572,\n",
       "   0.5689752697944641,\n",
       "   -0.7415931820869446,\n",
       "   0.4714924097061157,\n",
       "   0.08370988070964813,\n",
       "   0.5340216159820557,\n",
       "   0.278860867023468,\n",
       "   -0.09496843069791794,\n",
       "   -0.11151673644781113,\n",
       "   -0.15514229238033295,\n",
       "   -0.43747419118881226,\n",
       "   -0.866813600063324,\n",
       "   0.7113158702850342,\n",
       "   0.238603875041008,\n",
       "   0.22847682237625122,\n",
       "   0.5276827812194824,\n",
       "   0.5207658410072327,\n",
       "   -0.14396122097969055,\n",
       "   0.17295026779174805,\n",
       "   0.10046196728944778,\n",
       "   -0.21848656237125397,\n",
       "   -0.052546847611665726,\n",
       "   -0.22555501759052277,\n",
       "   0.28051143884658813,\n",
       "   0.5794099569320679,\n",
       "   0.0998472273349762,\n",
       "   0.2269897162914276,\n",
       "   -0.6186583638191223,\n",
       "   0.2805485725402832,\n",
       "   0.06699632853269577,\n",
       "   0.6560161113739014,\n",
       "   -0.10874095559120178,\n",
       "   0.2053752839565277,\n",
       "   -0.3896101415157318,\n",
       "   -0.040296487510204315,\n",
       "   -0.4295681416988373,\n",
       "   0.018299587070941925,\n",
       "   -0.015278875827789307,\n",
       "   0.060627929866313934,\n",
       "   -0.1054285392165184,\n",
       "   0.35363802313804626,\n",
       "   -0.4453752934932709,\n",
       "   -0.32608094811439514,\n",
       "   0.19720464944839478,\n",
       "   0.11971316486597061,\n",
       "   0.0946442112326622,\n",
       "   -0.01612602360546589,\n",
       "   -0.17233771085739136,\n",
       "   -0.06811206042766571,\n",
       "   0.20292791724205017,\n",
       "   -0.01877501793205738,\n",
       "   -0.427668035030365,\n",
       "   0.7741931676864624,\n",
       "   0.46006670594215393,\n",
       "   -0.07403848320245743,\n",
       "   -0.38440415263175964,\n",
       "   -0.6232848763465881,\n",
       "   0.40760478377342224,\n",
       "   -0.49342775344848633,\n",
       "   -0.28425487875938416,\n",
       "   -0.3611735701560974,\n",
       "   -0.05005580931901932,\n",
       "   -0.11414416879415512,\n",
       "   -0.2465554177761078,\n",
       "   0.11512145400047302,\n",
       "   -0.42984795570373535,\n",
       "   0.5296779274940491,\n",
       "   0.043837182223796844,\n",
       "   -0.060486529022455215,\n",
       "   -0.019612984731793404,\n",
       "   0.39701053500175476,\n",
       "   0.43416866660118103,\n",
       "   -0.17765669524669647,\n",
       "   -0.4391706883907318,\n",
       "   -0.3043603003025055,\n",
       "   0.32540667057037354,\n",
       "   0.506328284740448,\n",
       "   0.15033040940761566,\n",
       "   0.6072776913642883,\n",
       "   0.011618631891906261,\n",
       "   0.030567236244678497,\n",
       "   -0.529504120349884,\n",
       "   0.5136767029762268,\n",
       "   0.28676000237464905,\n",
       "   0.8457046151161194,\n",
       "   0.42558372020721436,\n",
       "   -0.12779268622398376,\n",
       "   0.32173237204551697,\n",
       "   0.1889234185218811,\n",
       "   -0.11684306710958481,\n",
       "   -0.21690334379673004,\n",
       "   -0.16602878272533417,\n",
       "   -0.3157052993774414,\n",
       "   -0.4700184166431427,\n",
       "   -0.4744366407394409,\n",
       "   -0.321975439786911,\n",
       "   0.29202327132225037,\n",
       "   0.9017191529273987,\n",
       "   0.12796010076999664,\n",
       "   0.12219768762588501,\n",
       "   0.20404095947742462,\n",
       "   0.413437157869339,\n",
       "   -0.0532064251601696,\n",
       "   0.32263702154159546,\n",
       "   0.1761266589164734,\n",
       "   -0.14216044545173645,\n",
       "   0.4350399374961853,\n",
       "   0.004195425659418106,\n",
       "   0.06537966430187225,\n",
       "   -0.11007660627365112,\n",
       "   0.13539478182792664,\n",
       "   -0.030017603188753128,\n",
       "   -0.10725226253271103,\n",
       "   0.9718599319458008,\n",
       "   -0.060900766402482986,\n",
       "   0.21223565936088562,\n",
       "   0.18304206430912018,\n",
       "   0.164345845580101,\n",
       "   -0.15211132168769836,\n",
       "   -0.2818068861961365,\n",
       "   0.12234903872013092,\n",
       "   -0.6682173609733582,\n",
       "   -0.5247056484222412,\n",
       "   -0.06638265401124954,\n",
       "   -0.11201828718185425,\n",
       "   0.38741037249565125,\n",
       "   0.21633300185203552,\n",
       "   0.3654480576515198,\n",
       "   0.0086204307153821,\n",
       "   -0.30683764815330505,\n",
       "   0.27099791169166565,\n",
       "   -0.3368166387081146,\n",
       "   -0.3318989872932434,\n",
       "   -0.03968675807118416,\n",
       "   -0.09623861312866211,\n",
       "   0.4754255414009094,\n",
       "   -0.2419278472661972,\n",
       "   -0.028969047591090202,\n",
       "   -0.25039955973625183,\n",
       "   -0.5193383693695068,\n",
       "   -0.09433285892009735,\n",
       "   0.08773963898420334,\n",
       "   -0.14761310815811157,\n",
       "   -0.13884562253952026,\n",
       "   0.15040186047554016,\n",
       "   -0.20549246668815613,\n",
       "   -0.1539548933506012,\n",
       "   -0.5282682776451111,\n",
       "   -0.3748660981655121,\n",
       "   -0.02021203190088272,\n",
       "   0.5675984025001526,\n",
       "   -0.2096230685710907,\n",
       "   -0.17298905551433563,\n",
       "   -0.2957688570022583,\n",
       "   -0.5375962257385254,\n",
       "   0.10966800898313522,\n",
       "   0.3378791809082031,\n",
       "   0.4128841757774353,\n",
       "   -0.38690900802612305,\n",
       "   -0.38926124572753906,\n",
       "   -0.430248498916626,\n",
       "   0.2850137948989868,\n",
       "   -0.11109944432973862,\n",
       "   -0.18036504089832306,\n",
       "   0.14165174961090088,\n",
       "   0.13072524964809418,\n",
       "   0.00870427954941988,\n",
       "   0.14146390557289124,\n",
       "   0.3455699384212494,\n",
       "   -0.0028861055616289377,\n",
       "   0.07913078367710114,\n",
       "   -0.807161271572113,\n",
       "   0.13351556658744812,\n",
       "   0.32628902792930603,\n",
       "   0.009851592592895031,\n",
       "   0.14006035029888153,\n",
       "   0.2269192785024643,\n",
       "   0.026897011324763298,\n",
       "   -0.07606129348278046,\n",
       "   -0.2973063290119171,\n",
       "   0.26456454396247864,\n",
       "   0.37234508991241455,\n",
       "   0.15931250154972076,\n",
       "   0.23426397144794464,\n",
       "   -0.10927241295576096,\n",
       "   -0.17913395166397095,\n",
       "   -1.0576958656311035,\n",
       "   -0.20106418430805206,\n",
       "   0.16288051009178162,\n",
       "   0.22478924691677094,\n",
       "   0.2396278977394104,\n",
       "   0.4226123094558716,\n",
       "   -0.0597689189016819,\n",
       "   0.03742881491780281,\n",
       "   -0.14040407538414001,\n",
       "   0.42268022894859314,\n",
       "   0.012030419893562794,\n",
       "   0.06350363045930862,\n",
       "   -0.6553399562835693,\n",
       "   -0.2147085815668106,\n",
       "   -0.09395276755094528,\n",
       "   0.19316433370113373,\n",
       "   -0.16022184491157532,\n",
       "   -0.9528688192367554,\n",
       "   0.34300872683525085,\n",
       "   -0.10820504277944565,\n",
       "   0.1917925775051117,\n",
       "   0.28331634402275085,\n",
       "   0.14835882186889648,\n",
       "   0.38261091709136963,\n",
       "   -0.04849306866526604,\n",
       "   0.09964045137166977,\n",
       "   0.23071028292179108,\n",
       "   0.3187336027622223,\n",
       "   0.02529076859354973,\n",
       "   0.765903651714325,\n",
       "   0.3467594385147095,\n",
       "   0.17333142459392548,\n",
       "   -0.4499671459197998,\n",
       "   0.007897119037806988,\n",
       "   0.6057373881340027,\n",
       "   -0.43923938274383545,\n",
       "   -0.6469770073890686,\n",
       "   -0.721840500831604,\n",
       "   0.24914877116680145,\n",
       "   -0.2867042124271393,\n",
       "   -0.4173501133918762,\n",
       "   0.5088402628898621,\n",
       "   0.09954100102186203,\n",
       "   1.2018712759017944,\n",
       "   -0.0598415806889534,\n",
       "   -0.4512251913547516,\n",
       "   0.4939951002597809,\n",
       "   -0.14932066202163696,\n",
       "   0.48132258653640747,\n",
       "   0.12358377873897552,\n",
       "   0.18168039619922638,\n",
       "   -0.5983603000640869,\n",
       "   -0.14105714857578278,\n",
       "   -0.288070946931839,\n",
       "   -0.07949546724557877,\n",
       "   -0.17483197152614594,\n",
       "   0.008431525900959969,\n",
       "   -0.15261758863925934,\n",
       "   -0.35728782415390015,\n",
       "   -0.13240262866020203,\n",
       "   0.3406868875026703,\n",
       "   -0.33242613077163696,\n",
       "   0.524083137512207,\n",
       "   -0.4918464422225952,\n",
       "   0.24798384308815002,\n",
       "   0.13559794425964355,\n",
       "   -0.33833080530166626,\n",
       "   0.24623915553092957,\n",
       "   -0.6132382750511169,\n",
       "   0.2957671880722046,\n",
       "   0.20410625636577606,\n",
       "   0.4670617878437042,\n",
       "   -0.19984878599643707,\n",
       "   0.5938970446586609,\n",
       "   0.31613659858703613,\n",
       "   -0.11597724258899689,\n",
       "   -0.06220336630940437,\n",
       "   0.5259631276130676,\n",
       "   0.044961851090192795,\n",
       "   0.11060630530118942,\n",
       "   -0.40935251116752625,\n",
       "   0.10711958259344101,\n",
       "   0.39742234349250793,\n",
       "   0.8131203055381775,\n",
       "   0.06368853151798248,\n",
       "   0.014794973656535149,\n",
       "   0.14655743539333344,\n",
       "   -0.20495356619358063,\n",
       "   0.5492190718650818,\n",
       "   0.1661289632320404,\n",
       "   -0.37950241565704346,\n",
       "   0.9487369656562805,\n",
       "   -0.3462056815624237,\n",
       "   -0.12041141092777252,\n",
       "   0.17074362933635712,\n",
       "   -0.10987565666437149,\n",
       "   0.43470796942710876,\n",
       "   -0.3553905189037323,\n",
       "   0.35323044657707214,\n",
       "   0.16970542073249817,\n",
       "   0.2350703775882721,\n",
       "   0.03813415765762329,\n",
       "   1.0640106201171875,\n",
       "   -0.3930932879447937,\n",
       "   0.2580021917819977,\n",
       "   0.08430951088666916,\n",
       "   0.2009057253599167,\n",
       "   0.6940430402755737,\n",
       "   0.2040446400642395,\n",
       "   -0.4077298045158386,\n",
       "   -0.09767652302980423,\n",
       "   0.32291004061698914,\n",
       "   -0.00551588786765933,\n",
       "   -0.44827792048454285,\n",
       "   -8.640649795532227,\n",
       "   0.6765815019607544,\n",
       "   -0.10054365545511246,\n",
       "   -0.02622792311012745,\n",
       "   0.1348447948694229,\n",
       "   0.2414712905883789,\n",
       "   0.17126516997814178,\n",
       "   -0.47975054383277893,\n",
       "   -0.21327796578407288,\n",
       "   -0.32257476449012756,\n",
       "   -0.12264881283044815,\n",
       "   -0.017789550125598907,\n",
       "   0.054066888988018036,\n",
       "   -0.08259566128253937,\n",
       "   0.40824243426322937,\n",
       "   -0.3414228558540344,\n",
       "   0.1375359147787094,\n",
       "   0.09190976619720459,\n",
       "   0.6397035121917725,\n",
       "   -0.36878031492233276,\n",
       "   -0.16832704842090607,\n",
       "   0.02992294728755951,\n",
       "   -0.05979675427079201,\n",
       "   0.7439447641372681,\n",
       "   -0.2034699022769928,\n",
       "   0.8705410361289978,\n",
       "   0.35516929626464844,\n",
       "   0.14089369773864746,\n",
       "   0.44728145003318787,\n",
       "   -0.12661074101924896,\n",
       "   -0.2678869664669037,\n",
       "   0.3026527166366577,\n",
       "   -0.07628758251667023,\n",
       "   -0.17142599821090698,\n",
       "   -0.013351463712751865,\n",
       "   0.05043729022145271,\n",
       "   0.08545923978090286,\n",
       "   -0.21878381073474884,\n",
       "   -0.3192090690135956,\n",
       "   -0.8778055310249329,\n",
       "   0.23840433359146118,\n",
       "   -0.3974704444408417,\n",
       "   -0.09175150096416473,\n",
       "   0.1525876671075821,\n",
       "   0.5591399073600769,\n",
       "   0.31252700090408325,\n",
       "   -0.03051840513944626,\n",
       "   0.641873300075531,\n",
       "   0.1253788024187088,\n",
       "   0.253182590007782,\n",
       "   0.49253419041633606,\n",
       "   0.4097660183906555,\n",
       "   0.20669670403003693,\n",
       "   0.3484806716442108,\n",
       "   -0.1276489645242691,\n",
       "   0.15885592997074127,\n",
       "   -0.3979649841785431,\n",
       "   0.6651554107666016,\n",
       "   0.08404359221458435,\n",
       "   -0.8051372766494751,\n",
       "   -0.1565983146429062,\n",
       "   -0.1741277575492859,\n",
       "   0.6929267644882202,\n",
       "   0.07764724642038345,\n",
       "   -0.44723647832870483,\n",
       "   -0.11799924075603485,\n",
       "   -0.4284740388393402,\n",
       "   -0.07523128390312195,\n",
       "   0.04148486629128456,\n",
       "   -0.5630863308906555,\n",
       "   -0.5472524166107178,\n",
       "   -0.26462313532829285,\n",
       "   0.04557158425450325,\n",
       "   -0.3560697138309479,\n",
       "   0.29251036047935486,\n",
       "   -0.6638211011886597,\n",
       "   -0.13172726333141327,\n",
       "   0.10895023494958878,\n",
       "   -0.3436434864997864,\n",
       "   0.3819118142127991,\n",
       "   -0.1835448294878006,\n",
       "   0.3407430648803711,\n",
       "   0.4370788335800171,\n",
       "   -0.6676292419433594,\n",
       "   -0.10450468212366104,\n",
       "   0.010518878698348999,\n",
       "   -0.44917282462120056,\n",
       "   0.4352802038192749,\n",
       "   0.2566801905632019,\n",
       "   -0.09171664714813232,\n",
       "   -0.2674656808376312,\n",
       "   0.09788863360881805,\n",
       "   -0.05004055052995682,\n",
       "   -0.035083841532468796,\n",
       "   0.07424070686101913,\n",
       "   -0.04703905060887337,\n",
       "   -0.1455746293067932,\n",
       "   0.19091589748859406,\n",
       "   -0.004856927786022425,\n",
       "   0.9004971385002136,\n",
       "   -0.39042791724205017,\n",
       "   -0.20269905030727386,\n",
       "   -0.42893335223197937,\n",
       "   0.09756720066070557,\n",
       "   -0.08464032411575317,\n",
       "   -0.1271873563528061,\n",
       "   0.11188577115535736,\n",
       "   -0.24016298353672028,\n",
       "   -0.011290968395769596,\n",
       "   -0.030286742374300957,\n",
       "   -0.17167598009109497,\n",
       "   0.4323952794075012,\n",
       "   0.746408998966217,\n",
       "   -0.029980391263961792,\n",
       "   -0.09443042427301407,\n",
       "   0.45872536301612854,\n",
       "   0.387508362531662,\n",
       "   -0.21265198290348053,\n",
       "   -0.07020840048789978,\n",
       "   -0.2500961720943451,\n",
       "   -0.34350523352622986,\n",
       "   -0.6404616236686707,\n",
       "   0.4385432004928589,\n",
       "   -0.3225852847099304,\n",
       "   0.32572999596595764,\n",
       "   0.4143334627151489,\n",
       "   -0.07656629383563995,\n",
       "   0.1782957762479782,\n",
       "   0.7073311805725098,\n",
       "   -0.04004232957959175,\n",
       "   0.6580309271812439,\n",
       "   0.17214643955230713,\n",
       "   -0.4292677938938141,\n",
       "   0.13446921110153198,\n",
       "   0.17528881132602692,\n",
       "   -0.21409083902835846,\n",
       "   0.001527841784991324,\n",
       "   -0.1957933008670807,\n",
       "   -0.35962221026420593,\n",
       "   -0.1226646825671196,\n",
       "   -0.6709002256393433,\n",
       "   -0.01388273574411869,\n",
       "   0.018594520166516304,\n",
       "   0.3536868691444397,\n",
       "   0.6433118581771851,\n",
       "   -0.2519237697124481,\n",
       "   0.06898932158946991,\n",
       "   -0.14182962477207184,\n",
       "   -0.5137295126914978,\n",
       "   -0.4543919861316681,\n",
       "   -0.1687382012605667,\n",
       "   0.18941234052181244,\n",
       "   0.6594245433807373,\n",
       "   0.24381324648857117,\n",
       "   0.6103265881538391,\n",
       "   -0.2937887907028198,\n",
       "   0.3371622860431671,\n",
       "   1.0620768070220947,\n",
       "   -0.33909738063812256,\n",
       "   -0.07685018330812454,\n",
       "   -0.2047095000743866,\n",
       "   -0.09979186207056046,\n",
       "   0.28058505058288574,\n",
       "   0.24900399148464203,\n",
       "   -0.14818929135799408,\n",
       "   0.2783282697200775,\n",
       "   -0.1508525311946869,\n",
       "   0.16574545204639435,\n",
       "   0.10170652717351913,\n",
       "   0.250964492559433,\n",
       "   0.058350395411252975,\n",
       "   -0.24396707117557526,\n",
       "   0.30049973726272583,\n",
       "   0.1707598865032196,\n",
       "   0.16095517575740814,\n",
       "   -0.25607576966285706,\n",
       "   0.4637744128704071,\n",
       "   0.05866376310586929,\n",
       "   0.20186066627502441,\n",
       "   0.09791429340839386,\n",
       "   0.9664275646209717,\n",
       "   0.5513370633125305,\n",
       "   -0.07509879022836685,\n",
       "   0.029834669083356857,\n",
       "   -0.4979150891304016,\n",
       "   0.03459133952856064,\n",
       "   0.42882218956947327,\n",
       "   0.26169970631599426,\n",
       "   -0.04570058360695839,\n",
       "   -0.10850834101438522,\n",
       "   0.241525799036026,\n",
       "   0.2013428658246994,\n",
       "   -0.3603682816028595,\n",
       "   0.4849586486816406,\n",
       "   -0.12864571809768677,\n",
       "   0.20942340791225433,\n",
       "   -0.2292465716600418,\n",
       "   -0.35657376050949097,\n",
       "   0.14567561447620392,\n",
       "   -0.014472989365458488,\n",
       "   -0.0878966748714447,\n",
       "   -0.1493716835975647,\n",
       "   0.36802470684051514,\n",
       "   -0.3244100511074066,\n",
       "   -0.32569870352745056,\n",
       "   0.4131494164466858,\n",
       "   0.24584896862506866,\n",
       "   -0.16232897341251373,\n",
       "   0.44300952553749084,\n",
       "   0.21592503786087036,\n",
       "   0.287086546421051],\n",
       "  [0.2704889476299286,\n",
       "   0.1566821038722992,\n",
       "   -0.21538855135440826,\n",
       "   -0.16829931735992432,\n",
       "   0.6598244309425354,\n",
       "   -0.2477935403585434,\n",
       "   0.34117403626441956,\n",
       "   0.015212330035865307,\n",
       "   -0.15180614590644836,\n",
       "   0.2622113525867462,\n",
       "   0.0050264629535377026,\n",
       "   0.3260760009288788,\n",
       "   -0.170437291264534,\n",
       "   0.13932716846466064,\n",
       "   -0.8387953042984009,\n",
       "   0.037362467497587204,\n",
       "   0.28407010436058044,\n",
       "   0.00560168968513608,\n",
       "   -0.02556312456727028,\n",
       "   0.15819929540157318,\n",
       "   0.07068689167499542,\n",
       "   -0.1588667631149292,\n",
       "   0.24196310341358185,\n",
       "   0.05266636237502098,\n",
       "   0.18224552273750305,\n",
       "   0.2875426113605499,\n",
       "   0.32672053575515747,\n",
       "   0.849166989326477,\n",
       "   -0.17913781106472015,\n",
       "   0.5304568409919739,\n",
       "   0.3548373579978943,\n",
       "   0.21336105465888977,\n",
       "   0.3197745680809021,\n",
       "   -0.02487325854599476,\n",
       "   -0.11034005135297775,\n",
       "   0.03818201646208763,\n",
       "   -0.191256582736969,\n",
       "   0.2971954643726349,\n",
       "   -0.23124343156814575,\n",
       "   0.12048667669296265,\n",
       "   -0.040319062769412994,\n",
       "   -0.11449290066957474,\n",
       "   -0.023995503783226013,\n",
       "   0.00301969307474792,\n",
       "   -0.017957095056772232,\n",
       "   -0.13359880447387695,\n",
       "   -0.3251340985298157,\n",
       "   -0.17043517529964447,\n",
       "   0.15771730244159698,\n",
       "   0.3267558217048645,\n",
       "   -0.008189762942492962,\n",
       "   -0.5808811187744141,\n",
       "   -0.24822120368480682,\n",
       "   -0.40274715423583984,\n",
       "   -0.0686522051692009,\n",
       "   -0.16874268651008606,\n",
       "   -0.04123455658555031,\n",
       "   -0.17978867888450623,\n",
       "   -0.514621913433075,\n",
       "   0.37180542945861816,\n",
       "   0.12456651031970978,\n",
       "   0.1685231626033783,\n",
       "   0.6325783729553223,\n",
       "   0.11719659715890884,\n",
       "   -0.18686413764953613,\n",
       "   0.25135815143585205,\n",
       "   0.0014546194579452276,\n",
       "   -0.12507817149162292,\n",
       "   0.1994076669216156,\n",
       "   -0.12600532174110413,\n",
       "   0.009528267197310925,\n",
       "   0.021738912910223007,\n",
       "   -0.03098277561366558,\n",
       "   -0.3972284197807312,\n",
       "   0.47229015827178955,\n",
       "   0.0005992079968564212,\n",
       "   0.2881809175014496,\n",
       "   -0.3466895818710327,\n",
       "   -0.3532631993293762,\n",
       "   -0.03210371360182762,\n",
       "   0.406296968460083,\n",
       "   0.2704188823699951,\n",
       "   0.09890873730182648,\n",
       "   0.3141525089740753,\n",
       "   0.25025343894958496,\n",
       "   -0.010308127850294113,\n",
       "   0.16103748977184296,\n",
       "   0.19846530258655548,\n",
       "   0.15697503089904785,\n",
       "   0.34240201115608215,\n",
       "   -0.0988338440656662,\n",
       "   -0.2441994994878769,\n",
       "   0.06396032869815826,\n",
       "   0.16316257417201996,\n",
       "   -0.4174017608165741,\n",
       "   0.2406664937734604,\n",
       "   -0.4262517988681793,\n",
       "   0.22274039685726166,\n",
       "   -0.03753111511468887,\n",
       "   0.04393374174833298,\n",
       "   -0.1346745640039444,\n",
       "   -0.19136489927768707,\n",
       "   0.27775436639785767,\n",
       "   -0.13864609599113464,\n",
       "   0.15246307849884033,\n",
       "   -0.27373966574668884,\n",
       "   -0.5061296820640564,\n",
       "   -0.7519990801811218,\n",
       "   -0.5665065050125122,\n",
       "   0.3571271002292633,\n",
       "   0.053915608674287796,\n",
       "   0.6686204075813293,\n",
       "   0.40178126096725464,\n",
       "   -0.03640816733241081,\n",
       "   -0.4991779327392578,\n",
       "   -0.12251291424036026,\n",
       "   0.15181931853294373,\n",
       "   0.3484559953212738,\n",
       "   -0.0402369387447834,\n",
       "   -0.39091262221336365,\n",
       "   0.41840028762817383,\n",
       "   0.34488874673843384,\n",
       "   -0.02598952129483223,\n",
       "   -0.22647307813167572,\n",
       "   0.027883244678378105,\n",
       "   -0.0025201623793691397,\n",
       "   -0.18043562769889832,\n",
       "   -0.14192937314510345,\n",
       "   0.23908886313438416,\n",
       "   0.1396646797657013,\n",
       "   -0.278604120016098,\n",
       "   -0.4180937111377716,\n",
       "   -0.005833759438246489,\n",
       "   0.45473524928092957,\n",
       "   0.048730988055467606,\n",
       "   0.12085498124361038,\n",
       "   -0.32972174882888794,\n",
       "   0.03415266051888466,\n",
       "   -0.9670754075050354,\n",
       "   0.07584881782531738,\n",
       "   -0.44971466064453125,\n",
       "   0.10873781144618988,\n",
       "   0.4678407609462738,\n",
       "   -0.1628398448228836,\n",
       "   -0.0406019389629364,\n",
       "   0.08491242676973343,\n",
       "   -0.20084312558174133,\n",
       "   -0.09557782113552094,\n",
       "   0.39163723587989807,\n",
       "   0.29938551783561707,\n",
       "   -0.3853532075881958,\n",
       "   0.18919934332370758,\n",
       "   0.04875316843390465,\n",
       "   0.06813285499811172,\n",
       "   -0.2973158359527588,\n",
       "   -0.11096104979515076,\n",
       "   0.27128034830093384,\n",
       "   -0.7036789655685425,\n",
       "   0.8165763020515442,\n",
       "   0.24340423941612244,\n",
       "   -0.29430168867111206,\n",
       "   -0.4595535099506378,\n",
       "   -0.4243871569633484,\n",
       "   0.3639110028743744,\n",
       "   0.2537541091442108,\n",
       "   -0.6121047139167786,\n",
       "   -0.18234875798225403,\n",
       "   0.2944416105747223,\n",
       "   -0.1177469789981842,\n",
       "   0.6756156086921692,\n",
       "   0.4874630272388458,\n",
       "   0.23089931905269623,\n",
       "   -0.23773233592510223,\n",
       "   -0.06251443922519684,\n",
       "   0.1624116450548172,\n",
       "   0.3346039652824402,\n",
       "   0.2555680572986603,\n",
       "   -0.24654366075992584,\n",
       "   0.30701273679733276,\n",
       "   0.04956797882914543,\n",
       "   0.050376951694488525,\n",
       "   -0.04486841708421707,\n",
       "   -0.2703549861907959,\n",
       "   -0.21052198112010956,\n",
       "   -0.13134141266345978,\n",
       "   -0.23428811132907867,\n",
       "   0.14279954135417938,\n",
       "   -0.7091217637062073,\n",
       "   0.6887481808662415,\n",
       "   -0.0779346227645874,\n",
       "   0.07293049246072769,\n",
       "   -0.31154105067253113,\n",
       "   -0.7176699638366699,\n",
       "   -0.10344760864973068,\n",
       "   -0.8400518298149109,\n",
       "   0.17481452226638794,\n",
       "   0.315400093793869,\n",
       "   0.20919567346572876,\n",
       "   0.07392918318510056,\n",
       "   0.7409330606460571,\n",
       "   -0.16798008978366852,\n",
       "   0.11457108706235886,\n",
       "   -0.06842157989740372,\n",
       "   0.07183758914470673,\n",
       "   0.05610297992825508,\n",
       "   0.021548647433519363,\n",
       "   -0.36041730642318726,\n",
       "   0.5176117420196533,\n",
       "   -0.18345896899700165,\n",
       "   -0.41684237122535706,\n",
       "   -0.17852967977523804,\n",
       "   -0.3432011306285858,\n",
       "   0.10425775498151779,\n",
       "   0.0008237215224653482,\n",
       "   0.32671648263931274,\n",
       "   -0.3826330900192261,\n",
       "   0.17936132848262787,\n",
       "   -0.3918711841106415,\n",
       "   0.17378777265548706,\n",
       "   0.08455296605825424,\n",
       "   -0.3839455544948578,\n",
       "   -0.022221187129616737,\n",
       "   0.6806159615516663,\n",
       "   -0.1360488384962082,\n",
       "   -0.03499940037727356,\n",
       "   0.7893762588500977,\n",
       "   -0.48201701045036316,\n",
       "   0.4446515440940857,\n",
       "   0.46896639466285706,\n",
       "   -0.015163179486989975,\n",
       "   0.1909482181072235,\n",
       "   0.2803393304347992,\n",
       "   -0.026473844423890114,\n",
       "   -0.3884860873222351,\n",
       "   -0.28053566813468933,\n",
       "   0.5652724504470825,\n",
       "   -0.19074390828609467,\n",
       "   -0.1835089772939682,\n",
       "   -0.2625313103199005,\n",
       "   -0.5773026943206787,\n",
       "   0.1741524636745453,\n",
       "   -0.005111993756145239,\n",
       "   0.6079859137535095,\n",
       "   -0.05640951171517372,\n",
       "   -0.17008301615715027,\n",
       "   -0.11082751303911209,\n",
       "   -0.11141221970319748,\n",
       "   0.40516918897628784,\n",
       "   -0.11774946749210358,\n",
       "   0.24378558993339539,\n",
       "   -0.3240915536880493,\n",
       "   -0.0712694600224495,\n",
       "   -0.38785961270332336,\n",
       "   -0.0541035458445549,\n",
       "   0.23277077078819275,\n",
       "   -0.21814323961734772,\n",
       "   0.5615981221199036,\n",
       "   -0.11108923703432083,\n",
       "   0.05532669275999069,\n",
       "   -0.5107290744781494,\n",
       "   0.5725888013839722,\n",
       "   0.013936524279415607,\n",
       "   0.28695857524871826,\n",
       "   -0.16210509836673737,\n",
       "   -0.07882026582956314,\n",
       "   -0.11265001446008682,\n",
       "   -0.4859992563724518,\n",
       "   -0.04930370673537254,\n",
       "   -0.5126752853393555,\n",
       "   0.43138036131858826,\n",
       "   0.09454572945833206,\n",
       "   -0.0901263877749443,\n",
       "   0.3865424394607544,\n",
       "   0.1449027806520462,\n",
       "   -0.4029698967933655,\n",
       "   0.5476502180099487,\n",
       "   0.3105771243572235,\n",
       "   -0.4348989725112915,\n",
       "   0.13938412070274353,\n",
       "   -0.10260365903377533,\n",
       "   0.35837963223457336,\n",
       "   0.6478506922721863,\n",
       "   -0.3321208953857422,\n",
       "   0.5317217707633972,\n",
       "   -0.5623788237571716,\n",
       "   -0.5293629169464111,\n",
       "   0.1615431010723114,\n",
       "   0.7328397035598755,\n",
       "   0.14152471721172333,\n",
       "   -0.11595266312360764,\n",
       "   0.047750189900398254,\n",
       "   -0.33966293931007385,\n",
       "   -0.2776382565498352,\n",
       "   0.542287290096283,\n",
       "   0.3527221977710724,\n",
       "   0.06423904746770859,\n",
       "   -0.3293789327144623,\n",
       "   0.507475733757019,\n",
       "   -0.4263392388820648,\n",
       "   -0.2720251977443695,\n",
       "   0.20316463708877563,\n",
       "   -0.025758421048521996,\n",
       "   0.33031225204467773,\n",
       "   0.04340102896094322,\n",
       "   -0.6946108937263489,\n",
       "   0.6312723159790039,\n",
       "   0.1047837883234024,\n",
       "   0.2238142341375351,\n",
       "   0.2445373237133026,\n",
       "   0.49006301164627075,\n",
       "   0.46956124901771545,\n",
       "   0.22519251704216003,\n",
       "   -0.10173380374908447,\n",
       "   -0.033450089395046234,\n",
       "   -0.1409032791852951,\n",
       "   -0.3817378878593445,\n",
       "   -0.04903393238782883,\n",
       "   0.012665797024965286,\n",
       "   0.0009604305378161371,\n",
       "   0.030626747757196426,\n",
       "   -0.17692938446998596,\n",
       "   0.4990815818309784,\n",
       "   -0.41582152247428894,\n",
       "   0.22434107959270477,\n",
       "   -0.05617408826947212,\n",
       "   0.1388859748840332,\n",
       "   -0.09369789063930511,\n",
       "   0.48757660388946533,\n",
       "   -0.10114531964063644,\n",
       "   -0.22522130608558655,\n",
       "   -0.10824283212423325,\n",
       "   -0.10733162611722946,\n",
       "   -0.10540460795164108,\n",
       "   0.39563706517219543,\n",
       "   -0.11213980615139008,\n",
       "   0.061641544103622437,\n",
       "   0.02432710863649845,\n",
       "   0.47014376521110535,\n",
       "   -0.548873782157898,\n",
       "   0.25769615173339844,\n",
       "   0.015597130171954632,\n",
       "   -0.3209715783596039,\n",
       "   0.11687345057725906,\n",
       "   -0.35220229625701904,\n",
       "   -0.11630919575691223,\n",
       "   0.31123819947242737,\n",
       "   0.3925224244594574,\n",
       "   0.03631749004125595,\n",
       "   0.30670323967933655,\n",
       "   -0.23669539391994476,\n",
       "   -0.3549070954322815,\n",
       "   0.0812682956457138,\n",
       "   -0.17264576256275177,\n",
       "   -0.29193323850631714,\n",
       "   0.37778979539871216,\n",
       "   0.4306504726409912,\n",
       "   0.0679822787642479,\n",
       "   0.09067796915769577,\n",
       "   0.18065930902957916,\n",
       "   -0.1249755322933197,\n",
       "   -0.18020468950271606,\n",
       "   -0.27074912190437317,\n",
       "   0.05156572908163071,\n",
       "   -0.09962492436170578,\n",
       "   -0.20803318917751312,\n",
       "   -0.05708950385451317,\n",
       "   -0.5023210048675537,\n",
       "   -0.29205772280693054,\n",
       "   0.06015932932496071,\n",
       "   0.2345990538597107,\n",
       "   0.5431780219078064,\n",
       "   -0.3666069507598877,\n",
       "   0.5095208287239075,\n",
       "   0.3262034058570862,\n",
       "   0.3090537488460541,\n",
       "   -0.03678462281823158,\n",
       "   -0.024213120341300964,\n",
       "   0.263068288564682,\n",
       "   0.18294432759284973,\n",
       "   -0.8472194075584412,\n",
       "   0.18613332509994507,\n",
       "   0.010789887979626656,\n",
       "   -0.19137883186340332,\n",
       "   0.04862619563937187,\n",
       "   0.22032701969146729,\n",
       "   -0.07183381170034409,\n",
       "   0.058266155421733856,\n",
       "   -0.3075862526893616,\n",
       "   0.5217293500900269,\n",
       "   -0.2900807857513428,\n",
       "   0.2820419669151306,\n",
       "   -0.05290219932794571,\n",
       "   0.29706650972366333,\n",
       "   -0.36826783418655396,\n",
       "   0.17399181425571442,\n",
       "   0.032663606107234955,\n",
       "   -0.6018847227096558,\n",
       "   0.5473199486732483,\n",
       "   0.25602850317955017,\n",
       "   -0.34285104274749756,\n",
       "   0.16273368895053864,\n",
       "   0.25029057264328003,\n",
       "   -0.21448469161987305,\n",
       "   0.26656603813171387,\n",
       "   -0.6228357553482056,\n",
       "   -0.322553813457489,\n",
       "   -0.01614350453019142,\n",
       "   0.42706769704818726,\n",
       "   0.013884011656045914,\n",
       "   -0.92833411693573,\n",
       "   0.054453495889902115,\n",
       "   -0.44055822491645813,\n",
       "   0.2837664783000946,\n",
       "   0.8677451610565186,\n",
       "   0.1835465282201767,\n",
       "   -0.15333056449890137,\n",
       "   0.1767890304327011,\n",
       "   -0.21311889588832855,\n",
       "   0.028456589207053185,\n",
       "   0.22995461523532867,\n",
       "   -0.16822530329227448,\n",
       "   -0.47774213552474976,\n",
       "   -0.250521183013916,\n",
       "   -0.2404150664806366,\n",
       "   0.4411167800426483,\n",
       "   0.20260226726531982,\n",
       "   -0.06036391854286194,\n",
       "   -0.24747216701507568,\n",
       "   -0.21246196329593658,\n",
       "   -0.00353547022677958,\n",
       "   -0.06224364414811134,\n",
       "   -0.07603613287210464,\n",
       "   0.06815077364444733,\n",
       "   0.3610629141330719,\n",
       "   -0.019025756046175957,\n",
       "   0.2320881187915802,\n",
       "   -0.05716833099722862,\n",
       "   0.18874616920948029,\n",
       "   0.05222032219171524,\n",
       "   -0.28047263622283936,\n",
       "   0.18863411247730255,\n",
       "   -0.5492251515388489,\n",
       "   0.05304454639554024,\n",
       "   -0.8842459917068481,\n",
       "   0.2214244157075882,\n",
       "   -0.2661969065666199,\n",
       "   -0.1766316145658493,\n",
       "   0.24325722455978394,\n",
       "   -0.12930546700954437,\n",
       "   -0.119296133518219,\n",
       "   0.43411150574684143,\n",
       "   0.09164772182703018,\n",
       "   0.34538760781288147,\n",
       "   0.20361298322677612,\n",
       "   0.3434419333934784,\n",
       "   0.08104643225669861,\n",
       "   -0.36214107275009155,\n",
       "   -0.13482341170310974,\n",
       "   -0.6124115586280823,\n",
       "   0.16778524219989777,\n",
       "   -0.2492198646068573,\n",
       "   0.45567306876182556,\n",
       "   -2.0997113097109832e-05,\n",
       "   0.2646808922290802,\n",
       "   0.21099461615085602,\n",
       "   0.32898715138435364,\n",
       "   -0.29379892349243164,\n",
       "   0.7859096527099609,\n",
       "   -0.06815669685602188,\n",
       "   0.26211458444595337,\n",
       "   0.09641850739717484,\n",
       "   0.3679996132850647,\n",
       "   0.13283702731132507,\n",
       "   0.9933959245681763,\n",
       "   -0.037785276770591736,\n",
       "   0.3191249966621399,\n",
       "   0.17931845784187317,\n",
       "   -0.19380782544612885,\n",
       "   -0.34329718351364136,\n",
       "   0.22006316483020782,\n",
       "   -0.3294244408607483,\n",
       "   0.16755275428295135,\n",
       "   -0.31496694684028625,\n",
       "   0.03364858776330948,\n",
       "   0.09576740860939026,\n",
       "   0.27099302411079407,\n",
       "   0.9067676067352295,\n",
       "   0.24268917739391327,\n",
       "   0.11451932042837143,\n",
       "   0.4106087386608124,\n",
       "   0.27445393800735474,\n",
       "   0.10705753415822983,\n",
       "   -0.03656598925590515,\n",
       "   -0.22545550763607025,\n",
       "   0.20897828042507172,\n",
       "   0.4915686249732971,\n",
       "   0.08622283488512039,\n",
       "   0.2210005819797516,\n",
       "   0.4244522750377655,\n",
       "   0.13810855150222778,\n",
       "   -0.29599303007125854,\n",
       "   0.040714941918849945,\n",
       "   -0.13757911324501038,\n",
       "   0.16460353136062622,\n",
       "   -0.6933709979057312,\n",
       "   0.4662778973579407,\n",
       "   -0.028910096734762192,\n",
       "   0.06575077772140503,\n",
       "   -0.03242700174450874,\n",
       "   0.06256315857172012,\n",
       "   -0.13140791654586792,\n",
       "   -0.5594702363014221,\n",
       "   -0.30025649070739746,\n",
       "   -0.1959654688835144,\n",
       "   -0.13473722338676453,\n",
       "   0.1879427582025528,\n",
       "   0.4960445761680603,\n",
       "   0.10643766820430756,\n",
       "   0.12519869208335876,\n",
       "   -0.039609797298908234,\n",
       "   0.39036300778388977,\n",
       "   0.1082136258482933,\n",
       "   -0.1705687940120697,\n",
       "   0.08031871169805527,\n",
       "   0.6037557125091553,\n",
       "   -0.25391602516174316,\n",
       "   0.44372305274009705,\n",
       "   0.26769334077835083,\n",
       "   -0.16261030733585358,\n",
       "   -0.018006883561611176,\n",
       "   -0.3149087131023407,\n",
       "   -0.09894690662622452,\n",
       "   0.1339249312877655,\n",
       "   -0.37458521127700806,\n",
       "   1.176595687866211,\n",
       "   0.2271740734577179,\n",
       "   0.1342245489358902,\n",
       "   -0.14145663380622864,\n",
       "   -0.052575625479221344,\n",
       "   -0.12496079504489899,\n",
       "   0.0035106816794723272,\n",
       "   0.7553290128707886,\n",
       "   0.108204185962677,\n",
       "   0.3113078773021698,\n",
       "   0.007498712278902531,\n",
       "   0.4806961119174957,\n",
       "   0.10182838886976242,\n",
       "   0.531169056892395,\n",
       "   0.06917502731084824,\n",
       "   0.13883450627326965,\n",
       "   0.3566824495792389,\n",
       "   0.40600576996803284,\n",
       "   -0.7701289057731628,\n",
       "   0.43623194098472595,\n",
       "   0.609973132610321,\n",
       "   0.5601496696472168,\n",
       "   -0.517615795135498,\n",
       "   -8.88224983215332,\n",
       "   0.08591778576374054,\n",
       "   0.19002841413021088,\n",
       "   0.0005196048296056688,\n",
       "   0.06014157459139824,\n",
       "   0.1513286679983139,\n",
       "   0.3351221978664398,\n",
       "   -0.21233195066452026,\n",
       "   0.3100106120109558,\n",
       "   0.16985052824020386,\n",
       "   -0.091232530772686,\n",
       "   -0.16459955275058746,\n",
       "   0.49757516384124756,\n",
       "   -0.5497258901596069,\n",
       "   0.03979501500725746,\n",
       "   0.0012767205480486155,\n",
       "   -0.14203590154647827,\n",
       "   -0.06944171339273453,\n",
       "   0.16114650666713715,\n",
       "   -0.22323735058307648,\n",
       "   -0.6192087531089783,\n",
       "   -0.029661882668733597,\n",
       "   0.19930438697338104,\n",
       "   0.25604620575904846,\n",
       "   0.0004925106186419725,\n",
       "   0.23917876183986664,\n",
       "   -0.2522036135196686,\n",
       "   -0.1969243437051773,\n",
       "   -0.06634720414876938,\n",
       "   -0.37312978506088257,\n",
       "   -0.2408735752105713,\n",
       "   -0.2192090004682541,\n",
       "   -0.1971621960401535,\n",
       "   -0.015185783617198467,\n",
       "   0.04840538650751114,\n",
       "   -0.25037500262260437,\n",
       "   0.10146448016166687,\n",
       "   0.17023912072181702,\n",
       "   -0.1420925259590149,\n",
       "   -0.37562963366508484,\n",
       "   -0.23042622208595276,\n",
       "   0.16234371066093445,\n",
       "   0.43024852871894836,\n",
       "   -0.10344851016998291,\n",
       "   0.19775862991809845,\n",
       "   -0.11231040954589844,\n",
       "   0.42236605286598206,\n",
       "   0.23827409744262695,\n",
       "   0.08778727054595947,\n",
       "   0.4836825132369995,\n",
       "   0.5206188559532166,\n",
       "   0.3055417537689209,\n",
       "   -0.06565064936876297,\n",
       "   -0.16468866169452667,\n",
       "   -0.5692191123962402,\n",
       "   -0.113853819668293,\n",
       "   -0.13268132507801056,\n",
       "   0.38718846440315247,\n",
       "   -0.3004985451698303,\n",
       "   0.0009189682314172387,\n",
       "   -0.18314868211746216,\n",
       "   -0.40699490904808044,\n",
       "   0.4006217122077942,\n",
       "   -0.3280356824398041,\n",
       "   -0.3620861768722534,\n",
       "   0.06971760094165802,\n",
       "   -0.6421966552734375,\n",
       "   0.14855167269706726,\n",
       "   0.06987312436103821,\n",
       "   -0.08107913285493851,\n",
       "   -0.5309675335884094,\n",
       "   -0.5628451108932495,\n",
       "   -0.1761656254529953,\n",
       "   0.29762622714042664,\n",
       "   -0.040588460862636566,\n",
       "   -0.4840904474258423,\n",
       "   -0.6053865551948547,\n",
       "   0.15387167036533356,\n",
       "   -0.3144032061100006,\n",
       "   0.019195709377527237,\n",
       "   -0.8628643155097961,\n",
       "   0.8346050381660461,\n",
       "   0.5519164800643921,\n",
       "   0.00583373848348856,\n",
       "   -0.08096780627965927,\n",
       "   -0.3248247802257538,\n",
       "   -0.15598860383033752,\n",
       "   0.15913896262645721,\n",
       "   -0.06319792568683624,\n",
       "   0.3458782434463501,\n",
       "   0.000776916102040559,\n",
       "   -0.3449370861053467,\n",
       "   -0.1980772465467453,\n",
       "   -0.05000704154372215,\n",
       "   -0.1122734546661377,\n",
       "   0.0567355640232563,\n",
       "   -0.36781319975852966,\n",
       "   -0.33050423860549927,\n",
       "   0.3967895805835724,\n",
       "   -0.36023372411727905,\n",
       "   -0.6372102499008179,\n",
       "   -0.12956351041793823,\n",
       "   -0.262824147939682,\n",
       "   0.10332441329956055,\n",
       "   -0.20214860141277313,\n",
       "   -0.3992137610912323,\n",
       "   0.31066569685935974,\n",
       "   -0.47039884328842163,\n",
       "   0.005399932153522968,\n",
       "   -0.16022586822509766,\n",
       "   0.12886910140514374,\n",
       "   0.26433131098747253,\n",
       "   0.4062957763671875,\n",
       "   -0.557068407535553,\n",
       "   -0.23155717551708221,\n",
       "   0.04284530505537987,\n",
       "   0.9930723905563354,\n",
       "   0.010312997736036777,\n",
       "   0.057962242513895035,\n",
       "   -0.052084073424339294,\n",
       "   -0.077812559902668,\n",
       "   -0.5032777190208435,\n",
       "   0.24828574061393738,\n",
       "   0.3395993709564209,\n",
       "   0.16209331154823303,\n",
       "   0.05484495684504509,\n",
       "   0.049876388162374496,\n",
       "   0.05022110044956207,\n",
       "   0.09724512696266174,\n",
       "   -0.14259417355060577,\n",
       "   0.5513697266578674,\n",
       "   0.011150040663778782,\n",
       "   -0.01186341792345047,\n",
       "   0.352288156747818,\n",
       "   0.23027724027633667,\n",
       "   -0.27351319789886475,\n",
       "   0.049760863184928894,\n",
       "   -0.39139848947525024,\n",
       "   -0.3082927167415619,\n",
       "   -0.10738836228847504,\n",
       "   -0.5475690960884094,\n",
       "   -0.04023744910955429,\n",
       "   0.4157305359840393,\n",
       "   0.06600774079561234,\n",
       "   0.2638891637325287,\n",
       "   -0.47503662109375,\n",
       "   -0.053101323544979095,\n",
       "   0.13399028778076172,\n",
       "   -0.1496349424123764,\n",
       "   0.12503598630428314,\n",
       "   -0.44540008902549744,\n",
       "   -0.02000846154987812,\n",
       "   0.1000993400812149,\n",
       "   -0.08962812274694443,\n",
       "   0.2815771698951721,\n",
       "   -0.14359743893146515,\n",
       "   0.1987588256597519,\n",
       "   0.06333234906196594,\n",
       "   -0.23100996017456055,\n",
       "   0.05210791528224945,\n",
       "   -0.30702170729637146,\n",
       "   0.1284564882516861,\n",
       "   -0.019364921376109123,\n",
       "   0.10840258002281189,\n",
       "   -0.2854323089122772,\n",
       "   0.09791972488164902,\n",
       "   -0.3957374393939972,\n",
       "   0.3237032890319824,\n",
       "   0.40673309564590454,\n",
       "   -0.3031902015209198,\n",
       "   0.26429104804992676,\n",
       "   0.18985022604465485,\n",
       "   -0.14861319959163666,\n",
       "   0.22332735359668732,\n",
       "   0.029322218149900436,\n",
       "   -0.3148605525493622,\n",
       "   0.12900373339653015,\n",
       "   -0.5674054622650146,\n",
       "   -0.3473989963531494,\n",
       "   -0.1631489098072052,\n",
       "   0.10660894960165024,\n",
       "   -0.2072908729314804,\n",
       "   -0.45603445172309875,\n",
       "   -0.14816002547740936,\n",
       "   -0.2616685628890991,\n",
       "   0.14073722064495087,\n",
       "   0.027410760521888733,\n",
       "   0.15168032050132751,\n",
       "   0.15766368806362152,\n",
       "   0.17745612561702728,\n",
       "   -0.047319795936346054,\n",
       "   0.22014430165290833,\n",
       "   -0.6324769854545593,\n",
       "   0.07844559103250504,\n",
       "   -0.055423326790332794,\n",
       "   -0.0643179789185524,\n",
       "   -0.1477411836385727,\n",
       "   0.2943066656589508,\n",
       "   -0.2886042594909668,\n",
       "   0.049444347620010376,\n",
       "   -0.008627912029623985,\n",
       "   -0.16417953372001648,\n",
       "   -0.30130064487457275,\n",
       "   -0.4188288748264313,\n",
       "   0.10776584595441818,\n",
       "   -0.4128086566925049,\n",
       "   0.10787157714366913,\n",
       "   -0.05177251622080803,\n",
       "   -0.08718125522136688,\n",
       "   0.31653523445129395,\n",
       "   -0.023057743906974792],\n",
       "  [0.9966168999671936,\n",
       "   0.5051448941230774,\n",
       "   0.07963500916957855,\n",
       "   1.0646260976791382,\n",
       "   -0.08048919588327408,\n",
       "   -0.6290380954742432,\n",
       "   0.4297928512096405,\n",
       "   0.47899430990219116,\n",
       "   0.11691686511039734,\n",
       "   0.043897781521081924,\n",
       "   -0.25387969613075256,\n",
       "   0.21233026683330536,\n",
       "   -1.1743443012237549,\n",
       "   -0.17467465996742249,\n",
       "   -1.2236238718032837,\n",
       "   -0.5252813100814819,\n",
       "   -0.2666269838809967,\n",
       "   0.008942785672843456,\n",
       "   0.421027809381485,\n",
       "   0.49823591113090515,\n",
       "   -0.08733642101287842,\n",
       "   0.22606496512889862,\n",
       "   -0.47304001450538635,\n",
       "   -0.03792477771639824,\n",
       "   0.22226709127426147,\n",
       "   -0.3419159948825836,\n",
       "   0.6340203881263733,\n",
       "   2.0982704162597656,\n",
       "   0.028518812730908394,\n",
       "   1.1393635272979736,\n",
       "   0.21930713951587677,\n",
       "   -0.004971769638359547,\n",
       "   0.4880193769931793,\n",
       "   0.5020003318786621,\n",
       "   -0.5341132283210754,\n",
       "   0.4986732602119446,\n",
       "   -0.32173657417297363,\n",
       "   0.7203003764152527,\n",
       "   -0.7215859889984131,\n",
       "   0.5728530287742615,\n",
       "   -0.15725736320018768,\n",
       "   0.4148959815502167,\n",
       "   0.7037257552146912,\n",
       "   -0.8287937045097351,\n",
       "   0.7197131514549255,\n",
       "   -0.5470261573791504,\n",
       "   -0.6034754514694214,\n",
       "   0.560438871383667,\n",
       "   -1.074593424797058,\n",
       "   1.1847881078720093,\n",
       "   -0.10053883492946625,\n",
       "   -0.7975979447364807,\n",
       "   0.21009446680545807,\n",
       "   0.42202648520469666,\n",
       "   0.7596242427825928,\n",
       "   -0.6827674508094788,\n",
       "   -0.18409490585327148,\n",
       "   0.28066638112068176,\n",
       "   -0.42032450437545776,\n",
       "   0.28198733925819397,\n",
       "   -0.35841211676597595,\n",
       "   0.11121641844511032,\n",
       "   1.0599956512451172,\n",
       "   0.26284828782081604,\n",
       "   -1.1338157653808594,\n",
       "   0.2574010193347931,\n",
       "   0.7782741785049438,\n",
       "   -0.563504695892334,\n",
       "   -0.13029907643795013,\n",
       "   -0.15814101696014404,\n",
       "   1.0779767036437988,\n",
       "   0.08295059204101562,\n",
       "   -0.10979369282722473,\n",
       "   -0.7740585207939148,\n",
       "   0.28531283140182495,\n",
       "   -0.8226656913757324,\n",
       "   0.2338958978652954,\n",
       "   -0.1773068755865097,\n",
       "   -0.6278395652770996,\n",
       "   0.515824556350708,\n",
       "   0.6460173726081848,\n",
       "   0.5094988942146301,\n",
       "   -0.6028517484664917,\n",
       "   -0.25484827160835266,\n",
       "   0.9487481713294983,\n",
       "   1.568331003189087,\n",
       "   0.061843883246183395,\n",
       "   -0.8710119128227234,\n",
       "   -0.06904324144124985,\n",
       "   -0.2558548152446747,\n",
       "   0.4358595609664917,\n",
       "   -0.3820742964744568,\n",
       "   0.13903579115867615,\n",
       "   -0.056464433670043945,\n",
       "   -0.16141115128993988,\n",
       "   -0.0704694613814354,\n",
       "   -0.8051488995552063,\n",
       "   -0.06709546595811844,\n",
       "   -0.6068891882896423,\n",
       "   0.438849538564682,\n",
       "   -0.7309330105781555,\n",
       "   0.36923304200172424,\n",
       "   0.1278732419013977,\n",
       "   0.6440828442573547,\n",
       "   0.35313984751701355,\n",
       "   -0.2366175800561905,\n",
       "   -0.11988367885351181,\n",
       "   -1.7992644309997559,\n",
       "   -0.14372310042381287,\n",
       "   1.2027798891067505,\n",
       "   -0.5309543609619141,\n",
       "   0.32936885952949524,\n",
       "   -0.023165106773376465,\n",
       "   -0.8793080449104309,\n",
       "   -0.28262513875961304,\n",
       "   0.5906843543052673,\n",
       "   0.1617889106273651,\n",
       "   0.24590538442134857,\n",
       "   0.15462496876716614,\n",
       "   0.23513919115066528,\n",
       "   0.31806236505508423,\n",
       "   0.68306964635849,\n",
       "   -0.44232606887817383,\n",
       "   0.24310731887817383,\n",
       "   0.22383856773376465,\n",
       "   0.6882491111755371,\n",
       "   -0.7403789758682251,\n",
       "   -0.7198991179466248,\n",
       "   0.6861617565155029,\n",
       "   0.5845162272453308,\n",
       "   -0.3591892719268799,\n",
       "   -0.3482930660247803,\n",
       "   0.8548250794410706,\n",
       "   0.6806478500366211,\n",
       "   0.5622512102127075,\n",
       "   -0.3217059075832367,\n",
       "   0.02165728062391281,\n",
       "   0.14443977177143097,\n",
       "   -5.382905006408691,\n",
       "   -0.7796133160591125,\n",
       "   -0.8941162824630737,\n",
       "   -0.0055999415926635265,\n",
       "   1.3077349662780762,\n",
       "   -0.6629648804664612,\n",
       "   0.08606072515249252,\n",
       "   0.20400412380695343,\n",
       "   -0.38359537720680237,\n",
       "   0.4012024998664856,\n",
       "   -0.43124470114707947,\n",
       "   0.39637643098831177,\n",
       "   -0.6387624740600586,\n",
       "   -0.2424989640712738,\n",
       "   0.039793722331523895,\n",
       "   -0.8439100384712219,\n",
       "   -0.43808427453041077,\n",
       "   -0.5735235810279846,\n",
       "   0.27648937702178955,\n",
       "   0.0558021143078804,\n",
       "   0.5104981064796448,\n",
       "   0.051192108541727066,\n",
       "   -0.6289911866188049,\n",
       "   -0.7949190735816956,\n",
       "   -0.4085772633552551,\n",
       "   -0.5195651054382324,\n",
       "   0.51056307554245,\n",
       "   -0.6848453283309937,\n",
       "   0.3668154776096344,\n",
       "   0.2042338103055954,\n",
       "   -1.0965375900268555,\n",
       "   1.0480024814605713,\n",
       "   0.4561804234981537,\n",
       "   -0.8430496454238892,\n",
       "   -0.8177809715270996,\n",
       "   0.08612021058797836,\n",
       "   0.5965108871459961,\n",
       "   0.8865605592727661,\n",
       "   0.34649011492729187,\n",
       "   0.0013877124292775989,\n",
       "   0.165696382522583,\n",
       "   -0.5799515843391418,\n",
       "   0.21883176267147064,\n",
       "   0.4020368158817291,\n",
       "   -0.2286018431186676,\n",
       "   -0.42904579639434814,\n",
       "   -0.36325228214263916,\n",
       "   -0.7682769894599915,\n",
       "   0.15228895843029022,\n",
       "   0.37369227409362793,\n",
       "   1.6406638622283936,\n",
       "   0.11247320473194122,\n",
       "   0.2428351491689682,\n",
       "   -0.6825170516967773,\n",
       "   -1.2728891372680664,\n",
       "   0.2696455419063568,\n",
       "   -0.5900677442550659,\n",
       "   -0.7516140937805176,\n",
       "   -0.9755038022994995,\n",
       "   -0.15209734439849854,\n",
       "   0.382369726896286,\n",
       "   1.010291337966919,\n",
       "   -0.5913687348365784,\n",
       "   -0.013176951557397842,\n",
       "   -0.44378745555877686,\n",
       "   0.06705687195062637,\n",
       "   -0.0760844275355339,\n",
       "   -0.13592247664928436,\n",
       "   -0.30840808153152466,\n",
       "   0.05930526554584503,\n",
       "   -0.30219972133636475,\n",
       "   -1.9560679197311401,\n",
       "   -0.2783450484275818,\n",
       "   0.28361114859580994,\n",
       "   0.46100157499313354,\n",
       "   -0.20101284980773926,\n",
       "   0.2601601183414459,\n",
       "   -0.7664262056350708,\n",
       "   -0.5993596315383911,\n",
       "   -0.055508218705654144,\n",
       "   0.024367518723011017,\n",
       "   -0.24750129878520966,\n",
       "   0.4292953908443451,\n",
       "   0.021415453404188156,\n",
       "   0.24583397805690765,\n",
       "   -0.8333985209465027,\n",
       "   0.2704007625579834,\n",
       "   0.8741844296455383,\n",
       "   1.2351616621017456,\n",
       "   0.4104229807853699,\n",
       "   0.9193061590194702,\n",
       "   0.2944333851337433,\n",
       "   0.9814882874488831,\n",
       "   0.3362218141555786,\n",
       "   -0.33557307720184326,\n",
       "   -1.0878207683563232,\n",
       "   -0.25340980291366577,\n",
       "   0.5105868577957153,\n",
       "   0.09396006166934967,\n",
       "   0.41363149881362915,\n",
       "   -0.7450945377349854,\n",
       "   -0.16089147329330444,\n",
       "   -0.338284969329834,\n",
       "   -0.0302728284150362,\n",
       "   0.7167403697967529,\n",
       "   -0.5249881744384766,\n",
       "   -0.01639566943049431,\n",
       "   0.3375849425792694,\n",
       "   0.03130852058529854,\n",
       "   0.7587048411369324,\n",
       "   0.11407095938920975,\n",
       "   0.378921240568161,\n",
       "   -0.05915963277220726,\n",
       "   0.009813380427658558,\n",
       "   -0.43507418036460876,\n",
       "   -0.1467123031616211,\n",
       "   0.7179781794548035,\n",
       "   -0.1963999718427658,\n",
       "   0.6415843963623047,\n",
       "   0.31631404161453247,\n",
       "   -0.06818921864032745,\n",
       "   -0.822092592716217,\n",
       "   0.11204072833061218,\n",
       "   -0.14812278747558594,\n",
       "   0.5145488977432251,\n",
       "   -0.19566485285758972,\n",
       "   -0.1641717106103897,\n",
       "   0.12814438343048096,\n",
       "   -0.278413325548172,\n",
       "   0.3698034882545471,\n",
       "   -0.42356616258621216,\n",
       "   0.47679510712623596,\n",
       "   0.36251914501190186,\n",
       "   -0.5520097017288208,\n",
       "   -0.04420401528477669,\n",
       "   0.4539164900779724,\n",
       "   -0.07958497107028961,\n",
       "   1.0332520008087158,\n",
       "   0.5768619775772095,\n",
       "   -0.14286598563194275,\n",
       "   -0.33160528540611267,\n",
       "   -1.1887609958648682,\n",
       "   -0.2750045955181122,\n",
       "   0.5394744873046875,\n",
       "   -0.32952815294265747,\n",
       "   1.3745572566986084,\n",
       "   -0.9100782871246338,\n",
       "   0.6111763119697571,\n",
       "   0.05957794934511185,\n",
       "   0.7276809215545654,\n",
       "   -0.2569981515407562,\n",
       "   -0.09530457109212875,\n",
       "   -0.07372764497995377,\n",
       "   -0.7207304835319519,\n",
       "   -2.1627869606018066,\n",
       "   -0.6804229021072388,\n",
       "   0.33078864216804504,\n",
       "   -0.6422057747840881,\n",
       "   -0.5625908374786377,\n",
       "   0.1534266620874405,\n",
       "   -0.7942052483558655,\n",
       "   -0.2707167863845825,\n",
       "   0.41013026237487793,\n",
       "   0.49176913499832153,\n",
       "   0.1012629047036171,\n",
       "   -0.17397192120552063,\n",
       "   -6.40937614440918,\n",
       "   -0.0465901643037796,\n",
       "   0.3637155592441559,\n",
       "   0.41334837675094604,\n",
       "   -0.5569249987602234,\n",
       "   0.10336075723171234,\n",
       "   1.4915560483932495,\n",
       "   0.32203301787376404,\n",
       "   -0.341882586479187,\n",
       "   0.5405731201171875,\n",
       "   0.09602413326501846,\n",
       "   -1.0446789264678955,\n",
       "   0.059054307639598846,\n",
       "   -0.2988525927066803,\n",
       "   -0.7720312476158142,\n",
       "   0.2632538080215454,\n",
       "   0.44042178988456726,\n",
       "   1.3648982048034668,\n",
       "   -0.5464102029800415,\n",
       "   0.6322746872901917,\n",
       "   -0.953704833984375,\n",
       "   -0.06843577325344086,\n",
       "   -0.7914631962776184,\n",
       "   0.9899362325668335,\n",
       "   0.08795998245477676,\n",
       "   -1.2708137035369873,\n",
       "   0.8117465376853943,\n",
       "   -0.5911769866943359,\n",
       "   -0.16479794681072235,\n",
       "   -0.3419395089149475,\n",
       "   -0.8356945514678955,\n",
       "   -0.25207051634788513,\n",
       "   -0.2521522045135498,\n",
       "   0.3393944203853607,\n",
       "   -0.8077844977378845,\n",
       "   0.36378106474876404,\n",
       "   0.9284154176712036,\n",
       "   0.242136150598526,\n",
       "   -0.03469124436378479,\n",
       "   -0.7579527497291565,\n",
       "   -0.05295749381184578,\n",
       "   -0.05127827078104019,\n",
       "   -0.1221049576997757,\n",
       "   -0.1475236564874649,\n",
       "   -0.3573981523513794,\n",
       "   0.1402631551027298,\n",
       "   -0.47300124168395996,\n",
       "   -0.1428396850824356,\n",
       "   -0.29832345247268677,\n",
       "   -1.038833498954773,\n",
       "   0.21773679554462433,\n",
       "   -0.058681197464466095,\n",
       "   0.3419226109981537,\n",
       "   0.2445361167192459,\n",
       "   -0.4015962481498718,\n",
       "   1.1249369382858276,\n",
       "   0.2955513894557953,\n",
       "   0.2319306880235672,\n",
       "   0.864277184009552,\n",
       "   -0.26469603180885315,\n",
       "   -1.0354653596878052,\n",
       "   0.07946725189685822,\n",
       "   -0.5151568651199341,\n",
       "   0.009704827331006527,\n",
       "   0.004450289066880941,\n",
       "   0.7084024548530579,\n",
       "   1.580001950263977,\n",
       "   -0.12735779583454132,\n",
       "   0.12497884035110474,\n",
       "   -0.21149352192878723,\n",
       "   0.42236965894699097,\n",
       "   0.4437720775604248,\n",
       "   -0.12323874235153198,\n",
       "   -0.17022475600242615,\n",
       "   -0.37636348605155945,\n",
       "   -2.5258705615997314,\n",
       "   0.12171721458435059,\n",
       "   0.20285636186599731,\n",
       "   0.46400678157806396,\n",
       "   -0.32884979248046875,\n",
       "   0.3366886377334595,\n",
       "   0.07762907445430756,\n",
       "   -0.1014433279633522,\n",
       "   -0.07687824219465256,\n",
       "   0.6196880340576172,\n",
       "   0.6219305992126465,\n",
       "   0.730476975440979,\n",
       "   -0.17463287711143494,\n",
       "   0.4130706787109375,\n",
       "   -0.8491224050521851,\n",
       "   -1.5872989892959595,\n",
       "   -0.26810282468795776,\n",
       "   -0.9491369724273682,\n",
       "   0.41002053022384644,\n",
       "   -0.5752220749855042,\n",
       "   -0.36705806851387024,\n",
       "   -0.7260388135910034,\n",
       "   -0.1550842672586441,\n",
       "   -0.6914814710617065,\n",
       "   0.05365879088640213,\n",
       "   -0.6965552568435669,\n",
       "   -0.44687074422836304,\n",
       "   -0.06434597074985504,\n",
       "   0.523719310760498,\n",
       "   -0.046263907104730606,\n",
       "   0.2958787977695465,\n",
       "   -0.7005382776260376,\n",
       "   -0.7283099889755249,\n",
       "   0.29247742891311646,\n",
       "   1.048648715019226,\n",
       "   0.7302667498588562,\n",
       "   -1.5917123556137085,\n",
       "   0.18622945249080658,\n",
       "   0.1252831369638443,\n",
       "   0.5190650224685669,\n",
       "   0.6232808828353882,\n",
       "   0.7696922421455383,\n",
       "   -0.25548964738845825,\n",
       "   0.0664399117231369,\n",
       "   -0.441997230052948,\n",
       "   1.3128165006637573,\n",
       "   0.4945227801799774,\n",
       "   0.5364695191383362,\n",
       "   -0.2478456348180771,\n",
       "   0.029972169548273087,\n",
       "   0.43332940340042114,\n",
       "   -0.009822933934628963,\n",
       "   0.32519617676734924,\n",
       "   -0.6948181986808777,\n",
       "   0.6346612572669983,\n",
       "   -0.7995088696479797,\n",
       "   -0.09101318567991257,\n",
       "   -0.6642318964004517,\n",
       "   0.6526349186897278,\n",
       "   -0.7232749462127686,\n",
       "   -0.5824539661407471,\n",
       "   -0.022724991664290428,\n",
       "   -0.9558683633804321,\n",
       "   -0.3048456907272339,\n",
       "   -1.6428204774856567,\n",
       "   -0.16110624372959137,\n",
       "   -0.5148828029632568,\n",
       "   -0.11817045509815216,\n",
       "   0.45777928829193115,\n",
       "   0.8849979043006897,\n",
       "   -1.02391517162323,\n",
       "   -0.149305522441864,\n",
       "   0.13701805472373962,\n",
       "   -0.19286927580833435,\n",
       "   -0.4469333589076996,\n",
       "   0.5568046569824219,\n",
       "   0.22600223124027252,\n",
       "   0.9051114320755005,\n",
       "   0.09258101135492325,\n",
       "   -0.10196765512228012,\n",
       "   1.2752867937088013,\n",
       "   -1.3741133213043213,\n",
       "   1.2203527688980103,\n",
       "   -1.004939079284668,\n",
       "   1.2225936651229858,\n",
       "   5.112769603729248,\n",
       "   0.4303185045719147,\n",
       "   0.055198028683662415,\n",
       "   1.0985205173492432,\n",
       "   -0.6836612820625305,\n",
       "   -0.051340896636247635,\n",
       "   -0.47957083582878113,\n",
       "   0.29901957511901855,\n",
       "   1.598543405532837,\n",
       "   1.5668598413467407,\n",
       "   -0.40182432532310486,\n",
       "   -0.07427553832530975,\n",
       "   -0.5434637665748596,\n",
       "   0.5273312330245972,\n",
       "   -0.12276135385036469,\n",
       "   0.6446512341499329,\n",
       "   0.38230931758880615,\n",
       "   0.24053889513015747,\n",
       "   -0.6423051357269287,\n",
       "   -0.4800141453742981,\n",
       "   1.1436495780944824,\n",
       "   0.10598520934581757,\n",
       "   0.7766116857528687,\n",
       "   -0.029249999672174454,\n",
       "   -0.016469145193696022,\n",
       "   0.5901612043380737,\n",
       "   0.2668987214565277,\n",
       "   0.1501656025648117,\n",
       "   -0.7249333262443542,\n",
       "   0.39419010281562805,\n",
       "   0.44486743211746216,\n",
       "   0.864647388458252,\n",
       "   -0.05879271402955055,\n",
       "   0.296902060508728,\n",
       "   -0.18510477244853973,\n",
       "   0.36294472217559814,\n",
       "   0.13071702420711517,\n",
       "   -0.3848944306373596,\n",
       "   -0.13767024874687195,\n",
       "   0.4585378170013428,\n",
       "   0.09381721913814545,\n",
       "   1.0631777048110962,\n",
       "   0.06681753695011139,\n",
       "   -0.5895482897758484,\n",
       "   -0.1401912122964859,\n",
       "   -1.2560532093048096,\n",
       "   -0.4526909589767456,\n",
       "   -1.6102303266525269,\n",
       "   -0.1578635275363922,\n",
       "   -0.03284459188580513,\n",
       "   -0.16954651474952698,\n",
       "   0.09797997772693634,\n",
       "   0.9523444175720215,\n",
       "   0.11758315563201904,\n",
       "   0.45506078004837036,\n",
       "   0.19058772921562195,\n",
       "   -0.4279686510562897,\n",
       "   -0.4608752131462097,\n",
       "   -1.6068918704986572,\n",
       "   -0.01125053595751524,\n",
       "   0.10770009458065033,\n",
       "   0.0888144001364708,\n",
       "   0.8807694911956787,\n",
       "   0.6028737425804138,\n",
       "   0.4714268147945404,\n",
       "   0.11569835245609283,\n",
       "   -1.0927037000656128,\n",
       "   -0.555263876914978,\n",
       "   -0.18707175552845,\n",
       "   -0.18943911790847778,\n",
       "   1.2592535018920898,\n",
       "   -0.08302122354507446,\n",
       "   -0.42439913749694824,\n",
       "   0.004646526649594307,\n",
       "   0.12677554786205292,\n",
       "   0.28445327281951904,\n",
       "   0.17360207438468933,\n",
       "   0.9757004380226135,\n",
       "   0.014936447143554688,\n",
       "   -0.3695821762084961,\n",
       "   -0.10539412498474121,\n",
       "   0.42237570881843567,\n",
       "   -0.22875599563121796,\n",
       "   1.032848596572876,\n",
       "   0.2999240756034851,\n",
       "   0.49591532349586487,\n",
       "   0.4197913706302643,\n",
       "   0.8442890644073486,\n",
       "   -1.7034056186676025,\n",
       "   0.5264269709587097,\n",
       "   1.239797830581665,\n",
       "   0.08535662293434143,\n",
       "   -0.9382030963897705,\n",
       "   -1.5194189548492432,\n",
       "   0.4530245363712311,\n",
       "   0.14635150134563446,\n",
       "   0.18718335032463074,\n",
       "   -0.20729316771030426,\n",
       "   0.36814287304878235,\n",
       "   0.7737182378768921,\n",
       "   0.04379668086767197,\n",
       "   0.4584104120731354,\n",
       "   0.4226534366607666,\n",
       "   -0.7509169578552246,\n",
       "   0.5669096112251282,\n",
       "   0.23469670116901398,\n",
       "   -0.04661572352051735,\n",
       "   -0.338193416595459,\n",
       "   0.18430359661579132,\n",
       "   -0.03263033553957939,\n",
       "   -0.7767752408981323,\n",
       "   0.8298140168190002,\n",
       "   0.7776117324829102,\n",
       "   0.5364068150520325,\n",
       "   0.48094049096107483,\n",
       "   -0.07273264229297638,\n",
       "   0.2522660791873932,\n",
       "   0.5163940191268921,\n",
       "   0.78644859790802,\n",
       "   -0.3075466752052307,\n",
       "   -0.3332900106906891,\n",
       "   0.6611077189445496,\n",
       "   -0.7583490610122681,\n",
       "   -0.4271779954433441,\n",
       "   0.43606504797935486,\n",
       "   -0.5050902366638184,\n",
       "   -0.0008139899582602084,\n",
       "   0.2606758177280426,\n",
       "   -1.368190050125122,\n",
       "   0.16441211104393005,\n",
       "   -0.04791705682873726,\n",
       "   0.046414002776145935,\n",
       "   -1.6265348196029663,\n",
       "   -0.6035724878311157,\n",
       "   0.16399377584457397,\n",
       "   -0.3901684880256653,\n",
       "   0.9699581265449524,\n",
       "   0.5005286931991577,\n",
       "   0.034054744988679886,\n",
       "   -0.8668599724769592,\n",
       "   0.24439910054206848,\n",
       "   0.18158769607543945,\n",
       "   0.49089616537094116,\n",
       "   5.377182483673096,\n",
       "   0.25533995032310486,\n",
       "   -0.5807626247406006,\n",
       "   -0.8583127856254578,\n",
       "   -0.40705060958862305,\n",
       "   0.44609153270721436,\n",
       "   -0.30213892459869385,\n",
       "   0.7278071641921997,\n",
       "   0.46428677439689636,\n",
       "   -0.6722900867462158,\n",
       "   -0.484165757894516,\n",
       "   -0.5266551971435547,\n",
       "   1.8777134418487549,\n",
       "   -0.15462079644203186,\n",
       "   -1.0588326454162598,\n",
       "   0.9768960475921631,\n",
       "   -0.934913694858551,\n",
       "   0.2663835883140564,\n",
       "   0.6076730489730835,\n",
       "   -0.6981247663497925,\n",
       "   -1.0044281482696533,\n",
       "   -0.2843930423259735,\n",
       "   0.3644404709339142,\n",
       "   -0.10854515433311462,\n",
       "   -0.10744345188140869,\n",
       "   -0.16359031200408936,\n",
       "   -1.286490797996521,\n",
       "   0.21947742998600006,\n",
       "   -1.082360863685608,\n",
       "   -0.3427116274833679,\n",
       "   -0.6361171007156372,\n",
       "   0.3854300081729889,\n",
       "   1.1239668130874634,\n",
       "   -0.3336482048034668,\n",
       "   0.20753996074199677,\n",
       "   -1.0489848852157593,\n",
       "   -0.006863444112241268,\n",
       "   0.5844831466674805,\n",
       "   0.3371303975582123,\n",
       "   0.7886572480201721,\n",
       "   -0.16389818489551544,\n",
       "   -0.4167410731315613,\n",
       "   -0.5695075392723083,\n",
       "   0.2233905792236328,\n",
       "   -1.2867100238800049,\n",
       "   -0.26526689529418945,\n",
       "   -0.2036408632993698,\n",
       "   0.3655514121055603,\n",
       "   0.381023108959198,\n",
       "   0.8225749731063843,\n",
       "   -1.0550479888916016,\n",
       "   -0.3023071587085724,\n",
       "   -0.8101992011070251,\n",
       "   -0.34596768021583557,\n",
       "   0.4030512273311615,\n",
       "   -0.5622057914733887,\n",
       "   -0.00959702767431736,\n",
       "   -0.24118593335151672,\n",
       "   -0.4373783767223358,\n",
       "   0.1373283863067627,\n",
       "   -0.0890805795788765,\n",
       "   -0.22050029039382935,\n",
       "   0.8654414415359497,\n",
       "   0.2782454192638397,\n",
       "   -0.05349360778927803,\n",
       "   0.7777310013771057,\n",
       "   0.7855011820793152,\n",
       "   -0.47951680421829224,\n",
       "   0.1498071700334549,\n",
       "   -0.1903293877840042,\n",
       "   0.4283089339733124,\n",
       "   -1.2399762868881226,\n",
       "   0.32876282930374146,\n",
       "   -0.30284643173217773,\n",
       "   0.09259230643510818,\n",
       "   -0.3208226263523102,\n",
       "   0.2410888522863388,\n",
       "   -0.5625882148742676,\n",
       "   -0.06966029852628708,\n",
       "   -0.33742204308509827,\n",
       "   1.1417272090911865,\n",
       "   0.24445295333862305,\n",
       "   -0.7591789960861206,\n",
       "   -0.01609971560537815,\n",
       "   0.50249183177948,\n",
       "   -0.40348178148269653,\n",
       "   0.007405699696391821,\n",
       "   -0.832991898059845,\n",
       "   -0.9079856872558594,\n",
       "   -0.14958155155181885,\n",
       "   -0.883924663066864,\n",
       "   0.08374589681625366,\n",
       "   0.6902539134025574,\n",
       "   0.2695484757423401,\n",
       "   1.088568925857544,\n",
       "   -0.47279101610183716,\n",
       "   -0.17973731458187103,\n",
       "   -0.47555696964263916,\n",
       "   -0.35999250411987305,\n",
       "   0.7644365429878235,\n",
       "   -0.07563011348247528,\n",
       "   0.23733848333358765,\n",
       "   0.15481609106063843,\n",
       "   -0.8402565121650696,\n",
       "   0.16034536063671112,\n",
       "   -0.5869187712669373,\n",
       "   0.443844199180603,\n",
       "   0.3376533091068268,\n",
       "   -0.775661289691925,\n",
       "   -0.3447466492652893,\n",
       "   -1.3999536037445068,\n",
       "   -0.35335445404052734,\n",
       "   0.16453224420547485,\n",
       "   -0.6445146799087524,\n",
       "   -0.39252981543540955,\n",
       "   0.48214706778526306,\n",
       "   -0.6707289814949036,\n",
       "   -0.3503211736679077,\n",
       "   0.5845310688018799,\n",
       "   0.4962245523929596,\n",
       "   -0.059254225343465805,\n",
       "   -0.3247285485267639,\n",
       "   0.38811126351356506,\n",
       "   0.8882651329040527,\n",
       "   -0.01973639987409115,\n",
       "   0.010696951299905777,\n",
       "   0.19900038838386536,\n",
       "   -0.058501992374658585,\n",
       "   0.6227803230285645,\n",
       "   -0.31818094849586487,\n",
       "   0.13128970563411713,\n",
       "   -0.1555286943912506,\n",
       "   0.02386578358709812,\n",
       "   -0.5985124707221985,\n",
       "   0.3148367404937744,\n",
       "   -0.09440997987985611,\n",
       "   0.460196316242218,\n",
       "   0.6278017163276672,\n",
       "   -0.5441293716430664,\n",
       "   0.3171488344669342,\n",
       "   -0.990204930305481,\n",
       "   -0.44530653953552246,\n",
       "   -0.7756787538528442,\n",
       "   1.1484860181808472,\n",
       "   0.5179705023765564,\n",
       "   0.396702378988266,\n",
       "   -0.23297885060310364,\n",
       "   -1.2480072975158691,\n",
       "   -0.3433884382247925,\n",
       "   0.9208349585533142,\n",
       "   0.47731414437294006,\n",
       "   -0.5381407737731934,\n",
       "   0.6531153321266174,\n",
       "   -0.30533578991889954,\n",
       "   -0.2754021883010864,\n",
       "   0.32471978664398193,\n",
       "   0.4298003017902374,\n",
       "   0.3646903336048126,\n",
       "   -0.161582350730896,\n",
       "   0.6605662703514099,\n",
       "   -0.1673630326986313]]]"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eDw1hdw_vIH"
   },
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them.\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions and the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 998
    },
    "colab_type": "code",
    "id": "-hmKXX4m0DSe",
    "outputId": "cb325650-b568-4549-b9c8-b2c541d92b57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this exercise, we first use a sentence to test the meaning of words modeled by BERT.\n",
      "The text is as follows: I bought a computer from Apple store in the big apple and eat an apple and an orange on my way back\n",
      "[CLS]           101\n",
      "i             1,045\n",
      "bought        4,149\n",
      "a             1,037\n",
      "computer      3,274\n",
      "from          2,013\n",
      "apple         6,207\n",
      "store         3,573\n",
      "in            1,999\n",
      "the           1,996\n",
      "big           2,502\n",
      "apple         6,207\n",
      "and           1,998\n",
      "eat           4,521\n",
      "an            2,019\n",
      "apple         6,207\n",
      "and           1,998\n",
      "an            2,019\n",
      "orange        4,589\n",
      "on            2,006\n",
      "my            2,026\n",
      "way           2,126\n",
      "back          2,067\n",
      ".             1,012\n",
      "[SEP]           102\n",
      " \n",
      "The same word may have different meanings in BERT. Here is first 5 vector values for several words.\n",
      "\n",
      "computer   [-0.4419652   0.38158175 -0.28876624 -0.5917116   1.9018192 ]\n",
      "apple (store)   [ 0.11290839  0.09668598 -0.03233772 -0.57972157  1.5654407 ]\n",
      "(the big) apple [-0.23825629  0.30330437  0.62161016 -0.52176327  2.0015793 ]\n",
      "(eat an) apple  [-0.39477226  1.336863   -0.90837115 -0.8911237   0.75193185]\n",
      "orange  [-0.9411976   0.7390771  -0.56432897 -0.43837705  1.1940875 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEuCAYAAABVgc95AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEdEzSToREdEzSToREdEz/x8uzaKZRucTjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "It is obvious that Apple Inc. is close to computer, while the fruit apple is close to orange. The big apple is not close to any one of them, though.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "tokenizer = BertTokenizer.from_pretrained('../content/mybert/vocab.txt')\n",
    "print(\"In this exercise, we first use a sentence to test the meaning of words modeled by BERT.\")\n",
    "print(\"The text is as follows: I bought a computer from Apple store in the big apple and eat an apple and an orange on my way back\")\n",
    "text = \"I bought a computer from Apple store in the big apple and eat an apple and an orange on my way back.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "model_embedding = BertModel.from_pretrained('../content/mybert')\n",
    "model_embedding.eval()\n",
    "output = model_embedding(tokens_tensor, segments_tensors)\n",
    "word_embeddings, sentence_embedding = output\n",
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)\n",
    "print(\" \")\n",
    "print('The same word may have different meanings in BERT. Here is first 5 vector values for several words.')\n",
    "print('')\n",
    "print(\"computer  \", str(token_vecs[4][:5]))\n",
    "print(\"apple (store)  \", str(token_vecs[6][:5]))\n",
    "print(\"(the big) apple\", str(token_vecs[11][:5]))\n",
    "print(\"(eat an) apple \", str(token_vecs[15][:5]))\n",
    "print(\"orange \", str(token_vecs[18][:5]))\n",
    "thearray=np.array([token_vecs[18],token_vecs[15],token_vecs[11],token_vecs[4],token_vecs[6]])\n",
    "heatmapMatrixC = []\n",
    "for i in thearray:\n",
    "    column = []\n",
    "    for j in thearray:\n",
    "        column.append(1-cosine(i,j))\n",
    "    heatmapMatrixC.append(column)\n",
    "heatmapMatrixC = np.array(heatmapMatrixC)\n",
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixC, cmap='BuPu')\n",
    "cbar = plt.colorbar(hmap)\n",
    "cbar.set_label('cosine similarity')\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
    "a = ax.set_xticklabels(['orange','apple (fruit)','big apple','computer','Apple (Inc.)'], minor=False,rotation=270)\n",
    "a = ax.set_yticklabels(['orange','apple (fruit)','big apple','computer','Apple (Inc.)'], minor=False)\n",
    "plt.show()\n",
    "print(\" \")\n",
    "print(\"It is obvious that Apple Inc. is close to computer, while the fruit apple is close to orange. The big apple is not close to any one of them, though.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361,
     "referenced_widgets": [
      "79a635b14f984f20b209e7e3dae38e56",
      "54b79428b57744e696a8bee377960d2f",
      "52574793d4324d098cc8826a1eb23212",
      "7a0a55e08bfb4d9baa0777f6944936ae",
      "ed7a086eb21143cd86db4196d1e669c0",
      "e921a62ea30847178f0ec3f0b077d52d",
      "de054ece27e24ec7b7cbf067edbb2519",
      "49fdd42cc5b0438cbd334a8fd66e7716",
      "85a4f525707c4556a9c5ba9133154fd7",
      "b0cacd0e5c00408d9760a97c2b8f2592",
      "cbdf54eef1794e98bfc5ecbffdd49e59",
      "16e58bf38bca4a7791e8a0bcf38d22bb",
      "bab4c89b8a46433da1649903e8832caf",
      "3843c77b6fb44d36aeb29f590d7b014e",
      "fe20bb95b2784f4a8d15f0961440298c",
      "6ffafda6efc244eabd05a499661e1efe"
     ]
    },
    "colab_type": "code",
    "id": "lVcmMj82ZKcD",
    "outputId": "cc07e82b-07e7-4477-be42-39450024bb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we use pipeline to do some exploration.\n",
      " \n",
      "First, let's do some sentiment analysis.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a635b14f984f20b209e7e3dae38e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1, The service needs improvements.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9973888}]\n",
      "2, I will certainly buy mobile phone from Sony.\n",
      "[{'label': 'POSITIVE', 'score': 0.81283975}]\n",
      " \n",
      "Next, question-answering task.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a4f525707c4556a9c5ba9133154fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our question: What is my favorite product from Apple? Our answer: I don't really like ipad, iphone is not good either, but I love ipod.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 409.52it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 4660.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.23145787603156087, 'start': 56, 'end': 60, 'answer': 'ipod'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Now we use pipeline to do some exploration.\")\n",
    "from transformers import pipeline\n",
    "print(\" \")\n",
    "print(\"First, let's do some sentiment analysis.\")\n",
    "nlp_sentiment = pipeline('sentiment-analysis')\n",
    "print(\"1, The service needs improvements.\")\n",
    "print(nlp_sentiment(\"The service needs improvements.\"))\n",
    "print(\"2, I will certainly buy mobile phone from Sony.\")\n",
    "print(nlp_sentiment(\"I will certainly buy mobile phone from Sony.\"))\n",
    "print(\" \")\n",
    "print(\"Next, question-answering task.\")\n",
    "nlp_question = pipeline('question-answering')\n",
    "print(\"Our question: What is my favorite product from Apple? Our answer: I don't really like ipad, iphone is not bad, but I love ipod most.\")\n",
    "print(nlp_question({\n",
    "    'question': 'What is my favorite product from Apple?',\n",
    "    'context': \"I don't really like ipad, iphone is not bad, but I love ipod most.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Wd-I8pV_vII"
   },
   "source": [
    "## Text Generation using BERT\n",
    "\n",
    "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "67a0e8fcdcfa49fd8ce607169fd76c90",
      "0aa7acf3f9c84a498b0e583105e86bd8",
      "e0482685b18a4b08a4d47cf94b16d2d0",
      "27e8637530184116b237a411245705a5",
      "4108dada4e7a436db3c1bf1d62eb71ce",
      "910717bfec544ddfaad283feb5467b3a",
      "bcb6792329da49cb853b67a24837f4ea",
      "40009705f2d3474f9535cfc69d010642",
      "6411f5ce1ec147c787581e3a7fdf0d50",
      "b87879eb1fb643cbba9f4e49cdeb9921",
      "78349090137f4988a315a27f5eee097a",
      "dcc90a4b6edf461eb1f4785d94bb699c",
      "0c69931beb6e4e4fa379eb61a4855770",
      "1b7054cd78724aaeb2c677f04a2eba92",
      "d582e8dc3b464327abf3e4da0a7f9519",
      "cd7e975035794a52ae29cca9542b788d",
      "c4714f21d8514605a5d00cc86b897f25",
      "a673e9e0877144d28b7318f6c9f8842f",
      "7519dd0cc6b74d19afe6b2c290be14be",
      "5fbb258deee44ee09493d75ec60555b2",
      "4b62b06e0f98429c9390451a3bbce25d",
      "c9e1b5fbfcf341e8a35d6b3c43ab3b13",
      "7018b77e581340e3b70b9c78e81f94d3",
      "2a56afe24de74da59653273a363d76a2",
      "8350cd49576b4add8baa17b646de87c1",
      "bbd6d784245c4d2791c1a56a90e29292",
      "7acff30ed03845f390d2d28cd19142a4",
      "1fd2d488f6ab465386ac54de28cfa8be",
      "7c9b659972c542ff8c46a9b6f3601d9d",
      "f8306512600a449d9abc49c9c1d8b134",
      "8d9dc4691f09421b82249759d59c52c3",
      "c566e7dfc8004acb91afe56d80d2cd09"
     ]
    },
    "colab_type": "code",
    "id": "CMSZ0ALb_vII",
    "outputId": "7569ea51-40e8-470f-9933-8ee0278976eb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a0e8fcdcfa49fd8ce607169fd76c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=224, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6411f5ce1ec147c787581e3a7fdf0d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4714f21d8514605a5d00cc86b897f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8350cd49576b4add8baa17b646de87c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=548118077, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "AROxpGpV_vIJ",
    "outputId": "08420fe1-1f72-4a41-9f7d-134ca6a3f92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and to take away half the joy from this year will be to see a lot of teams get some new ideas, or not, or there was someone else in charge of this year.\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0voUlxS_vIM"
   },
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "This [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzn81y2a_vIN"
   },
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wisOoNIa_vIO"
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../content/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../content/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFUSiRKf_vIQ"
   },
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "h145Fz-E_vIR",
    "outputId": "9e12b4d6-ca23-417c-98ab-20d40b74b2c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>682703233492619264</td>\n",
       "      <td>I would like to wish everyone A HAPPY AND HEAL...</td>\n",
       "      <td>2015-12-31 23:21:49+00:00</td>\n",
       "      <td>6776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16495</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>682700657304989696</td>\n",
       "      <td>Do you believe that The State Department, on N...</td>\n",
       "      <td>2015-12-31 23:11:35+00:00</td>\n",
       "      <td>2755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6824</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>682635132038176768</td>\n",
       "      <td>THANK YOU ILLINOIS! Let's not forget to get fa...</td>\n",
       "      <td>2015-12-31 18:51:12+00:00</td>\n",
       "      <td>2468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6047</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>682605293856002048</td>\n",
       "      <td>HAPPY BIRTHDAY to my son, @DonaldJTrumpJr! Ver...</td>\n",
       "      <td>2015-12-31 16:52:38+00:00</td>\n",
       "      <td>2080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8416</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>682578783912759296</td>\n",
       "      <td>I would feel sorry for @JebBush and how badly ...</td>\n",
       "      <td>2015-12-31 15:07:18+00:00</td>\n",
       "      <td>1875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5780</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source              id_str  ... favorite_count is_retweet\n",
       "0  Twitter for Android  682703233492619264  ...          16495      False\n",
       "1  Twitter for Android  682700657304989696  ...           6824      False\n",
       "2   Twitter for iPhone  682635132038176768  ...           6047      False\n",
       "3   Twitter for iPhone  682605293856002048  ...           8416      False\n",
       "4  Twitter for Android  682578783912759296  ...           5780      False\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqzqP39E_vIV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "irV46gi0_vIY",
    "outputId": "155edeab-5722-4ac2-cd50-89771264398d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206    Feels good to be home after seven months, but ...\n",
       "379     .@secupp, who can't believe that her candidate...\n",
       "1948    â€œDo more, be more, give more--and everyone wil...\n",
       "3950    â€œDonald Trump: Iâ€™ve made up my mind on 2016â€ h...\n",
       "4125    \"@pink_sprnva: @pastormike7 @thehill I agree w...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 174,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uU0IQ5B-_vIZ"
   },
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImgV6leV_vIb"
   },
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7z8OBGp_vIc"
   },
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fc7XLW__vId"
   },
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ujOq6SMHApS9",
    "outputId": "18de28ab-8af7-4991-b4d4-64ef46cf6af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
      "  Cloning git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git to /tmp/pip-req-build-uun9v0xd\n",
      "  Running command git clone -q git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git /tmp/pip-req-build-uun9v0xd\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.17.5)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.25.3)\n",
      "Collecting python-docx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.5MB 3.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (6.2.2)\n",
      "Collecting pdfminer2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/97/bd2a2de878438c27ffd710b5d6562c7a0230b0f3ca86059ec635ed231eb1/pdfminer2-20151206-py2.py3-none-any.whl (117kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 52.6MB/s \n",
      "\u001b[?25hCollecting GitPython\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 73.0MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wordcloud in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.22.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.2.5)\n",
      "Requirement already satisfied, skipping upgrade: gensim in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.1.3)\n",
      "Collecting pyanno3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/1a/ee2b136ea0283adb2a9302c29594127f84b6e34cb0b02b91c63bed0a534b/pyanno3-2.0.2.tar.gz (76kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 12.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (4.6.3)\n",
      "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.11.15)\n",
      "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.4)\n",
      "Collecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
      "Collecting speechrecognition\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.8MB 82kB/s \n",
      "\u001b[?25hCollecting pysoundfile\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: IPython in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (5.5.0)\n",
      "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.1.9)\n",
      "Collecting transformers==2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481kB 48.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.2.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->lucem-illud-2020==8.0.1) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->lucem-illud-2020==8.0.1) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx->lucem-illud-2020==8.0.1) (4.2.6)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from pdfminer2->lucem-illud-2020==8.0.1) (1.12.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 12.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lucem-illud-2020==8.0.1) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->lucem-illud-2020==8.0.1) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (1.1.0)\n",
      "Collecting traits\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/af/c6dc88130106d69e4f9a192043c85ed4cb522f83b9041b8691f0b0678405/traits-6.0.0.tar.gz (441kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450kB 77.0MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (1.14.15)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->lucem-illud-2020==8.0.1) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=0.6 in /usr/local/lib/python3.6/dist-packages (from pysoundfile->lucem-illud-2020==8.0.1) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->lucem-illud-2020==8.0.1) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->lucem-illud-2020==8.0.1) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (2.1.3)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (1.0.18)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (7.0.8)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.2.4)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (3.0.12)\n",
      "Collecting tokenizers==0.0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 44.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.27.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras->lucem-illud-2020==8.0.1) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->lucem-illud-2020==8.0.1) (3.13)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->lucem-illud-2020==8.0.1) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->lucem-illud-2020==8.0.1) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=0.6->pysoundfile->lucem-illud-2020==8.0.1) (2.19)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->lucem-illud-2020==8.0.1) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->lucem-illud-2020==8.0.1) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->lucem-illud-2020==8.0.1) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->lucem-illud-2020==8.0.1) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->lucem-illud-2020==8.0.1) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->lucem-illud-2020==8.0.1) (1.0.0)\n",
      "Building wheels for collected packages: lucem-illud-2020, python-docx, pyanno3, traits\n",
      "  Building wheel for lucem-illud-2020 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lucem-illud-2020: filename=lucem_illud_2020-8.0.1-cp36-none-any.whl size=35133 sha256=eb89e3cb514c4b385735947eacce0b176e2cb597f133f3190752e144b330ea28\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-drv2u5uc/wheels/a8/16/91/3c63788e494d360378317fe5ec9f4972f661844af8ae8c26f0\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=2a4bba5ef90ee6e485e36bac0c59a1088ad908357eded5348e1a8ba94d5e385b\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
      "  Building wheel for pyanno3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyanno3: filename=pyanno3-2.0.2-cp36-none-any.whl size=116993 sha256=08585a286f3a07a6a8762b9999da327af9d4126c4aa0099d3ae5e66b2be7a954\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/5d/f6/7c1618b7471ec03ac97f6913baba31ae007003c4fa2bc99855\n",
      "  Building wheel for traits (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for traits: filename=traits-6.0.0-cp36-cp36m-linux_x86_64.whl size=385394 sha256=bd6ac97d863e8c81c6d8ca2cd86f1c8ddb207a327fb3ff9c76862ce06b2ae36f\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/a7/f0/d1dfae8d3a4e5638a40818830c741c1c0e9f8a590b9ea22935\n",
      "Successfully built lucem-illud-2020 python-docx pyanno3 traits\n",
      "Installing collected packages: python-docx, pdfminer2, smmap, gitdb, GitPython, traits, pyanno3, pydub, speechrecognition, pysoundfile, tokenizers, transformers, lucem-illud-2020\n",
      "  Found existing installation: tokenizers 0.5.2\n",
      "    Uninstalling tokenizers-0.5.2:\n",
      "      Successfully uninstalled tokenizers-0.5.2\n",
      "  Found existing installation: transformers 2.5.1\n",
      "    Uninstalling transformers-2.5.1:\n",
      "      Successfully uninstalled transformers-2.5.1\n",
      "Successfully installed GitPython-3.1.0 gitdb-4.0.2 lucem-illud-2020-8.0.1 pdfminer2-20151206 pyanno3-2.0.2 pydub-0.23.1 pysoundfile-0.9.0.post1 python-docx-0.8.10 smmap-3.0.1 speechrecognition-3.8.1 tokenizers-0.0.11 traits-6.0.0 transformers-2.4.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tokenizers",
         "transformers"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7pGQELt_vIe"
   },
   "outputs": [],
   "source": [
    "address = \"/Users/bhargavvader/Downloads/Academics_tech/corpora/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQp7m6TG_vIh"
   },
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3dzsTEy_vIj",
    "outputId": "a4b876a4-0c35-468a-ba8f-e9bc7cfabfcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_us_blog_jfy.zip\n"
     ]
    }
   ],
   "source": [
    "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSM57ztV_vIk",
    "outputId": "2cd32103-fd0b-46a3-8f83-8bc63bd8a0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_gb_blog_akq.zip\n"
     ]
    }
   ],
   "source": [
    "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E775q3QB_vIn"
   },
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UqcpEcY_vIn",
    "outputId": "1a15a509-cee8-4cef-b7bd-9c9c462c0207"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"< h > There Will Be Blood < p > Symbol < p > TWBLD < p > Status < p > Inactive < p > Delist Date < p > Feb 19 2008 < p > Genre < p > Drama < p > MPAA Rating < p > R < p > Phase < p > Release < p > Release Date < p > Jan 25 2008 < p > Gross < p > $ 40,218,903 < p > Theaters < p > n a < h > Description < p > Loosely based on the novel Oil by Upton Sinclair There Will Be Blood chronicles the ugly side of oil drilling in 1920 's California Daniel Day Lewis plays a man who buys a family ranch and strikes black gold when he finds crude oil The prospector becomes wealthy but is also destroyed by\""
      ]
     },
     "execution_count": 248,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3Q7_0Iw_vIq"
   },
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8D51lZE_vIu"
   },
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtUGkOB7_vIw"
   },
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-gaTjsE_vI0"
   },
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fg31RDfX_vI1"
   },
   "source": [
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5K301Boj_vI2"
   },
   "outputs": [],
   "source": [
    "# !python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMxAM-yd_vI4"
   },
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqTdkMgw_vI4"
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jc3z2upQ_vI5"
   },
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qwjNWCu_vI6"
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhwzjiwQ_vI7"
   },
   "source": [
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-YddgU3_vI-"
   },
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXbkq7Kc_vI-"
   },
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j35xqPtu_vJB"
   },
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rvoxp7y_vJD",
    "outputId": "5cd819d6-40e2-4cea-8b78-f314f0154e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
      "\"\"\"@jeff_mcclaren: @realDonaldTrump @realDonaldTrump @foxandfriends @megynkelly @\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_cuXBYS_vJF"
   },
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fovaOsP5_vJF",
    "outputId": "8f7ac277-8e6d-48fe-8944-52e6e1dbe43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CrfMP4u0_vJG"
   },
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBKg-9LT_vJH"
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "f042a38cc7b24a5e8a80669573d940ad",
      "94c91ef43dce4460b08a36cbe0d25f16",
      "fb7c9e6d106043caad53cce96c89b484",
      "7b68753c33cc487db8941fae011fc6f6",
      "b494596a80e740c4b78ce7cc1b4102f3",
      "b9fefded8b384a19a5370b5694fe97cd",
      "21e86d0a58414b618e9b1e44ae0807f8",
      "cd7f78deed014a2ebda3f1a1f71c7510",
      "e6c0481c0bfb465ba265bcdfad70951c",
      "ee95415ffe6b42c1adcadfd1bf0f9ad9",
      "1cacdf03f2fc4ee2b6201d699daeb6c3",
      "0d6be7f7746e4e21b34658476fe1cc63",
      "7ece8e8c8c7e40378209c1d208096bab",
      "e57097898c444dc59fc6bdf63569479e",
      "ddeeea9f818f4da29fef748f1a11c6ed",
      "325a94541107432d8c4f2f162582d0cc",
      "d5c91fb5e7ea4308a2585f2406632f82",
      "e608bc0b294147488585d19d64164fa0",
      "0ba929806a8b450f937a4d279d4cdcc1",
      "919cefdc63b84520a870546833239c4a",
      "03ec901a81824e29b1d79c5d479e1060",
      "df4f9a6441854f9599cf310df5dacfb9",
      "40c30550ae044c3c81bb99d07b2b243f",
      "a8da77a81bcc49538821479245cb868d",
      "0f786775f4844100a7420e86619dbaef",
      "e1b3ac54a9e84afeb4614e6b8f8ff4ba",
      "53a2dfa96c284fa9b6c5e270e4513816",
      "684f038732ea49f1ab30a8984b415378",
      "ce12705a19764d7eaf45685f0cd31f8a",
      "b921eb0bb0f94d73a168dd1b5ddafa47",
      "5b0e47b9c7d74e6ca45746080c1c45b2",
      "a4555fb305254c9e9d7fb63d9bb26bec"
     ]
    },
    "colab_type": "code",
    "id": "m_PjhgpuQu9C",
    "outputId": "3778e2c0-14e4-430b-92ce-2f48c34302a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this part, we compare the original pretrained model with our fine-tuned model, and see whether the generated texts would be different.\n",
      " \n",
      "First we load the original model. We assume there is a fake company named East Mars Company, and see what text the model will generate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f042a38cc7b24a5e8a80669573d940ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=224, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c0481c0bfb465ba265bcdfad70951c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c91fb5e7ea4308a2585f2406632f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f786775f4844100a7420e86619dbaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=548118077, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "Here is our result.\n",
      "East Mars Company is a company that has been working on the Mars Exploration Rover (MRO) program since the early 1990s. The company has been working on the Mars Exploration Rover (MRO) program since the early 1990s.\n",
      "\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "print(\"In this part, we compare the original pretrained model with our fine-tuned model, and see whether the generated texts would be different.\")\n",
    "print(\" \")\n",
    "print(\"First we load the original model. We assume there is a fake company named East Mars Company, and see what text the model will generate.\")\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "sequence = \"East Mars Company is a company that \"\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(\" \")\n",
    "print(\"Here is our result.\")\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vvkR2uNaUnpX",
    "outputId": "ec429406-6386-4a30-ab47-cff26cbebc4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And see whether the business wisdom in our database help design a more practical business plan...\n"
     ]
    }
   ],
   "source": [
    "print(\"And see whether the business wisdom in our database help design a more practical business plan...\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"description.csv\",sep=',',header=0)\n",
    "train_text, test_text = train_test_split(df['companybusinessdescriptionlong'][0:100], test_size=0.2)\n",
    "train_text.to_frame().to_csv(r'train_text_bus', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text_bus', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KBkpRgBCBS2_",
    "outputId": "8e57f81a-89eb-4928-c4fd-4b0373881139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 443Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 95.4Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 420Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 226Mit/s]                                   \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 512Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 203Mit/s]                                                 \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 149Mit/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  5 22:29:43 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P0    28W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "dataset has 10031 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 | 18.37] loss=2.49 avg=2.49\n",
      "[20 | 30.88] loss=1.58 avg=2.03\n",
      "[30 | 43.39] loss=0.91 avg=1.66\n",
      "[40 | 55.87] loss=0.28 avg=1.31\n",
      "[50 | 68.32] loss=0.14 avg=1.07\n",
      "[60 | 80.82] loss=0.07 avg=0.90\n",
      "[70 | 93.31] loss=0.06 avg=0.77\n",
      "[80 | 105.81] loss=0.09 avg=0.69\n",
      "[90 | 118.30] loss=0.05 avg=0.61\n",
      "[100 | 130.79] loss=0.05 avg=0.55\n",
      "[110 | 143.26] loss=0.04 avg=0.50\n",
      "[120 | 155.73] loss=0.03 avg=0.46\n",
      "[130 | 168.18] loss=0.04 avg=0.43\n",
      "[140 | 180.67] loss=0.04 avg=0.40\n",
      "[150 | 193.14] loss=0.04 avg=0.37\n",
      "[160 | 205.61] loss=0.02 avg=0.35\n",
      "[170 | 218.10] loss=0.03 avg=0.33\n",
      "[180 | 230.55] loss=0.02 avg=0.31\n",
      "[190 | 243.04] loss=0.03 avg=0.29\n",
      "[200 | 255.52] loss=0.03 avg=0.28\n",
      "======== SAMPLE 1 ========\n",
      " earning a contract as a bank teller, agent/advised partner, and cash manager for more than 15 banking organizations. As of December 31, 2016, the Bank operated exclusively throughout the Chicago metropolitan area, as well as northwest Indiana, central and western Illinois, and eastern Iowa through 117 banking locations. The Bank offers capital market products to commercial customers as risk management solutions, which include derivatives and interest rate risk products. The Bank provides an array of services through the lending operations, such as commercial lending, auto, and residential mortgage banking, and willard and expense management. The Bank offers a range of consumer and industrial loans, with a focus on construction loans and consumer loans with a discount to sales taxes. The Bank offers a range of consumer and industrial debit and credit cards, including the most common are for individuals and businesses. The Bank offers a range of cash management services, including minimum and maximum transactions fee based on the value of your assets, such as reported and unreported sales, taxes, balances and liabilities. The Bank offers an expanded range of consumer lending products and services through your financial institution, commercial lending operations and other commercial banks. The Company operates through two formats: retail and consumer lending products and services. The Company offers its products by bank, branch, area and may include retail, consumer, cash management services, deposit gathering, overdraft management, community management, financial planning, and reward based services. The Company offers its products through its banking centers, retail outlets and through individual appointment only departments. The Company Accepts a Large Set of the Bankruptcy Debt Types. The Bank offers various types of loans, including commercial lines of credit, commercial term loans, real estate, construction, home equity, consumer and other loans. The Bank offers various types of deposits, such as demand deposits, interest checking deposits, savings deposits, money market deposits and transaction deposits. The Company\\'s loan portfolio consists of both commercial and consumer loans. Owns and operates the bank and trust. The Bank\\'s loan portfolio consists of both commercial and consumer loans. The Bank\\'s trust business consists of its business unit, Richard and Diane\\'s Bank, and its parent company, Douglas & Tina\\'s Bank. Provides consumer banking services.   Develops and manufactures equipment for the transportation of toxic chemical and medical equipment.   Operates as a holding company and acquires a commmercial bank icial.  1st Source Corporation is a bank holding company. The Company, through its subsidiaries, provides a range of financial products and services. It is engaged in commercial banking. 1st Source Bank (Bank), its banking subsidiary, offers commercial and consumer banking services, trust and wealth advisory services, and insurance to individual and business clients. The Bank offers a range of consumer and commercial banking services through its lending operations, retail branches and fee based businesses. It provides commercial, small business, agricultural, and real estate loans to primarily business clients mainly located within its regional market area. It provides a range of consumer banking products and services through its banking centers and at 1stsource.com. It also offers insurance products through 1st Source Insurance offices. It also provides a range of trust, investment, agency, and custodial services for individual, corporate and not-for-profit clients. Acquires commercial banks and acts as a holding company.   Provides banking services.   Operates electrical and mechanical construction contracting.  Provides information management systems and services to the financial and insurance industries. The Company\\'s leading services include transaction processing, outsourcing, business process outsourcing (BPO), software and systems solutions. Fiserv operates through three segments: Financial Institution Services, Insurance Services, and CheckFree. It serves more than 18,000 clients worldwide   FirstMerit Corporation is a bank holding company. The Company\\'s business consists of owning and supervising its affiliates. It operates through FirstMerit Bank, N.A. and its other subsidiaries, providing a range of banking, fiduciary, financial, insurance and investment services to customers throughout Ohio, the Chicago, Illinois-metropolitan area, Michigan, Wisconsin and Western Pennsylvania. Its segments include Commercial, Retail, Wealth and Other. The Commercial segment provides various lending, depository and related financial services. The Retail segment includes consumer lending and deposit gathering, residential mortgage loan origination and servicing, and branch-based small business banking. The Wealth segment offers a range of asset management, private banking, financial planning, estate settlement and administration, credit and deposit products and services. The Other segment includes activities of the parent company, community development operations and the treasury group. Provides banking, asset management, insurance, and investment services. The Company is engaged in a general commercial banking and investment management business. It also provides financial services, including institutional and investment banking, cash management, trade services, export finance, mortgage banking, corporate finance, asset-based lending, corporate services, commercial lending, real estate lending, investment management services, credit cards, discount brokerage services, student loan processing and full-service banking.   Manufactures array processors - high\n",
      "\n",
      "[210 | 278.06] loss=0.04 avg=0.27\n",
      "[220 | 290.53] loss=0.03 avg=0.25\n",
      "[230 | 303.02] loss=0.02 avg=0.24\n",
      "[240 | 315.52] loss=0.03 avg=0.23\n",
      "[250 | 328.00] loss=0.02 avg=0.22\n",
      "[260 | 340.46] loss=0.02 avg=0.21\n",
      "[270 | 352.97] loss=0.02 avg=0.21\n",
      "[280 | 365.46] loss=0.02 avg=0.20\n",
      "[290 | 377.95] loss=0.03 avg=0.19\n",
      "[300 | 390.41] loss=0.02 avg=0.19\n",
      "[310 | 402.87] loss=0.02 avg=0.18\n",
      "[320 | 415.39] loss=0.02 avg=0.17\n",
      "[330 | 427.86] loss=0.03 avg=0.17\n",
      "[340 | 440.33] loss=0.02 avg=0.16\n",
      "[350 | 452.81] loss=0.01 avg=0.16\n",
      "[360 | 465.30] loss=0.02 avg=0.15\n",
      "[370 | 477.79] loss=0.02 avg=0.15\n",
      "[380 | 490.27] loss=0.02 avg=0.15\n",
      "[390 | 502.78] loss=0.03 avg=0.14\n",
      "[400 | 515.24] loss=0.02 avg=0.14\n",
      "======== SAMPLE 1 ========\n",
      "ures   Elgin National Industries, Inc. (Elgin) owns and operates a diversified group of middle-market manufactured products and engineering services businesses. The Company operates through two operating segments. Through its Manufactured Products Segment, Elgin is a manufacturer and supplier of custom-designed products used by a variety of customers in the industrial equipment, durable goods, mining, mineral processing and electric utility industries. Through its Engineering Services Segment, Elgin provides design, engineering, procurement and construction management services for mineral processing and bulk materials handling systems used in the mining, mineral processing, electric utility, and the rail and marine transportation industries. Emerson Radio Corp. designs, sources, imports and markets a range of houseware and consumer electronic products. The Company also licenses its trademarks for a range of products around the world. Its product and branded categories consist of Houseware Products, which include microwave ovens and compact refrigerators; Audio Products, including clock radios, and Other, which consists of televisions, mobile and landline telephones and accessories, tablet computers and accessories, cameras and video cameras and accessories, and miscellaneous electronic and novelty products. Its product categories include bottle openers and wine coolers, microwaves, small appliances and refrigeration. Its products include bottle cooler/warmer, electric wine bottle opener, table-top electronic wine cooler, 700W microwave oven, coffee/espresso/capuccino maker and 3.1 CU.FT. compressor type compact fridge. Its trademarks include Emerson, Emerson Research, H.H. Scott, iDEA, IDIVA, Olevia, Scott and SmartSet. Provides air freight service.   Emisphere Technologies, Inc. is a biopharmaceutical company. The Company focuses on delivery of pharmaceutical compounds, medical foods and dietary supplements using its Eligen Technology. The Eligen Technology applies to the oral route of administration and other delivery pathways, such as buccal, rectal, inhalation, intra-vaginal or transdermal. Eligen delivery enables the transport of active ingredients across membranes, such as in the gastrointestinal tract, to reach the tissues of the body.  Owns and operates radio, television and magazine entities in large and medium sized markets throughout the United States. The Company owns and operates more than 20 radio stations serving some of the top markets in the US, including New York City, Los Angeles, and Chicago, as well as two radio networks (AgriAmerica and Network Indiana) in Indiana. Abroad, the Company has stakes in two stations in Argentina and one in Hungary. Emmis also has 15 network-affiliated television stations operating in 12 states.  Empi Inc is headquartered in Saint Paul, Minnesota, United States and is a manufacturer of navigational, measuring, electromedical, and control instruments. The company offers Medical Devices, Clinics. The company was founded in 1977. DJO Global Inc is its ultimate parent. Emulex Corporation (Emulex) is a provider of network connectivity, monitoring and management products. It provides solutions for networks that support enterprise, cloud, government and telecommunications sectors. The Company\\'s portfolio of input/output (I/O) connectivity offerings, including the line of Ethernet and Fiber Channel-based connectivity products, has been designed into server and storage solutions. Its monitoring and management solutions include its network visibility and recording products. The Company\\'s network connectivity, monitoring and management solutions are offered through two business segments: Connectivity Segment and the Visibility Segment. The Connectivity Segment comprises the Company\\'s Emulex products, and network connectivity products (NCP) and storage connectivity and other products (SCOP). Its Visibility Segment comprises its Network Visibility Products (NVP). Encore Wire Corporation is a manufacturer of electrical building wire and cable. The Company is a supplier of building wire for interior electrical wiring in commercial and industrial buildings, homes, apartments, and manufactured housing. The Company manufactures electric building wire, principally NM-B cable, for use primarily as interior wiring in homes, apartments and manufactured housing, and THHN/THWN-2 cable and metal-clad and armored cable for use primarily as wiring in commercial and industrial buildings. It offers an electrical building wire product line that consists primarily of UF-B cable and other types of wire products, including metal-clad and armored cable. All of these products are manufactured with copper or aluminum as the conductor. NM-B cable consists of either two or three insulated copper wire conductors, with an uninsulated ground wire, all sheathed in a polyvinyl chloride (PVC) jacket. Endosonics Corporation develops, manufactures and markets combined angioplasty imaging catheters, diagnostic imaging catheters and cathscanner ultrasound imaging systems for the diagnosis and treatment of coronary and peripheral vascular disease.  Energy Conversion Devices, Inc. (ECD), through its United Solar Ovonic (USO) subsidiary, is engaged in building-integrated and rooftop photovoltaics (PV).\n",
      "\n",
      "[410 | 536.78] loss=0.02 avg=0.13\n",
      "[420 | 549.28] loss=0.02 avg=0.13\n",
      "[430 | 561.72] loss=0.02 avg=0.13\n",
      "[440 | 574.19] loss=0.02 avg=0.13\n",
      "[450 | 586.68] loss=0.02 avg=0.12\n",
      "[460 | 599.14] loss=0.01 avg=0.12\n",
      "[470 | 611.63] loss=0.01 avg=0.12\n",
      "[480 | 624.15] loss=0.02 avg=0.11\n",
      "[490 | 636.63] loss=0.01 avg=0.11\n",
      "[500 | 649.12] loss=0.02 avg=0.11\n",
      "Saving checkpoint/run1/model-500\n",
      "[510 | 664.44] loss=0.02 avg=0.11\n",
      "[520 | 676.97] loss=0.01 avg=0.10\n",
      "[530 | 689.44] loss=0.02 avg=0.10\n",
      "[540 | 701.96] loss=0.02 avg=0.10\n",
      "[550 | 714.44] loss=0.02 avg=0.10\n",
      "[560 | 726.93] loss=0.01 avg=0.10\n",
      "[570 | 739.44] loss=0.02 avg=0.09\n",
      "[580 | 751.95] loss=0.02 avg=0.09\n",
      "[590 | 764.47] loss=0.02 avg=0.09\n",
      "[600 | 776.94] loss=0.01 avg=0.09\n",
      "======== SAMPLE 1 ========\n",
      " dietary treatment for gastrointestinal and infectious diseases.  The Company is a biopharmaceutical company.  The Company is engaged in a biopharmaceutical process company. E.P.A.R.S. provides a wide variety of financial services. Its products include commercial banks and financial products companies. Its FMC Agricultural Solutions segment performs differentials for different markets. It diversifies its loan and lease portfolio by offering a range of loan and lease products with various payment terms and rate structures. It offers commercial and industrial loans, commercial mortgage loans, commercial construction loans, commercial leases, residential mortgage loans, home equity, automobile loans, credit card, and other consumer loans and leases. It offers various types of deposits, such as demand deposits, interest checking deposits, savings deposits, money market deposits and transaction deposits. EOG Biochem, Inc. is a bioscience company focusing on delivering and applying technology capabilities to produce products and services. The Company\\'s segments include EOG Therapeutics, EOG Life Sciences and EOG Research Labs. The EOG Therapeutics segment develops multiple delivery pathways for the treatment of coronary and peripheral vascular disease. The Company\\'s EOG Life Sciences segment provides its other companies with clinical research and development services. The Company\\'s EOG Research Labs segment develops products based on a range of functional imaging imaging studies of the human gastrointestinal tract. The Company\\'s Ov diabetes products are marketed on a clinical trial basis. Enzo Clinical Labs is a clinical reference laboratory providing a range of clinical services to physicians, medical centers, other clinical labs and pharmaceutical companies. It provides a range of products and services with which to clinical research patients, promote their interest in conducting clinical trials and obtain product development approval. Enzo Wyeth Biochem, Inc. is a bioscience company providing a range of clinical services to physicians, medical centers and pharmaceutical companies. It provides a range of products and services with which to clinical research individuals, small animal health and life science products and bioscience research products. It also provides a range of other products and services that can be provided by manufacturer to physicians, medical centers and other clinical labs. Its Therapeutics segment offers a portfolio of products and services that can be marketed as natural products that are used to treat certain diseases. Its Aborted Ov diabetes drugs are sold as crude oral fluid products that are used to remove dead animals and tissues from the diabetes drug pipeline. Its Ov noninvasive diabetes drugs are marketed as tablet and/or inhalation drug products that are intended to be inhaled or ingested intramuscularly. Its Ov noninvasive diabetes drugs are sold as a rectal or transdermal. Its Ov drug therapy products are marketed as oral fluid products that are used to remove dead animals and tissues from the transdermal product pathways. Its Ov drug therapy drugs are sold as inhalation and/or diode fluids. Its Ov drug products are sold as an oral fluid product that touches the oral fluid bottle or cup used for oral delivery. Its Ov drug products are sold as a range of oral fluid products across a variety of industries. Its Ov drug products are sold primarily to manufacturers of wound therapy instruments, diagnostic instruments and monitoring devices. Its Ov drug products are sold to manufacturers of endoscopic imaging systems, other endoscopy diagnostic imaging systems and diffusers. It is a manufacturer of transdermal pharmaceutical agents. Its Ov drug products are sold to manufacturers of endoscopic imaging systems and/or other endoscopic imaging systems. It is a manufacturer of endophaer drugs. It is a manufacturer of endometriine and other endometriine analogues. It is a manufacturer of other endometriine and other endometriine analogues. Ov of Agrina Ovicea is a manufacturer of endometriine and other endometriine drugs. It is a manufacturer of other endometriine and other endometriine drugs. It is a manufacturer of other endometriine and other endometriine drugs. It has outstanding commercial banking and banking services.   OraSure Corporation is a health food company with products in the preparation of food for the public. The Company offers its products in a range of preparation methods, such as soups, rolls, pizzas and crusts. OraQuick provides a range of health food products and lubricants. It also offers a drug store and prescription drug treatment store product rotation service, home decor ingredient reprocess service and home decor ingredient comparison tool. OraSeal is a supplier of satellite television systems, power armor and emergency medical systems, and medical supplies. It supplies remote-controlled kitchenware, kitchenwarewareware clamps and tools to manufacturers of satellite television systems, as well as to automotive, industrial, and specialty computer and printer supply companies. It provides products with safeguards, such as onboard error correction and monitoring, onboard lighting, cooking oil and cooking spray, and cleaning products. Scott Electrical Industries, Inc. is a manufacturer of electrical building wire and cable. The Company operates in two segments: commercial mechanical construction and\n",
      "\n",
      "[610 | 798.61] loss=0.01 avg=0.09\n",
      "[620 | 811.07] loss=0.01 avg=0.09\n",
      "[630 | 823.59] loss=0.02 avg=0.08\n",
      "[640 | 836.08] loss=0.01 avg=0.08\n",
      "[650 | 848.52] loss=0.02 avg=0.08\n",
      "[660 | 860.99] loss=0.02 avg=0.08\n",
      "[670 | 873.47] loss=0.01 avg=0.08\n",
      "[680 | 885.94] loss=0.02 avg=0.08\n",
      "[690 | 898.40] loss=0.01 avg=0.08\n",
      "[700 | 910.88] loss=0.01 avg=0.08\n",
      "[710 | 923.32] loss=0.02 avg=0.07\n",
      "[720 | 935.81] loss=0.02 avg=0.07\n",
      "[730 | 948.28] loss=0.01 avg=0.07\n",
      "[740 | 960.80] loss=0.02 avg=0.07\n",
      "[750 | 973.27] loss=0.02 avg=0.07\n",
      "[760 | 985.75] loss=0.01 avg=0.07\n",
      "[770 | 998.23] loss=0.02 avg=0.07\n",
      "[780 | 1010.69] loss=0.01 avg=0.07\n",
      "[790 | 1023.16] loss=0.01 avg=0.07\n",
      "[800 | 1035.61] loss=0.01 avg=0.07\n",
      "======== SAMPLE 1 ========\n",
      " throughout the United States. Manufactures a school resource toolbelt, which protects teachers and students from unauthorized entry, use, abuse, neglect, fraud, mismanagement and mismanagement of personal and commercial savings. The Company sells the product directly to schools.   Enzo Biochem, Inc. is a bioscience company focusing on delivering and applying technology capabilities to produce products and services. The Company\\'s segments include Enzo Clinical Labs, Enzo Life Sciences and Enzo Therapeutics. Enzo Clinical Labs is a clinical reference laboratory providing a range of clinical services to physicians, medical centers, other clinical labs and pharmaceutical companies. It offers a menu of molecular and other clinical laboratory tests or procedures. Enzo Life Sciences manufactures, develops and markets products and tools to clinical research, drug development and bioscience research customers. Enzo Therapeutics is a biopharmaceutical venture that develops multiple approaches in the areas of gastrointestinal, infectious, ophthalmic and metabolic diseases. Its products in the development pipeline include a range of assays for detection of various women\\'s health infectious agents, as well as for use in the identification of pathogens for other markets. OraSure Technologies, Inc. makes oral fluid diagnostic products & specimen collection devices. It also services immunoassays and miscellaneous in vitro diagnostic tests which scopes for the detection of antibodies for the HIV virus, including OraQuick ADVANCE Rapid HIV-1/2 Antibody Test and the OraSure HIV-1 Oral Specimen Collection Device, a test for antibodies for the HCV virus, the OraQuick HCV Rapid Antibody Test and oral fluid testing solutions for drugs of abuse testing, including Intercept Oral Fluid Drug Testing System and Q.E.D. Saliva Alcohol Test.  Manufactures and markets children\\'s leisure products featuring licensed television and cartoon characters as well as water sports and camping products such as skis, flotation devices and sleeping bags.   Manufactures and retails home furnishings.  The Company offers a full range of furniture products and accessories designed to appeal to a broad range of consumers.  Evans & Sutherland Computer Corporation (E&S) focuses on the production of visual display systems used primarily in full-dome video projection applications, dome projection screens, dome architectural treatments, and content for planetariums, schools, science centers, other educational institutions and entertainment venues. It operates in the visual simulation market segment. The Company through its subsidiary, Spitz, Inc. (Spitz), is a supplier of planetarium systems, dome projection screens and other dome displays. It supplies total system solutions for its digital theater markets, as well as domes and other geometric structures in the architectural market. It offers a range of products and services for dome and planetarium theaters in educational institutions, training and entertainment venues. Its products include planetarium and dome theater systems consisting of hardware and software, and other visual display systems primarily used to project digital video on curved surfaces. Everest & Jennings, through its subsidiaries, manufactures wheelchairs & distributes homecare beds. Everest Re Group, Ltd. through its subsidiaries, is engaged in the underwriting of reinsurance and insurance in the United States, Bermuda and international markets. The Company operates in segments: U.S. Reinsurance, International, Bermuda and Insurance. The Company underwrites reinsurance both through brokers and directly with ceding companies. The Company underwrites insurance principally through general agent relationships, brokers and surplus lines brokers. The Company offers treaty and facultative reinsurance, and admitted and non-admitted insurance. Its products include the range of property and casualty reinsurance, and insurance coverage\\'s, including marine, aviation, surety, errors and omissions liability (E&O), directors\\' and officers\\' liability (D&O), medical malpractice, other specialty lines, accident and health (A&H) and workers\\' compensation. The Company\\'s subsidiaries include Everest Reinsurance (Bermuda), Ltd. (Bermuda Re) and Everest International Reinsurance, Ltd. Manufactures compact tape-based data storage subsystems for use in mid-sized computers. The Company\\'s products include an 8mm cartridge tape system which will hold over 2,000 megabytes of data and will fit in the standard 5 1/4 inch drive space. The subsystem features an integrated data buffer, proprietary tape handling methodology, onboard error correction code, and helical scanning technology. The Company markets its products to manufacturers, integrators and distributors of advanced micro, mini and super-mini computer systems.   Exar Corporation (Exar) designs, develops and markets analog mixed-signal integrated circuits (ICs) and sub-system solutions. The Company\\'s products are deployed in a range of applications, such as industrial, instrumentation and medical equipment, networking and telecommunication systems, servers, enterprise storage systems, flat panel displays, light emitting diode (LED) lighting solutions, set top boxes and digital video recorders. Exar\\'s product portfolio\n",
      "\n",
      "[810 | 1057.03] loss=0.01 avg=0.06\n",
      "[820 | 1069.53] loss=0.02 avg=0.06\n",
      "[830 | 1082.02] loss=0.02 avg=0.06\n",
      "[840 | 1094.50] loss=0.01 avg=0.06\n",
      "[850 | 1106.93] loss=0.01 avg=0.06\n",
      "[860 | 1119.39] loss=0.01 avg=0.06\n",
      "[870 | 1131.84] loss=0.01 avg=0.06\n",
      "[880 | 1144.33] loss=0.01 avg=0.06\n",
      "[890 | 1156.80] loss=0.01 avg=0.06\n",
      "[900 | 1169.27] loss=0.01 avg=0.06\n",
      "[910 | 1181.72] loss=0.01 avg=0.06\n",
      "[920 | 1194.22] loss=0.02 avg=0.06\n",
      "[930 | 1206.69] loss=0.01 avg=0.05\n",
      "[940 | 1219.17] loss=0.01 avg=0.05\n",
      "[950 | 1231.62] loss=0.01 avg=0.05\n",
      "[960 | 1244.07] loss=0.01 avg=0.05\n",
      "[970 | 1256.55] loss=0.01 avg=0.05\n",
      "[980 | 1269.01] loss=0.02 avg=0.05\n",
      "[990 | 1281.46] loss=0.01 avg=0.05\n",
      "[1000 | 1293.95] loss=0.01 avg=0.05\n",
      "Saving checkpoint/run1/model-1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "#In this cell we fine-tune a gpt-2 model on this dataset.\n",
    "%tensorflow_version 1.x\n",
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "gpt2.download_gpt2(model_name=\"124M\")\n",
    "!nvidia-smi\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              dataset='file0.txt',\n",
    "              model_name='124M',\n",
    "              steps=1000,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=200,\n",
    "              save_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "v8QYLXR6vtKA",
    "outputId": "5af97a3d-c7e7-4710-d94f-52f7802db7f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "East Mars Company is a company that develops, manufactures and markets products and tools that are used primarily to drill and cut cypress clover, cypress groover and other ornamental wedding and ceremony trees, and leaves and branches of the Eastern Mediterranean. The Company\\'s product is marketed\n",
      "====================\n",
      "East Mars Company is a company that develops, manufactures and markets products and tools that are used primarily in the semiconductor industry. The Company\\'s segments include Enzo Clinical Labs, Enzo Life Sciences and Enzo Therapeutics. Enzo Clinical Labs is a clinical reference laboratory providing\n",
      "====================\n",
      "East Mars Company is a company that provides value-added services in addition to those customarily provided by traditional airfreight forwarders, ocean freight forwarders and customs brokers. EGL, Inc. provides its services primarily through its network comprising approximately 400 facilities, agents and distribution centers located\n",
      "====================\n",
      "East Mars Company is a company that develops, invests and sells technology-based and retail chain solutions. The Company operates through two segments: Component Processing and SysTransmission. The Component Processing segment includes interconnect products, semiconductor products and component products. The interconnect, semiconductor\n",
      "====================\n",
      "East Mars Company is a company that develops, manufactures and markets products and tools to clinical research, drug development and bioscience research customers. The Company provides clinical research, drug development and bioscience research services to a broad range of customers, generally clinical research customers, pharmacist and drug company\n",
      "====================\n",
      "East Mars Company is a company that develops, manufactures and markets products and tools that are used primarily in nanometer (nm) range industries. The Company\\'s products include a range of building thin-film semiconductor products and component products, as well as over 18,000 products in\n",
      "====================\n",
      "East Mars Company is a company that develops, manufactures and markets products and tools to clinical research, drug development and bioscience research customers. The Company provides a wide variety of equipment and systems, including air and ocean sprayers, flat panel displays, light emitting diode (LED) lighting\n",
      "====================\n",
      "East Mars Company is a company that develops, manufactures and operates a portfolio of professional pest control, and lawn and garden products. The Company\\'s products include line of household pest control products, including household bleach, household bleach products, and cleaning products, as well as for use in the\n",
      "====================\n",
      "East Mars Company is a company that provides value-added services in addition to those customarily provided by traditional airfreight forwarders, ocean freight forwarders and customs brokers. EGL, Inc. provides its services primarily through its network comprising approximately 400 facilities, agents and distribution centers located\n",
      "====================\n",
      "East Mars Company is a company that develops, manufactures and markets products based on Eagle Hardware and proprietary materials management software. The Company\\'s products include a laboratory-based manufacturing process outsourcing process outsourcing (BPO) process outsourcing company that provides value-added services to customers throughout North America,\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              length=50,\n",
    "              temperature=0.7,\n",
    "              prefix=\"East Mars Company is a company that\",\n",
    "              nsamples=10,\n",
    "              batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nx1gVVB_vJH"
   },
   "source": [
    "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cxlb0BfG_vJH"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Vrphkof_vJI"
   },
   "outputs": [],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krey2KP__vJJ"
   },
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXu6nsEY_vJL"
   },
   "source": [
    "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icnvZZmA_vJL"
   },
   "outputs": [],
   "source": [
    "text = \"Do you have your chips with fish or with salsa?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSfchLl7_vJO"
   },
   "outputs": [],
   "source": [
    "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
    "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
    "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
    "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7AcRTXp_vJR"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMo0F3ZN_vJT"
   },
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_QvSbT1_vJU",
    "outputId": "2140b9af-e775-4c07-eeae-edd62cfad951"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6xU5E59_vJV"
   },
   "outputs": [],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmaP6OUJ_vJW",
    "outputId": "cb4861fe-7307-48ad-f3fe-799c26443e56"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUR_HpDo_vJX"
   },
   "source": [
    "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VBQBMYRu_vJX"
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vl9psbMD_vJY",
    "outputId": "1134a27c-4b38-4a66-a8ba-d2f3a67c8f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this section, we fine-tune on three set of company descriptions: mining, wholesale, and finance/insurance.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"In this section, we fine-tune on three set of company descriptions: mining, wholesale, and finance/insurance.\")\n",
    "df=pd.read_csv(\"description.csv\",sep=',',header=0)\n",
    "df1=df[df['siccolor']==2]\n",
    "df2=df[df['siccolor']==6]\n",
    "df3=df[df['siccolor']==8]\n",
    "train_text1, test_text1 = train_test_split(df1['companybusinessdescriptionlong'], test_size=0.2)\n",
    "train_text2, test_text2 = train_test_split(df2['companybusinessdescriptionlong'], test_size=0.2)\n",
    "train_text3, test_text3 = train_test_split(df3['companybusinessdescriptionlong'], test_size=0.2)\n",
    "train_text1.to_frame().to_csv(r'train_text_bus1', header=None, index=None, sep=' ', mode='a')\n",
    "test_text1.to_frame().to_csv(r'test_text_bus1', header=None, index=None, sep=' ', mode='a')\n",
    "train_text2.to_frame().to_csv(r'train_text_bus2', header=None, index=None, sep=' ', mode='a')\n",
    "test_text2.to_frame().to_csv(r'test_text_bus2', header=None, index=None, sep=' ', mode='a')\n",
    "train_text3.to_frame().to_csv(r'train_text_bus3', header=None, index=None, sep=' ', mode='a')\n",
    "test_text3.to_frame().to_csv(r'test_text_bus3', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4IHag3QTPz2r",
    "outputId": "67a7aabd-dbce-4bad-a76f-80c66aeaa24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/06/2020 03:08:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/06/2020 03:08:32 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "03/06/2020 03:08:32 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:08:34 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "03/06/2020 03:08:34 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "03/06/2020 03:08:35 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "03/06/2020 03:08:40 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "03/06/2020 03:08:43 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test_text_bus1', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_bus1', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_text_bus1', warmup_steps=0, weight_decay=0.0)\n",
      "03/06/2020 03:08:43 - INFO - __main__ -   Loading features from cached file roberta_cached_lm_510_train_text_bus1\n",
      "03/06/2020 03:08:43 - INFO - __main__ -   ***** Running training *****\n",
      "03/06/2020 03:08:43 - INFO - __main__ -     Num examples = 95\n",
      "03/06/2020 03:08:43 - INFO - __main__ -     Num Epochs = 1\n",
      "03/06/2020 03:08:43 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
      "03/06/2020 03:08:43 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "03/06/2020 03:08:43 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/06/2020 03:08:43 - INFO - __main__ -     Total optimization steps = 48\n",
      "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2% 1/48 [00:00<00:23,  2.01it/s]\u001b[A\n",
      "Iteration:   4% 2/48 [00:00<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:   6% 3/48 [00:01<00:21,  2.10it/s]\u001b[A\n",
      "Iteration:   8% 4/48 [00:01<00:20,  2.12it/s]\u001b[A\n",
      "Iteration:  10% 5/48 [00:02<00:20,  2.13it/s]\u001b[A\n",
      "Iteration:  12% 6/48 [00:02<00:19,  2.14it/s]\u001b[A\n",
      "Iteration:  15% 7/48 [00:03<00:19,  2.15it/s]\u001b[A\n",
      "Iteration:  17% 8/48 [00:03<00:18,  2.15it/s]\u001b[A\n",
      "Iteration:  19% 9/48 [00:04<00:18,  2.15it/s]\u001b[A\n",
      "Iteration:  21% 10/48 [00:04<00:17,  2.16it/s]\u001b[A\n",
      "Iteration:  23% 11/48 [00:05<00:17,  2.16it/s]\u001b[A\n",
      "Iteration:  25% 12/48 [00:05<00:16,  2.17it/s]\u001b[A\n",
      "Iteration:  27% 13/48 [00:06<00:16,  2.17it/s]\u001b[A\n",
      "Iteration:  29% 14/48 [00:06<00:15,  2.17it/s]\u001b[A\n",
      "Iteration:  31% 15/48 [00:06<00:15,  2.17it/s]\u001b[A\n",
      "Iteration:  33% 16/48 [00:07<00:14,  2.17it/s]\u001b[A\n",
      "Iteration:  35% 17/48 [00:07<00:14,  2.17it/s]\u001b[A\n",
      "Iteration:  38% 18/48 [00:08<00:13,  2.17it/s]\u001b[A\n",
      "Iteration:  40% 19/48 [00:08<00:13,  2.17it/s]\u001b[A\n",
      "Iteration:  42% 20/48 [00:09<00:12,  2.17it/s]\u001b[A\n",
      "Iteration:  44% 21/48 [00:09<00:12,  2.17it/s]\u001b[A\n",
      "Iteration:  46% 22/48 [00:10<00:11,  2.17it/s]\u001b[A\n",
      "Iteration:  48% 23/48 [00:10<00:11,  2.17it/s]\u001b[A\n",
      "Iteration:  50% 24/48 [00:11<00:11,  2.17it/s]\u001b[A\n",
      "Iteration:  52% 25/48 [00:11<00:10,  2.17it/s]\u001b[A\n",
      "Iteration:  54% 26/48 [00:12<00:10,  2.17it/s]\u001b[A\n",
      "Iteration:  56% 27/48 [00:12<00:09,  2.17it/s]\u001b[A\n",
      "Iteration:  58% 28/48 [00:12<00:09,  2.17it/s]\u001b[A\n",
      "Iteration:  60% 29/48 [00:13<00:08,  2.16it/s]\u001b[A\n",
      "Iteration:  62% 30/48 [00:13<00:08,  2.17it/s]\u001b[A\n",
      "Iteration:  65% 31/48 [00:14<00:07,  2.16it/s]\u001b[A\n",
      "Iteration:  67% 32/48 [00:14<00:07,  2.16it/s]\u001b[A\n",
      "Iteration:  69% 33/48 [00:15<00:06,  2.17it/s]\u001b[A\n",
      "Iteration:  71% 34/48 [00:15<00:06,  2.16it/s]\u001b[A\n",
      "Iteration:  73% 35/48 [00:16<00:05,  2.17it/s]\u001b[A\n",
      "Iteration:  75% 36/48 [00:16<00:05,  2.16it/s]\u001b[A\n",
      "Iteration:  77% 37/48 [00:17<00:05,  2.17it/s]\u001b[A\n",
      "Iteration:  79% 38/48 [00:17<00:04,  2.16it/s]\u001b[A\n",
      "Iteration:  81% 39/48 [00:18<00:04,  2.16it/s]\u001b[A\n",
      "Iteration:  83% 40/48 [00:18<00:03,  2.15it/s]\u001b[A\n",
      "Iteration:  85% 41/48 [00:18<00:03,  2.16it/s]\u001b[A\n",
      "Iteration:  88% 42/48 [00:19<00:02,  2.16it/s]\u001b[A\n",
      "Iteration:  90% 43/48 [00:19<00:02,  2.16it/s]\u001b[A\n",
      "Iteration:  92% 44/48 [00:20<00:01,  2.15it/s]\u001b[A\n",
      "Iteration:  94% 45/48 [00:20<00:01,  2.14it/s]\u001b[A\n",
      "Iteration:  96% 46/48 [00:21<00:00,  2.15it/s]\u001b[A\n",
      "Iteration:  98% 47/48 [00:21<00:00,  2.15it/s]\u001b[A\n",
      "Iteration: 100% 48/48 [00:22<00:00,  2.44it/s]\u001b[A\n",
      "Epoch: 100% 1/1 [00:22<00:00, 22.03s/it]\n",
      "03/06/2020 03:09:05 - INFO - __main__ -    global_step = 48, average loss = 1.9058235262831051\n",
      "03/06/2020 03:09:05 - INFO - __main__ -   Saving model checkpoint to output_roberta_bus1\n",
      "03/06/2020 03:09:05 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_bus1/config.json\n",
      "03/06/2020 03:09:06 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_bus1/pytorch_model.bin\n",
      "03/06/2020 03:09:07 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_bus1/config.json\n",
      "03/06/2020 03:09:07 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:09:07 - INFO - transformers.modeling_utils -   loading weights file output_roberta_bus1/pytorch_model.bin\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_bus1' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_bus1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_bus1/added_tokens.json. We won't load it.\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus1/vocab.json\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus1/merges.txt\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus1/special_tokens_map.json\n",
      "03/06/2020 03:09:11 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus1/tokenizer_config.json\n",
      "03/06/2020 03:09:12 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_bus1']\n",
      "03/06/2020 03:09:12 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_bus1/config.json\n",
      "03/06/2020 03:09:12 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:09:12 - INFO - transformers.modeling_utils -   loading weights file output_roberta_bus1/pytorch_model.bin\n",
      "03/06/2020 03:09:16 - INFO - __main__ -   Loading features from cached file roberta_cached_lm_510_test_text_bus1\n",
      "03/06/2020 03:09:16 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/06/2020 03:09:16 - INFO - __main__ -     Num examples = 10\n",
      "03/06/2020 03:09:16 - INFO - __main__ -     Batch size = 2\n",
      "Evaluating: 100% 5/5 [00:00<00:00,  7.47it/s]\n",
      "03/06/2020 03:09:17 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/06/2020 03:09:17 - INFO - __main__ -     perplexity = tensor(4.7212)\n",
      "03/06/2020 03:09:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/06/2020 03:09:23 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "03/06/2020 03:09:23 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:09:25 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "03/06/2020 03:09:25 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "03/06/2020 03:09:26 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "03/06/2020 03:09:30 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "03/06/2020 03:09:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test_text_bus2', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_bus2', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_text_bus2', warmup_steps=0, weight_decay=0.0)\n",
      "03/06/2020 03:09:33 - INFO - __main__ -   Loading features from cached file roberta_cached_lm_510_train_text_bus2\n",
      "03/06/2020 03:09:33 - INFO - __main__ -   ***** Running training *****\n",
      "03/06/2020 03:09:33 - INFO - __main__ -     Num examples = 103\n",
      "03/06/2020 03:09:33 - INFO - __main__ -     Num Epochs = 1\n",
      "03/06/2020 03:09:33 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
      "03/06/2020 03:09:33 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "03/06/2020 03:09:33 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/06/2020 03:09:33 - INFO - __main__ -     Total optimization steps = 52\n",
      "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2% 1/52 [00:00<00:23,  2.18it/s]\u001b[A\n",
      "Iteration:   4% 2/52 [00:00<00:22,  2.17it/s]\u001b[A\n",
      "Iteration:   6% 3/52 [00:01<00:22,  2.18it/s]\u001b[A\n",
      "Iteration:   8% 4/52 [00:01<00:22,  2.17it/s]\u001b[A\n",
      "Iteration:  10% 5/52 [00:02<00:21,  2.17it/s]\u001b[A\n",
      "Iteration:  12% 6/52 [00:02<00:21,  2.17it/s]\u001b[A\n",
      "Iteration:  13% 7/52 [00:03<00:20,  2.17it/s]\u001b[A\n",
      "Iteration:  15% 8/52 [00:03<00:20,  2.17it/s]\u001b[A\n",
      "Iteration:  17% 9/52 [00:04<00:19,  2.17it/s]\u001b[A\n",
      "Iteration:  19% 10/52 [00:04<00:19,  2.17it/s]\u001b[A\n",
      "Iteration:  21% 11/52 [00:05<00:18,  2.17it/s]\u001b[A\n",
      "Iteration:  23% 12/52 [00:05<00:18,  2.17it/s]\u001b[A\n",
      "Iteration:  25% 13/52 [00:05<00:17,  2.17it/s]\u001b[A\n",
      "Iteration:  27% 14/52 [00:06<00:17,  2.17it/s]\u001b[A\n",
      "Iteration:  29% 15/52 [00:06<00:17,  2.17it/s]\u001b[A\n",
      "Iteration:  31% 16/52 [00:07<00:16,  2.17it/s]\u001b[A\n",
      "Iteration:  33% 17/52 [00:07<00:16,  2.16it/s]\u001b[A\n",
      "Iteration:  35% 18/52 [00:08<00:15,  2.16it/s]\u001b[A\n",
      "Iteration:  37% 19/52 [00:08<00:15,  2.16it/s]\u001b[A\n",
      "Iteration:  38% 20/52 [00:09<00:14,  2.17it/s]\u001b[A\n",
      "Iteration:  40% 21/52 [00:09<00:14,  2.16it/s]\u001b[A\n",
      "Iteration:  42% 22/52 [00:10<00:13,  2.17it/s]\u001b[A\n",
      "Iteration:  44% 23/52 [00:10<00:13,  2.17it/s]\u001b[A\n",
      "Iteration:  46% 24/52 [00:11<00:12,  2.17it/s]\u001b[A\n",
      "Iteration:  48% 25/52 [00:11<00:12,  2.17it/s]\u001b[A\n",
      "Iteration:  50% 26/52 [00:11<00:11,  2.17it/s]\u001b[A\n",
      "Iteration:  52% 27/52 [00:12<00:11,  2.17it/s]\u001b[A\n",
      "Iteration:  54% 28/52 [00:12<00:11,  2.17it/s]\u001b[A\n",
      "Iteration:  56% 29/52 [00:13<00:10,  2.17it/s]\u001b[A\n",
      "Iteration:  58% 30/52 [00:13<00:10,  2.17it/s]\u001b[A\n",
      "Iteration:  60% 31/52 [00:14<00:09,  2.17it/s]\u001b[A\n",
      "Iteration:  62% 32/52 [00:14<00:09,  2.17it/s]\u001b[A\n",
      "Iteration:  63% 33/52 [00:15<00:08,  2.17it/s]\u001b[A\n",
      "Iteration:  65% 34/52 [00:15<00:08,  2.17it/s]\u001b[A\n",
      "Iteration:  67% 35/52 [00:16<00:07,  2.16it/s]\u001b[A\n",
      "Iteration:  69% 36/52 [00:16<00:07,  2.17it/s]\u001b[A\n",
      "Iteration:  71% 37/52 [00:17<00:06,  2.17it/s]\u001b[A\n",
      "Iteration:  73% 38/52 [00:17<00:06,  2.17it/s]\u001b[A\n",
      "Iteration:  75% 39/52 [00:17<00:06,  2.16it/s]\u001b[A\n",
      "Iteration:  77% 40/52 [00:18<00:05,  2.17it/s]\u001b[A\n",
      "Iteration:  79% 41/52 [00:18<00:05,  2.17it/s]\u001b[A\n",
      "Iteration:  81% 42/52 [00:19<00:04,  2.17it/s]\u001b[A\n",
      "Iteration:  83% 43/52 [00:19<00:04,  2.16it/s]\u001b[A\n",
      "Iteration:  85% 44/52 [00:20<00:03,  2.16it/s]\u001b[A\n",
      "Iteration:  87% 45/52 [00:20<00:03,  2.17it/s]\u001b[A\n",
      "Iteration:  88% 46/52 [00:21<00:02,  2.16it/s]\u001b[A\n",
      "Iteration:  90% 47/52 [00:21<00:02,  2.17it/s]\u001b[A\n",
      "Iteration:  92% 48/52 [00:22<00:01,  2.16it/s]\u001b[A\n",
      "Iteration:  94% 49/52 [00:22<00:01,  2.17it/s]\u001b[A\n",
      "Iteration:  96% 50/52 [00:23<00:00,  2.17it/s]\u001b[A\n",
      "Iteration:  98% 51/52 [00:23<00:00,  2.17it/s]\u001b[A\n",
      "Iteration: 100% 52/52 [00:23<00:00,  2.47it/s]\u001b[A\n",
      "Epoch: 100% 1/1 [00:23<00:00, 23.80s/it]\n",
      "03/06/2020 03:09:57 - INFO - __main__ -    global_step = 52, average loss = 1.455393652503307\n",
      "03/06/2020 03:09:57 - INFO - __main__ -   Saving model checkpoint to output_roberta_bus2\n",
      "03/06/2020 03:09:57 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_bus2/config.json\n",
      "03/06/2020 03:09:59 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_bus2/pytorch_model.bin\n",
      "03/06/2020 03:09:59 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_bus2/config.json\n",
      "03/06/2020 03:09:59 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:09:59 - INFO - transformers.modeling_utils -   loading weights file output_roberta_bus2/pytorch_model.bin\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_bus2' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_bus2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_bus2/added_tokens.json. We won't load it.\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus2/vocab.json\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus2/merges.txt\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus2/special_tokens_map.json\n",
      "03/06/2020 03:10:04 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus2/tokenizer_config.json\n",
      "03/06/2020 03:10:04 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_bus2']\n",
      "03/06/2020 03:10:04 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_bus2/config.json\n",
      "03/06/2020 03:10:04 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:10:04 - INFO - transformers.modeling_utils -   loading weights file output_roberta_bus2/pytorch_model.bin\n",
      "03/06/2020 03:10:09 - INFO - __main__ -   Loading features from cached file roberta_cached_lm_510_test_text_bus2\n",
      "03/06/2020 03:10:09 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/06/2020 03:10:09 - INFO - __main__ -     Num examples = 10\n",
      "03/06/2020 03:10:09 - INFO - __main__ -     Batch size = 2\n",
      "Evaluating: 100% 5/5 [00:00<00:00,  7.51it/s]\n",
      "03/06/2020 03:10:10 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/06/2020 03:10:10 - INFO - __main__ -     perplexity = tensor(3.6377)\n",
      "03/06/2020 03:10:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/06/2020 03:10:15 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "03/06/2020 03:10:15 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:10:17 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "03/06/2020 03:10:17 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "03/06/2020 03:10:18 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "03/06/2020 03:10:23 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "03/06/2020 03:10:26 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test_text_bus3', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_bus3', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_text_bus3', warmup_steps=0, weight_decay=0.0)\n",
      "03/06/2020 03:10:26 - INFO - __main__ -   Loading features from cached file roberta_cached_lm_510_train_text_bus3\n",
      "03/06/2020 03:10:26 - INFO - __main__ -   ***** Running training *****\n",
      "03/06/2020 03:10:26 - INFO - __main__ -     Num examples = 22\n",
      "03/06/2020 03:10:26 - INFO - __main__ -     Num Epochs = 1\n",
      "03/06/2020 03:10:26 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
      "03/06/2020 03:10:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "03/06/2020 03:10:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/06/2020 03:10:26 - INFO - __main__ -     Total optimization steps = 11\n",
      "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   9% 1/11 [00:00<00:04,  2.18it/s]\u001b[A\n",
      "Iteration:  18% 2/11 [00:00<00:04,  2.17it/s]\u001b[A\n",
      "Iteration:  27% 3/11 [00:01<00:03,  2.17it/s]\u001b[A\n",
      "Iteration:  36% 4/11 [00:01<00:03,  2.17it/s]\u001b[A\n",
      "Iteration:  45% 5/11 [00:02<00:02,  2.17it/s]\u001b[A\n",
      "Iteration:  55% 6/11 [00:02<00:02,  2.17it/s]\u001b[A\n",
      "Iteration:  64% 7/11 [00:03<00:01,  2.17it/s]\u001b[A\n",
      "Iteration:  73% 8/11 [00:03<00:01,  2.17it/s]\u001b[A\n",
      "Iteration:  82% 9/11 [00:04<00:00,  2.17it/s]\u001b[A\n",
      "Iteration:  91% 10/11 [00:04<00:00,  2.17it/s]\u001b[A\n",
      "Iteration: 100% 11/11 [00:05<00:00,  2.17it/s]\u001b[A\n",
      "Epoch: 100% 1/1 [00:05<00:00,  5.07s/it]\n",
      "03/06/2020 03:10:31 - INFO - __main__ -    global_step = 11, average loss = 1.9345612851056186\n",
      "03/06/2020 03:10:31 - INFO - __main__ -   Saving model checkpoint to output_roberta_bus3\n",
      "03/06/2020 03:10:31 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_bus3/config.json\n",
      "03/06/2020 03:10:33 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_bus3/pytorch_model.bin\n",
      "03/06/2020 03:10:33 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_bus3/config.json\n",
      "03/06/2020 03:10:33 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:10:33 - INFO - transformers.modeling_utils -   loading weights file output_roberta_bus3/pytorch_model.bin\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_bus3' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_bus3' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_bus3/added_tokens.json. We won't load it.\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus3/vocab.json\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus3/merges.txt\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus3/special_tokens_map.json\n",
      "03/06/2020 03:10:37 - INFO - transformers.tokenization_utils -   loading file output_roberta_bus3/tokenizer_config.json\n",
      "03/06/2020 03:10:38 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_bus3']\n",
      "03/06/2020 03:10:38 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_bus3/config.json\n",
      "03/06/2020 03:10:38 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "03/06/2020 03:10:38 - INFO - transformers.modeling_utils -   loading weights file output_roberta_bus3/pytorch_model.bin\n",
      "03/06/2020 03:10:43 - INFO - __main__ -   Loading features from cached file roberta_cached_lm_510_test_text_bus3\n",
      "03/06/2020 03:10:43 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/06/2020 03:10:43 - INFO - __main__ -     Num examples = 7\n",
      "03/06/2020 03:10:43 - INFO - __main__ -     Batch size = 2\n",
      "Evaluating: 100% 4/4 [00:00<00:00,  8.80it/s]\n",
      "03/06/2020 03:10:43 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/06/2020 03:10:43 - INFO - __main__ -     perplexity = tensor(5.5353)\n"
     ]
    }
   ],
   "source": [
    "!python run_language_modelling.py --output_dir=output_roberta_bus1 --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_text_bus1 --do_eval --eval_data_file=test_text_bus1 --mlm --overwrite_output_dir\n",
    "!python run_language_modelling.py --output_dir=output_roberta_bus2 --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_text_bus2 --do_eval --eval_data_file=test_text_bus2 --mlm --overwrite_output_dir\n",
    "!python run_language_modelling.py --output_dir=output_roberta_bus3 --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_text_bus3 --do_eval --eval_data_file=test_text_bus3 --mlm  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "colab_type": "code",
    "id": "jdJv8s2c3pO_",
    "outputId": "0cf9307b-ae41-4a16-98e1-c8df70f29f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do customers want, products or services?\n",
      " \n",
      "For Mining:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEmCAYAAAB1S3f/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Wholesale:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEmCAYAAAB1S3f/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Finance/Insurance:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEmCAYAAAB1S3f/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_1_model_embedding = RobertaModel.from_pretrained('output_roberta_bus1')\n",
    "roberta_1_tokenizer = RobertaTokenizer.from_pretrained('output_roberta_bus1')\n",
    "roberta_2_model_embedding = RobertaModel.from_pretrained('output_roberta_bus2')\n",
    "roberta_2_tokenizer = RobertaTokenizer.from_pretrained('output_roberta_bus2')\n",
    "roberta_3_model_embedding = RobertaModel.from_pretrained('output_roberta_bus3')\n",
    "roberta_3_tokenizer = RobertaTokenizer.from_pretrained('output_roberta_bus3')\n",
    "text = \"what do customers want, products or services?\"\n",
    "print('what do customers want, products or services?')\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector\n",
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div,cmap='BuPu')\n",
    "    plt.show()\n",
    "\n",
    "print(\" \")\n",
    "print(\"For Mining:\")\n",
    "visualise_diffs(text, roberta_1_model_embedding, roberta_1_tokenizer)\n",
    "print(\"For Wholesale:\")\n",
    "visualise_diffs(text, roberta_2_model_embedding, roberta_2_tokenizer)\n",
    "print(\"For Finance/Insurance:\")\n",
    "visualise_diffs(text, roberta_3_model_embedding, roberta_3_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Vv-ijP8OpGuH",
    "outputId": "6f777a2e-6eee-4194-bee6-7fbe6eba5fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are the clients, other companies or individuals?\n",
      " \n",
      "For Mining:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEuCAYAAABGVo+NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debxVdb3/8df7HFFKDXCoW06glzTQ\nck6cIEul4aaVpZalNtC1HLLh96t7u+qlvGmlZWYWGqE2qKn5I6O43lQsnEBFERRDsgS9paHmBAh8\nfn+s74nF5gx7wzp7rX32++ljPc6a12cfdX/Od1jfryICMzOzInSUHYCZmQ0cTipmZlYYJxUzMyuM\nk4qZmRXGScXMzArjpGJmZoVxUjEzG8AkTZb0V0kP9HBckr4jaaGk+yXtmTt2vKQ/pOX4ep7npGJm\nNrBNAcb3cvztwMi0TAAuBpC0BXAm8GZgX+BMScP6epiTipnZABYRtwJLeznlCODyyNwBDJX0WuBw\n4MaIWBoRTwM30ntyApxUzMza3TbAY7ntxWlfT/t7tVGhoQ1g79a7Sh/P5t8WTC47BJY+t6LsEADY\nesjgskNgtxFblB0CAE/9fVnZITDt94+WHQIA7zxoRNkhALDN0FdoQ65v5Pvml/zqk2TVVl0mRcSk\nDXn+hnBSMTOrmI4GKpFSAtmQJLIE2C63vW3atwQYV7P/lr5u5uovM7OKkVT3UoCpwEdSL7D9gGcj\n4glgOnCYpGGpgf6wtK9XLqmYmVVMIyWVvkj6GVmJYytJi8l6dA0CiIjvA9OAdwALgReBE9OxpZK+\nAsxKt5oYEb01+ANOKmZmldNRTAkEgIg4to/jAXy6h2OTgYYac51UzMwqplOdZYew3pxUzMwqRi3c\n3O2kYmZWMUVWfzWbk4qZWcUU2VDfbE4qZmYVU1BX4VK0ZFKR9C3gTxHx7bQ9HXgsIj6ets8je3Hn\nkIh4V3mRmpk1rpVLKq0a+UxgfwBJHcBWwOjc8f2BjUuIy8xsg3Wqs+6lalo1qdwGjEnro4EHgOfS\nm5+bAG8A7gE2k3SNpIck/USpTCnprZLulTQ3zTWwSSmfwsysGx3qqHupmupFVIeIeBxYKWl7slLJ\n7cCdZIlmb2AusALYA/gMMArYEThA0mCy+QWOjojdyKoAT+ruOZImSJotafaf+HP/figzs0QN/FM1\nLZlUktvIEkpXUrk9tz0znXNXRCyOiNXAHGA4sDPwx4h4OJ1zGXBwdw+IiEkRsXdE7L0D2/fbBzEz\ny2vlkkpLNtQnXe0qu5FVfz0GfA74O/CjdM7y3PmraO3Pa2ZtoqOCJZB6VS/N1e824F3A0ohYlQY6\nG0pWBXZbL9ctAIZL+ue0/WFgRr9GambWADfUl2MuWa+vO2r2PRsRT/V0UUQsIxuF8+eS5gKrge/3\nZ6BmZo3oaOCfqmnZ6qCIWAW8qmbfCbn1W8hNKBMRJ+fWf0vWiG9mVjl++dHMzApTxRJIvZxUzMwq\nxgNKmplZYTz0vZmZFaaKvbrq5aRiZlYxVXypsV5OKmZmFdPKLz86qZiZVYxcUjEzs6K4pNIG/m3B\n5LJD4L92/mjZITD25AllhwDAg8OHlh0C8/d4XdkhALBq1eqyQ+DFvzxfdggAXHfDg2WHAMApx+25\nQdcX3aYiaTxwAdAJXBoR59Qc3wGYDGwNLAWOi4jF6dgqstFKAP4cEe/u7VlOKmZmFaOO4pKKpE7g\nIuBQYDEwS9LUiJifO+2bwOURcZmkQ4CvkY2LCPBSROxe7/Nat+LOzGygkupf+rYvsDAiFkXECuBK\n4Iiac0YBN6X1m7s5XjcnFTOzilGH6l7qsA3Z1CBdFqd9efcB703r7wE2l7Rl2h6cJiu8Q9KRfT3M\nScXMrGoaKKnkZ6hNy/o0fH4eGCvpXmAssIRsDiqAHSJib+CDwLcl7dTbjdymYmZWNfWVQIBshlpg\nUi+nLAG2y21vm/bl7/E4qaQiaTPgfRHxTDq2JP1cJOkWshHeH+kx9LojNzOzplBnR91LHWYBIyWN\nkLQxcAwwda3nSVtpzcsxXyLrCYakYZI26ToHOADIN/Cvw0nFzKxqOlT/0oeIWAmcDEwHHgSujoh5\nkiZK6uoePA5YIOlh4DXA2Wn/G4DZku4ja8A/p6bX2Dpc/WVmVjUFD30fEdOAaTX7zsitXwNc0811\ntwG7NfIsJxUzs6ppoE2lapxUzMyqxmN/tQ5JnWl+ezOzSqrz/ZNKat102ANJ10u6W9K8rv7akp6X\ndF5qbBojaS9JM9J50yW9tuSwzczW6Oyof6mYgVhS+WhELJX0CrIxbq4FNgXujIjPSRoEzACOiIgn\nJR1N1tOh/NEazcygpdtUqpfmNtypqURyB9kLPyPJ3gy9Nh3fGdgVuFHSHODLZC8DrSP/pur1V13e\n/5GbmQHK3pSva6maAVVSkTQOeBswJiJeTG9/DgaW5dpRBMyLiDF93S//puodD/81+iVoM7NaLqlU\nxhDg6ZRQdgH26+acBcDWksYASBokaXQzgzQz61WxoxQ31UBLKr8BNpL0IHAOWRXYWtLQz0cB56Zq\nsjnA/k2N0sysNwW+Ud9sA6r6KyKWA2/v5tBmNefNAQ5uSlBmZg2qc0yvShpQScXMbECoYAmkXk4q\nZmZVU8G2kno5qZiZVY1LKmZmVpQqvn9SLycVM7Oq2cgN9WZmVhSXVMzMrDBuUzEzs6K4TaUNLH1u\nRdkhMPbkCWWHwIzvTio7BADGnvLJskNg2Y5blB0CACuXryw7BFY981LZIQCwanXZERTEJRUzMyuM\nSypmZlaYTicVMzMrSguXVFq3M7SZ2QClDtW91HU/abykBZIWSvpiN8d3kPRbSfdLukXStrljx0v6\nQ1qO7+tZTipmZlWjBpa+biV1AheRjeA+CjhW0qia074JXB4RbwQmAl9L124BnAm8GdgXOFPSsN6e\n56RiZlY1xU7StS+wMCIWpfmkrgSOqDlnFHBTWr85d/xw4MaIWBoRTwM3AuN7e5iTiplZ1RQ7Sdc2\nwGO57cVpX959wHvT+nuAzSVtWee1a4deT0RmZtZEDSQVSRMkzc4t6/NC2+eBsZLuBcYCS4BV6xO6\ne3+ZmVVNA3/uR8QkoLe3kpcA2+W2t0378vd4nFRSkbQZ8L6IeEbSEmBczbW39BbPgCqpSBoq6VNp\nfZykG8qOycysYcW2qcwCRkoaIWlj4Bhg6tqP01aSuvLBl4DJaX06cJikYamB/rC0r0cDKqkAQ4FP\nlR2EmdmGkFT30peIWAmcTJYMHgSujoh5kiZKenc6bRywQNLDwGuAs9O1S4GvkCWmWcDEtK9HA636\n6xxgJ0lzgJeBFyRdA+wK3A0cFxEhaS/gfGAz4CnghIh4oqygzczWUvC7jxExDZhWs++M3Po1wDU9\nXDuZNSWXPg20pPJFYNeI2F3SOOD/AaOBx4GZwAGS7gQuBI6IiCclHU2WlT9aUsxmZmvrbN1KpNaN\nvD53RcTiiFgNzAGGAzuTlVxuTCWaL5M1Pq0j36vi19f9pFkxm1m7K/Dlx2YbaCWVWstz66vIPq+A\neRExpq+L870qpt29OPolQjOzWi089P1AK6k8B2zexzkLgK0ljQGQNEjS6H6PzMysXi6pVENE/E3S\nTEkPAC8Bf+nmnBWSjgK+I2kI2e/g28C85kZrZtaDFh6leEAlFYCI+GAP+0/Orc8BDm5aUGZmDah3\n9OEqGnBJxcys5TmpmJlZYZxUzMysMK2bU5xUzMwqxw31ZmZWmBZ+2cNJxcysalxSMTOzoqjTScXM\nzIriksrAt/WQwWWHwIPDh5YdAmNP+WTZIQAw48IflB0CY0f9e9khZJatLDuCynwJaugmZYdQjIr8\nPteHk4qZWdW4od7MzArjkoqZmRXGDfVmZlYYl1TMzKwwTipmZlYYN9SbmVlhWrik0sL50MxsgJLq\nX+q6ncZLWiBpoaQvdnN8e0k3S7pX0v2S3pH2D5f0kqQ5afl+X89yScXMrGoK7P0lqRO4CDgUWAzM\nkjQ1IubnTvsycHVEXCxpFDANGJ6OPRIRu9f7vMqVVCSdJenzaX2ipLet531278q2ZmYtRQ0sfdsX\nWBgRiyJiBXAlcETNOQG8Kq0PAR5f39Arl1TyIuKMiPif9bx8d8BJxcxaT4fqXiRNkDQ7t0youds2\nwGO57cVpX95ZwHGSFpOVUk7JHRuRqsVmSDqor9BLr/6S9BHg82SZ8n7gkdyxKcANEXGNpL2A84HN\ngKeAEyLiCUm3AHcCbwGGAh9L2xOBV0g6EPga8L/ABenWARwcEc/1+wc0M2tUAw31ETEJmLSBTzwW\nmBIR50kaA1whaVfgCWD7iPhb+g6+XtLoiPh7TzcqNalIGk1Wl7d/RDwlaQvg1G7OGwRcCBwREU9K\nOho4G/hoOmWjiNg3VXedGRFvk3QGsHdEnJzu8Uvg0xExU9JmwLL+/4RmZuuh2M5fS4Dtctvbpn15\nHwPGA0TE7ZIGA1tFxF+B5Wn/3ZIeAV4PzO7pYWVXfx0C/DwingKIiKU9nLczsCtwo6Q5ZIlo29zx\n69LPu1nTuFRrJnC+pFOBoRHR59Cu+WLlL668vM8PY2ZWiM6O+pe+zQJGShohaWPgGGBqzTl/Bt4K\nIOkNwGDgSUlbp4Z+JO0IjAQW9faw0qu/6iRgXkSM6eH48vRzFT18pog4R9KvyNpZZko6PCIe6u2h\n+WLlrIVPxXpFbmbWqAJLKhGxUtLJwHSgE5gcEfMkTQRmR8RU4HPAJZJOJ2seOCEiQtLBwERJLwOr\ngX/t5Y9/oPykchPwC0nnpzq7LXo4bwGwtaQxqWg2CHh9RMzr5d7PAZt3bUjaKSLmAnMl7QPsAjwk\n6aGI2KWgz2NmtuE6iq3/iohpZA3w+X1n5NbnAwd0c921wLWNPKvUpJKy5dnADEmrgHuBR7s5b4Wk\no4DvSBpCFve3gd6Sys3AF1N12deAAyW9hSzbzgN+LWkriq69NDPbUC38Rn3ZJRUi4jLgsh6OnZBb\nnwMc3M0543LrT5HaVFIRbZ/cqVfVXivpULKXgszMqqN1c0r5SaVMEXFD2TGYma2j4OqvZmrrpGJm\nVkmepMvMzArjNhUzMyuMk4qZmRWm7NfSN4CTiplZ1bikYmZmhXFSMTOzwrj318C324ieRpBpnvl7\nvK7sEFi2Y/m/B4Cxo/697BCYcdLZZYcAwNiTPlF2CLDzlmVHAED8fYAMPu6SipmZFcYN9WZmVhS5\npGJmZoVxUjEzs6J0eOwvMzMrjNtUzMysKG5TMTOz4jipmJlZUVo4pzipmJlVTgtnFScVM7OKUQsP\n09KyfQwkDZX0qdz2OEmeHtjMWp6kupeqadmkAgwFPtXnWXWS5FKbmVWCVP9S3/00XtICSQslfbGb\n49tLulnSvZLul/SO3LEvpesWSDq8r2e1TFKR9FlJD6TlM8A5wE6S5kj6RjptM0nXSHpI0k+U0rik\nvSTNkHS3pOmSXpv23yLp25JmA6eV88nMzGoUmFUkdQIXAW8HRgHHShpVc9qXgasjYg/gGOB76dpR\naXs0MB74Xrpfj1oiqUjaCzgReDOwH/AJ4FzgkYjYPSK+kE7dA/gM2S9uR+AASYOAC4GjImIvYDKQ\nH15244jYOyLO6+a5EyTNljT7h5dM6q+PZ2a2loKrv/YFFkbEoohYAVwJHFFzTgCvSutDgMfT+hHA\nlRGxPCL+CCxM9+tRq1T5HAj8IiJeAJB0HXBQN+fdFRGL0zlzgOHAM8CuwI3pX0An8ETumqt6emhE\nTAImASxbtTo2+FOYmdWjgT/3JU0AJuR2TUrfXV22AR7LbS8m+wM97yzgvyWdAmwKvC137R01127T\nWzytklTqtTy3vors8wmYFxFjerjmhX6PysysAeqoP6vk//jdAMcCUyLiPEljgCsk7bo+N2qJ6i/g\nd8CRkl4paVPgPcBMYPM6rl0AbJ1+UUgaJGl0/4VqZrZhCm6oXwJsl9veNu3L+xhwNUBE3A4MBraq\n89q1tERSiYh7gCnAXcCdwKURcTcwMzXcf6OXa1cARwHnSroPmAPs3/9Rm5mtn4LbVGYBIyWNkLQx\nWcP71Jpz/gy8NT37DWRJ5cl03jGSNpE0AhhJ9j3co5ap/oqI84Hza/Z9sOa0W3LHTs6tzwEO7uae\n4woN0sysCAX+uR8RKyWdDEwna1OeHBHzJE0EZkfEVOBzwCWSTidrtD8hIgKYJ+lqYD6wEvh0RKzq\n7Xktk1TMzNpF0S81RsQ0YFrNvjNy6/OBA3q49mzW7jHbKycVM7OKkSfpMjOzwlRw+JV6OamYmVWM\nSypmZlaYFi6oOKmYmVVOC2cVJxUzs4qp4pD29XJSqdNTf19WdgisWrW67BBYuXxl2SFklpUfx9iT\nPlF2CADMuPiSskPgoA8cV3YIAHQMH1p2CIVo5Um6nFTMzCqmdVOKk4qZWeW4+svMzArTwjnFScXM\nrGqcVMzMrDBq4VYVJxUzs4ppYI6uynFSMTOrGDfUm5lZYVo3pTipmJlVjksqZmZWmBbOKa0xR30j\nJL1O0jVlx2Fmtr46pLqXqhlwJZWIeBw4quw4zMzWVwVzRd3Wu6Qi6SOS7pd0n6QrJA2XdFPa91tJ\n26fzpki6WNIdkhZJGidpsqQHJU3J3e95Sd+SNC9dv3Xa/wlJs9JzrpX0ytx9vyPptnTfo9L+4ZIe\nSOudkr6Rrr9f0ifT/tdKulXSHEkPSDpovX+DZmYFk1T3UjXrlVQkjQa+DBwSEW8CTgMuBC6LiDcC\nPwG+k7tkGDAGOB2YCnwLGA3sJmn3dM6mwOyIGA3MAM5M+6+LiH3Scx4EPpa772uBA4F3Aed0E+rH\ngGcjYh9gH+ATkkYAHwSmR8TuwJuAOT18zgmSZkua/ZMpk+v87ZiZbRg1sFTN+lZ/HQL8PCKeAoiI\npZLGAO9Nx68Avp47/5cREZLmAn+JiLkAkuYBw8m+1FcDV6Xzfwxcl9Z3lfRVYCiwGTA9d9/rI2I1\nMF/Sa7qJ8zDgjV2lGGAIMBKYBUyWNCjdo9ukEhGTgEkAi59+Mfr4nZiZFaKCBZC6NatNZXn6uTq3\n3rXdUwxdX+JTgCMj4j5JJwDjurkvdJ+0BZwSEdPXOSAdDLwTmCLp/Ii4vI/PYGbWFEVXa0kaD1wA\ndAKXRsQ5Nce/Bbwlbb4SeHVEDE3HVgFz07E/R8S7e3vW+rap3AS8X9KW6aFbALcBx6TjHwJ+1+A9\nO1jTwP5B4PdpfXPgiVSq+FCD95wOnJSuRdLrJW0qaQeyEtMlwKXAng3e18ys33So/qUvkjqBi4C3\nA6OAYyWNyp8TEadHxO6pSeBC1tQUAbzUdayvhALrWVKJiHmSzgZmpCx2L3AK8CNJXwCeBE5s8LYv\nAPtK+jLwV+DotP8/gDvTPe8kSzL1upSseu0eZan/SeBIstLOFyS9DDwPfKTBWM3M+k3BJZV9gYUR\nsSjd+0rgCGB+D+cfy5o27Yatd/VXRFwGXFaz+5Buzjsht/4osGt3x9L2Z7u5/mLg4t7um7Y3q31G\nam/5t7TkdRe7mVklNJJTJE0AJuR2TUrtwV22AR7LbS8G3tzDvXYARpDVRnUZLGk2sBI4JyKu7y2e\nAfeeiplZq2tk6Pt8h6ICHANcExGrcvt2iIglknYEbpI0NyIe6ekGlXmjvqukYWbW7qT6lzosAbbL\nbW+b9nXnGOBn+R0RsST9XATcAuzR28Mqk1TMzCzTKdW91GEWMFLSCEkbkyWOqbUnSdqF7J3C23P7\nhknaJK1vBRxAz20xgKu/zMwqp8h2+ohYKelkst6wncDk1NlqItkL510J5hjgyojIv5P3BuAHklaT\nFULOiQgnFTOzVlL0y48RMQ2YVrPvjJrts7q57jZgt0ae5aRiZlYxVRzTq15OKmZmFdO6KcVJpW7T\nfv9o2SHw4l+eLzsEVj3zUtkhZKrwl9zOW5YdAQAHfeC4skPgd1f/uOwQADj4w8eXHUIhXFIxM7PC\ndLRwv1wnFTOzimnk5ceqcVIxM6uYFq79clIxM6sat6mYmVlhWjinOKmYmVWNSypmZlaYeibfqion\nFTOzinFJxczMCtPCOcVJxcysavyeipmZFcYllRanrAJTaU57M7NSdbRwS30LjzDTGEmflfRAWj4j\nabikBZIuBx5g7ek2zcxKU/B0wk3VFiUVSXsBJwJvJhtV+k5gBjASOD4i7igxPDOztbRym0q7lFQO\nBH4RES9ExPPAdcBBwJ96SyiSJkiaLWn2rb+5ulmxmlmbc0mldb3Q28GImARMApj0y/nR27lmZkVp\n5fdU2qWk8jvgSEmvlLQp8J60z8ysclxSqbiIuEfSFOCutOtS4OnyIjIz61lHFbNFndqlpEJEnB8R\nu6bl2xHxaETsWnZcZma1OqS6l3pIGp96uy6U9MVujn9L0py0PCzpmdyx4yX9IS19ztfcFiUVM7NW\nUmRBRVIncBFwKLAYmCVpakTM7zonIk7PnX8KsEda3wI4E9gbCODudG2PNT1tU1IxM2sVBbep7Ass\njIhFEbECuBI4opfzjwV+ltYPB26MiKUpkdwIjO/tYU4qZmYVowb+qcM2wGO57cVp37rPlXYARgA3\nNXptFycVM7OKaaSkkn+fLi0TNuDRxwDXRMSq9b2B21TMzCqmkbG/8u/T9WAJaw9DtW3a151jgE/X\nXDuu5tpbeovHJRUzs4qRVPdSh1nASEkjJG1MljimdvPMXYBhwO253dOBwyQNkzQMOCzt65FLKmZm\nFVPkWyoRsVLSyWTJoBOYHBHzJE0EZkdEV4I5BrgyIiJ37VJJXyFLTAATI2Jpb89zUjEzq5iih2mJ\niGnAtJp9Z9Rsn9XDtZOByfU+S7mkZL1Y8sxLpf+irrvhwbJDYMXjz5UdAgAaOrjsEIi/Lys7hMyT\nL5UdAfFENf67uPWKy8oOAYCpccMGZYU//e2Fur9vdthy00q9fu+SiplZxbTygJJOKmZmFdO6KcVJ\nxcysclq4oOKkYmZWNS2cU5xUzMwqp4WLKk4qZmYV07opxUnFzKxy3PvLzMwK08I5xUnFzKxqWjin\n9D2gpKTbGrmhpHGSbkjr7+5u6sqa8ydKeltv91kfkh6VtFUPxzaRdL2kB9Ly5vV9jplZ0QqepKup\n+iypRMT+63vzNFDZOqNh1pxzRm/H+0kHcEFE3CzpcOBsYJ3EZmZWjgpmizrVU1J5Pv0cJ+kWSddI\nekjST5RakySNT/vuAd6bu/YESd+VNETSnyR1pP2bSnpM0iBJUyQd1cd9zpL0+dz2A5KGp/XrJd0t\naV53k9OkZ/1K0n3puqMj4qWIuDmdsglQkUGczMwGeEmlxh7AaOBxYCZwgKTZwCXAIcBC4KraiyLi\nWUlzgLHAzcC7gOkR8XJXLwdJg/u6Tw8+moZnfgUwS9K1EfG33PHxwOMR8c70nCFdByRtB3wLOLrO\nZ5mZ9bsqJot6NTpJ110RsTgiVgNzgOHALsAfI+IPaRz+H/dw7VWs+fI+hnWTRr33qXWqpPuAO8hm\nNxtZc3wucKikcyUdFBHP5o5dAPxnRMzu7sb5aTp/POWHdYZjZrZhCp6jvqkaLaksz62vavD6qcB/\nSdoC2Au4qYFrV7J2AhwMWZUcWVvImIh4UdItXce6RMTDkvYE3gF8VdJvI2JiOvxG4JM9PTQ/TWcV\nhr43s/bQTiWV7jwEDJe0U9o+truTIuJ5stnDLgBuiIhVDdznUWBPgJQgRqT9Q4CnU0LZBdiv9rmS\nXge8GBE/Br7RdZ/kdODZ2mvMzMqkBpaq2eD3VCJiWWog/5WkF4HfAZv3cPpVwM+BcQ3e51rgI5Lm\nAXcCD6f9vwH+VdKDwAKyKrBauwHfkLQaeBk4KXfspPScFXV+XDOz/tfCRRXP/FinKlR/eebHNTzz\nY45nfvyHgTLz47PLXq77+2bI4EGVykB+o97MrGIqlSUa5KRiZlY1LVz95aRiZlYxrZtSiun9ZWZm\nBSr6jfo0WskCSQt7Go9R0gckzU+jk/w0t3+VpDlp6XXYLXBJxcysgoorq0jqBC4CDgUWk408MjUi\n5ufOGQl8CTggIp6W9OrcLV6KiN3rfZ5LKmZmFdOh+pc67AssjIhFEbECuBI4ouacTwAXRcTTABHx\n1/WOfX0vNDOz/lFw9dc2wGO57cVpX97rgddLminpDknjc8cGp+Gq7pB0ZF8Pc/WXmVnl1F/9lV4a\nz4/QPikNMdWIjcjGTRwHbAvcKmm3iHgG2CEilkjaEbhJ0tyIeKTHO0WElyYswISyY6hKHFWIoSpx\nVCGGqsRRhRiqFEeBn2cM2ajwXdtfAr5Uc873gRNz278F9unmXlOAo3p7nqu/mmeduV5KUoU4qhAD\nVCOOKsQA1YijCjFAdeIoyixgpKQRkjYmGyW+thfX9aThs9KMua8HFkkaJmmT3P4DgPn0wtVfZmYD\nWESslHQyMB3oBCZHxDxJE4HZkc3QOx04TNJ8shHovxARf5O0P/CDNHZiB3BO5HqNdcdJxcxsgIuI\nacC0mn1n5NYD+Gxa8ufcRjYob91c/dU8jTac9ZcqxFGFGKAacVQhBqhGHFWIAaoTR0vyKMVmZlYY\nl1TMzKwwTipmZlYYJxUzMyuMk4pZE0nqlHR62XFUgTLblR2HFctJpR9JGiTpVEnXpOUUSYNKiOM1\nkn4o6ddpe5Skj7VbDFUQEauAY8uOQ9IBkjZN68dJOl/SDs2MIXVjndbnif1I0j6SbpQ0VdIeZcYy\nUDip9K+Lgb2A76Vlz7Sv2aaQvdz0urT9MPCZNoxhLZIeTMvJTX70TEnflXSQpD27libHcDHwoqQ3\nAZ8DHgEub3IMAPdI2qeE53b5IfAN4FLg55I+JGlLSRtJelWJcbUsdynuR5Lui4g39bWvCXHMioh9\nJN0bEXukfXOigTkSBkIMPcS1JbBfRPyqic+8uZvdERGHNDGGeyJiT0lnAEsi4odd+5oVQ4rjIbKB\nDB8FXiAbSTEi4o1Nev7ciNgtrW8HfJvsZb/PAv8aEe9qRhwDid+o71+rJO0UaUTPNMrnqhLieCF9\neUaKYz/g2XaLIVX3vBQRqyW9HtgF+HUzEwpARLylmc/rwXOSvgR8GDhIUgfQ9KpZ4HBgGHBQ2r4V\neKaJz18oaWxEzIiIx4D35VHxS7AAAAofSURBVI7d0MQ4BgyXVPqRpLcCPwIWkf0FtgPZSKDd/aXa\nn3HsCVwI7Ao8AGxNNtLo/W0Ww91kX17DgJlkA+2tiIgPNSuGFMdrgP8CXhcRb5c0ChgTET9sYgz/\nBHwQmBURv5O0PTAuIppaBSbpNODjwHVk/48cCVwSERc26fkbAx0RsawZz2sHTir9LI3wuXPaXBAR\ny5v8/A5gP+CuFIdSHC83M44Uy0ZlxpCr8jkFeEVEfL2MKrjUWeFHwL9HxJvS7+XermqYJsaxAzAy\nIv5H0iuBzoh4rskx3E+WUF9I25sCtzer+isXx/uB30TEc5L+A9gD+GpE3NPMOAYCN9T3v73I/jrf\nHTha0kea+fCIWE02TejKiJgXEQ+UkVCSfYE3kXVYOLbZvwuyXqxjgA8BXVVenU2OAWCriLgaWA3Z\nKLI0uVpU0ieAa4AfpF3bkA1/3mxi7c++iiInaK/ff6SEciDwVrIG/DI61bQ8t6n0I0lXADsBc1jz\nP07Q/F42v5X0PuC6KKloWpHfxWlkExT9Ig39vSPQ1KrIpPT2JeDTZEn+ToCI+IOkVzc5BshKbHdK\n+kXaPpLsC73Zuv6bfCfZzIm/kvTVEuJoeU4q/WtvYFRZX+Q5nyTrzbJS0jLW9LBpZpfJKvwuXhMR\n7+7aiIhFkn5XQhyfJZskaSdJM0ntS02OYXlErFCa5DxVwTX9301EnC/pFuDAtOvEiLi32XEASyT9\nADgUODdVW7smZz24TaUfSfo5cGpEPFGBWLYg67o5uGtfRMxo4vNL/11012W2jG606bllty99nayX\n1UeAU4BPAfMj4t+bGUdVpDal8cDcVGp7LbBbRPx3yaG1HCeVfiDpl2R/9W1O1pZyF/CPBvr8X8tN\niufjZFU/25JVP+0H3BYRb23Cs0v/XUh6O/AO4APAVblDryIrPe3b3zF0E9P+wHBytQXN7HmVOnB8\nDDiMLLFNBy6tQKm6qdIfWz2KiKXNimWgcPVX/7iJrM//PUBZjeJ5pwH7AHdExFsk7ULWpbUZvkn2\npXUuWX15l659zfA4MBt4N3B3bv9zQNPH4apC+1LqwHFJWtrZ3WS/e+V+dglgxzKCamVOKv1jG2B/\nskbh+8neibiNrHRQxl8+yyJimSQkbRIRD0naue/LNlxXFZukQbXVbZJe0aQY7gPuk/TTEnu+5ZXW\nviTp6oj4gKS5dNOG0uyuvGWLiBFlxzDQOKn0g4j4PPzjxaq9yRLMicAkSc9ExKgmh7RY0lCyLqM3\nSnoa+FMzHizpJLL6+h3TOwldNidLts20r6SzyF5C3Yg1HRaa/dfoA8A/AWW0L52Wfnr4kRqShrFu\nu+Ot5UXUmtym0o8kDQHGAAekn0PJGgJPLDGmscAQshe9VjTheUPI3mD/GvDF3KHnml1qS+NMnU5W\n5fGPdyMi4m9Nen7p7UvWvR7aHW9v5nhsA4WTSj+QNAkYTVZnfydwB1l7xtOlBtbmJN0ZEW8u8flj\nezve5N547yVr03o1WYmtjG7mlZGqA7vaHXfvaneMiPeWHFrLcfVX/9ge2AT4A7AEWExzB8mz7t0s\n6Rtk40zlSwhNGYoj1750bkT83/wxSecCTUsqwNeBf4mIB5v4zCorrd1xoHFJpZ8oe6tsNFl7yv5k\nQ7UsJStSn1lmbO2qCkPOpzi6e1/m/mY2kkuaGREHNOt5VZfe6D+RbI6fQ4CngUER8Y5SA2tBTir9\nTNK2ZG0q+5M1jm4ZEUPLjcrKkOu0sBOwMHdoc7KegU0bLVnSBWSdBa5n7VLbdc2Koaqa3e440Dip\n9ANJp7KmhPIyqTtxWuamdwSsycoecr6m08I5wMHp0O+bPTSJpB91szsi4qPNjKMq0tD/64iIPzc7\nllbnpNIPJJ1PejelCkO0WKZCQ86XOoeIrSv33o7IuhSPIBs+Z3SpgbUgJxVrG6rIlMZVmENE0mCy\nYVpGs/Z7GW1ZUqmVJpX7VER8vOxYWo1H4bR2UoUh56Eac4hcQdamcjhZr7NtybrAG//oEVha9/NW\n5i7F1k6qMOQ8VGMOkX+OiPdLOiIiLpP0U6CMaQAqQdJnc5sdZBPJPV5SOC3NScXaRkTck3r2lDqt\nckXmEOn63M9I2hX4X7IXIdvV5rn1lWQzg15bUiwtzW0qNuBJOiQibkpvka+jHbvRpmFJrgXeSFZy\n2oxsSt0f9HqhWR+cVGzAk/SfEXGmu9Fardx4bN3yeGyNc1Ixa0Opw8JZZC/mBll7yleaNbhmVeTG\nY3svWceFH6ftY4G/RETT59tpdU4qNuDVNMKuIyLOb1YsVSHpRuBW1nyJfggYFxFvKy+q8kiaHRF7\n97XP+uaGemsHm/dyrF3/qnptRHwlt/1VSUeXFk35NpW0Y0QsApA0Ati05JhakpOKDXgR8Z8Aki4D\nTouIZ9L2MOC8MmMr0X9LOga4Om0fRTZPfbs6HbhF0iKynoE7AJ8sN6TW5Oovaxv5N+l729cOJD1H\n9pd41zh0HcALab0t51WRtAmwS9p8KCKW93a+dc8lFWsnHZKGdU2WJmkL2vT/gYjorUqwbfTS3Xwn\nSW3Z3XxDteX/UNa2zgNul/TztP1+4OwS4ymVpDcCw8l9D7Thl+hY4CbgX7o5FmSDfloDXP1lbSUN\nd981KddNETG/zHjKImky2YuP81hTBda27+xI6oyIVX2faX1xUjFrQ5LmR8SosuOoCkl/Bn4DXEX2\nx4a/GNeTRyk2a0+3p1KbZXYB/gf4NPBHSd+VdGAf11g3XFIxa0PpTfKpZANJLifrRhvNnNOlqlJX\n8wuAD0VEZ9nxtBo31Ju1px8CHwbmsqZNpa2lRHs0MB6YDXyg3Ihak0sqZm1I0u0RMabsOKpC0qPA\nvWQvg07tmpXTGuekYtaGJH0PGAr8kqz6C2jLLsUASHpVRPy97DgGAld/mbWnV5Alk8Ny+9ruvQxJ\n/ycivk429tk6xyPi1OZH1dqcVMzaUEScWHYMFfFg+nl3qVEMIK7+MmtDkrYFLiSbTwWy+VROi4jF\n5UVlA4GTilkbSvOp/BS4Iu06jqwL7aHlRdV8nvmxeE4qZm1I0pyI2L2vfQOdZ34snttUzNrT3yQd\nB/wsbR8LtNVUwgARMQNA0nk1szz+UtLsksJqaR6mxaw9fZTs5b7/BZ4gm6TrhDIDKtmmknbs2vDM\nj+vPJRWz9jQROL5mbplvkiWbduSZHwviNhWzNuRZMNflmR+L4ZKKWXvyLJjr2os1k5a9Kc38eHm5\nIbWedv+PyKxdeRbMHElXADsBc4CuyboCcFJpkKu/zNqUZ8FcQ9KDwChPzrXhXFIxa1MpibRtIqnx\nANl7Kk+UHUirc1IxM4OtgPmS7mLtUZv9Rn2DnFTMzOCssgMYKNymYmZmhXFJxczalqTfR8SBkp5j\n7YElBUREvKqk0FqWSypmZlYYj/1lZmaFcVIxM7PCOKmYmVlhnFTMzKwwTipmZlaY/w8G2k9pa/6Q\nZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Wholesale:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEuCAYAAABGVo+NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Finance/Insurance:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEuCAYAAABGVo+NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npGJmZoX5/+2QRBlf/RCiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Who are the clients, other companies or individuals?\"\n",
    "print(\"Who are the clients, other companies or individuals?\")\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector\n",
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div,cmap='BuPu')\n",
    "    plt.show()\n",
    "\n",
    "print(\" \")\n",
    "print(\"For Mining:\")\n",
    "visualise_diffs(text, roberta_1_model_embedding, roberta_1_tokenizer)\n",
    "print(\"For Wholesale:\")\n",
    "visualise_diffs(text, roberta_2_model_embedding, roberta_2_tokenizer)\n",
    "print(\"For Finance/Insurance:\")\n",
    "visualise_diffs(text, roberta_3_model_embedding, roberta_3_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "colab_type": "code",
    "id": "pdfFOmYyxO4q",
    "outputId": "52811be5-80fe-4f22-e362-291af67d0684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficiency, innovation, profit, satisfaction\n",
      " \n",
      "For Mining:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZXmYGJmZqX9f8F4xRNo19+NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Wholesale:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns9IcTMzMrDQHEzMzK83BxMzMSvv/1t0FkKZUaN8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Finance/Insurance:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"efficiency, innovation, profit, satisfaction\"\n",
    "print( \"efficiency, innovation, profit, satisfaction\")\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector\n",
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div,cmap='BuPu')\n",
    "    plt.show()\n",
    "\n",
    "print(\" \")\n",
    "print(\"For Mining:\")\n",
    "visualise_diffs(text, roberta_1_model_embedding, roberta_1_tokenizer)\n",
    "print(\"For Wholesale:\")\n",
    "visualise_diffs(text, roberta_2_model_embedding, roberta_2_tokenizer)\n",
    "print(\"For Finance/Insurance:\")\n",
    "visualise_diffs(text, roberta_3_model_embedding, roberta_3_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "8-Deep-Neural-Nets-and-Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0267db5236654a2cbab5f6d0a15055fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75eea0a1cb604f85aad6950b4cfe643e",
      "max": 267844284,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bcae8e6ce22b48a18c1ecad412a7ea80",
      "value": 267844284
     }
    },
    "0366a1c198b74cf8bc59f7b989a78962": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03ec901a81824e29b1d79c5d479e1060": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0400d7b7a7de4f59a75cdcafb641788f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_798c179ba832479abd0865f4122248ad",
       "IPY_MODEL_8717d409a1d34953ab3c933f39a98a8a"
      ],
      "layout": "IPY_MODEL_cd2d2f02d6e14ec7bcbae92b5ab41ae2"
     }
    },
    "0459bebe18284628b8cb69153caa2ad9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04eedd44f1784e44b967e3e21da0cc2b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06b046743c9c4677bc50476642edf86b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7754dc2db14f46819ae717b97d258a8b",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_189879bb380e4174babb002788421dcc",
      "value": 230
     }
    },
    "0aa7acf3f9c84a498b0e583105e86bd8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ba929806a8b450f937a4d279d4cdcc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df4f9a6441854f9599cf310df5dacfb9",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03ec901a81824e29b1d79c5d479e1060",
      "value": 456318
     }
    },
    "0c69931beb6e4e4fa379eb61a4855770": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0c8ac724bedf46ffbad468fcb7a62854": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d6be7f7746e4e21b34658476fe1cc63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_325a94541107432d8c4f2f162582d0cc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ddeeea9f818f4da29fef748f1a11c6ed",
      "value": "100% 1.04M/1.04M [00:00&lt;00:00, 2.01MB/s]"
     }
    },
    "0f786775f4844100a7420e86619dbaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53a2dfa96c284fa9b6c5e270e4513816",
       "IPY_MODEL_684f038732ea49f1ab30a8984b415378"
      ],
      "layout": "IPY_MODEL_e1b3ac54a9e84afeb4614e6b8f8ff4ba"
     }
    },
    "110aa51584474035b9885ddb15006d46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ca99b53bae64f0f8826568b6458129c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dcd9948b90884364aaa0302020454586",
      "value": "100% 263M/263M [00:07&lt;00:00, 34.6MB/s]"
     }
    },
    "15f69272b659412fb72b3fbbf2de3481": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0459bebe18284628b8cb69153caa2ad9",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20ab167e39954d12892198819771ecf0",
      "value": 230
     }
    },
    "168914421c44490ba1d9ebef4dd2285b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16e58bf38bca4a7791e8a0bcf38d22bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ffafda6efc244eabd05a499661e1efe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fe20bb95b2784f4a8d15f0961440298c",
      "value": "100% 230/230 [00:00&lt;00:00, 9.34kB/s]"
     }
    },
    "189879bb380e4174babb002788421dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1b7054cd78724aaeb2c677f04a2eba92": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cacdf03f2fc4ee2b6201d699daeb6c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e57097898c444dc59fc6bdf63569479e",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ece8e8c8c7e40378209c1d208096bab",
      "value": 1042301
     }
    },
    "1d474aaa9fcc4958906303906618f82d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d071e6f1dbb84a42b3734f8453b4d5f2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fa1e59f9356944169597cc80b15ceb34",
      "value": "100% 230/230 [00:00&lt;00:00, 6.17kB/s]"
     }
    },
    "1fd2d488f6ab465386ac54de28cfa8be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c566e7dfc8004acb91afe56d80d2cd09",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8d9dc4691f09421b82249759d59c52c3",
      "value": "100% 548M/548M [00:13&lt;00:00, 39.2MB/s]"
     }
    },
    "20ab167e39954d12892198819771ecf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "21e86d0a58414b618e9b1e44ae0807f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2214f935952241c896122dc30fdba56c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "23147d17f077408f84782289fea9d2d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25b9f6330a4d4fa4b275b7c1e0d2d678": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "277e4e17087b42c48277645acf81d7c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5845154f5c84d47966f9818145e995d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_74fa92af960941d58ffa5372a43d7d0f",
      "value": "100% 1.03k/1.03k [00:00&lt;00:00, 24.6kB/s]"
     }
    },
    "27e8637530184116b237a411245705a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40009705f2d3474f9535cfc69d010642",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bcb6792329da49cb853b67a24837f4ea",
      "value": "100% 224/224 [00:00&lt;00:00, 8.81kB/s]"
     }
    },
    "2a56afe24de74da59653273a363d76a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c53acb46651407aaa27ea7618ce4dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0267db5236654a2cbab5f6d0a15055fc",
       "IPY_MODEL_fc7d08ca0dda4e3abd5cc5ed64ff4aa8"
      ],
      "layout": "IPY_MODEL_e762d375038844ceb64ec475ab72810e"
     }
    },
    "2d04f9e061994fb1a25baa976bf7c000": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32c5cc026037428da5a352cdd7b18ae3",
      "max": 546,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eeac5bb82c744e728b10fe7a1b461b3a",
      "value": 546
     }
    },
    "2d148fd728ce46e79b36d378b1fd36da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2db97fa8ea044d7397bfc87a36629403": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30c428ba615a470ab6405498d5b47e26": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "325a94541107432d8c4f2f162582d0cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32c5cc026037428da5a352cdd7b18ae3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "367489dd91ed4fcdb38b8f9e38392f5b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37e659127e874855b294e29f3232c697": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3843c77b6fb44d36aeb29f590d7b014e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38d800a371544d7d914870a22a971791": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_573846cb97fe4b019f0e430c63e11624",
       "IPY_MODEL_ccd661cec496474ebdfefa7e1f4a8926"
      ],
      "layout": "IPY_MODEL_367489dd91ed4fcdb38b8f9e38392f5b"
     }
    },
    "3edc98e5e74f4f23a4e886d7caa48c00": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f8be95358e742189ec05e713415d8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40009705f2d3474f9535cfc69d010642": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40c30550ae044c3c81bb99d07b2b243f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4108dada4e7a436db3c1bf1d62eb71ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4256cedc972d4e34b118630cce1d3a94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6562a944aeef4f86bcc9c3c5ebf9512a",
       "IPY_MODEL_277e4e17087b42c48277645acf81d7c2"
      ],
      "layout": "IPY_MODEL_b94d9faee7c64a4f8d00e7544746a392"
     }
    },
    "42f9a4d3a0644586aa9a2300059b8b5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d04f9e061994fb1a25baa976bf7c000",
       "IPY_MODEL_864d218ed3894b8f9d178fb6d91d57ce"
      ],
      "layout": "IPY_MODEL_3f8be95358e742189ec05e713415d8bb"
     }
    },
    "44f31b755da44bccad5d98afffc6f9aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "450ec91b8b154f3d9b1bb9de297132bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4600154a77644e64a9b696920e76a798": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b797454c7f0b4dfe9972afad19682e9d",
       "IPY_MODEL_dd22f69726974e8692b388a3ead4481c"
      ],
      "layout": "IPY_MODEL_6c8e1a0cba1e49c29aeb80e8cd9f51d8"
     }
    },
    "484daf2c6f5540d9b8fdd65129d97ea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49fdd42cc5b0438cbd334a8fd66e7716": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b62b06e0f98429c9390451a3bbce25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4d78a3d10ada4b7f817bc8ec8afcfa31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dbb3164b60949f89945db07f24b88c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d148fd728ce46e79b36d378b1fd36da",
      "max": 939,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea9ac97b52ab4b7a8829625a44436d9f",
      "value": 939
     }
    },
    "52574793d4324d098cc8826a1eb23212": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e921a62ea30847178f0ec3f0b077d52d",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed7a086eb21143cd86db4196d1e669c0",
      "value": 230
     }
    },
    "52c4fadf83ee4ebf825dab5a3ca82177": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_835df7e043fb439492cc8a0a79c8c1c0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0366a1c198b74cf8bc59f7b989a78962",
      "value": "100% 232k/232k [00:00&lt;00:00, 359kB/s]"
     }
    },
    "52f34b5a72794d2fa14a71405a36edf5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53a2dfa96c284fa9b6c5e270e4513816": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b921eb0bb0f94d73a168dd1b5ddafa47",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce12705a19764d7eaf45685f0cd31f8a",
      "value": 548118077
     }
    },
    "549ea68b85ba4235b5bcece65a99029e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63967fe3a91e4834996504ed6a3dbab5",
       "IPY_MODEL_c4cbcbcf9b2d487b95e493243912cfed"
      ],
      "layout": "IPY_MODEL_d49515c60cf847319a766c6ea71757d4"
     }
    },
    "54b79428b57744e696a8bee377960d2f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "573846cb97fe4b019f0e430c63e11624": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3edc98e5e74f4f23a4e886d7caa48c00",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a02b997d8bd24a55ac5dd69c931e8e2d",
      "value": 230
     }
    },
    "5b0e47b9c7d74e6ca45746080c1c45b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fbb258deee44ee09493d75ec60555b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a56afe24de74da59653273a363d76a2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7018b77e581340e3b70b9c78e81f94d3",
      "value": "100% 456k/456k [00:00&lt;00:00, 1.77MB/s]"
     }
    },
    "600b0c9be1084a829610ec52c5b89b44": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63967fe3a91e4834996504ed6a3dbab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce75547630244baabe6837ab3f5f4c54",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92bc2dd05ba24674b3bc505380775244",
      "value": 213450
     }
    },
    "6411f5ce1ec147c787581e3a7fdf0d50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78349090137f4988a315a27f5eee097a",
       "IPY_MODEL_dcc90a4b6edf461eb1f4785d94bb699c"
      ],
      "layout": "IPY_MODEL_b87879eb1fb643cbba9f4e49cdeb9921"
     }
    },
    "6562a944aeef4f86bcc9c3c5ebf9512a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44f31b755da44bccad5d98afffc6f9aa",
      "max": 1031,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a82f1b1be1644d73891f311d0a1eb8eb",
      "value": 1031
     }
    },
    "67a0e8fcdcfa49fd8ce607169fd76c90": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0482685b18a4b08a4d47cf94b16d2d0",
       "IPY_MODEL_27e8637530184116b237a411245705a5"
      ],
      "layout": "IPY_MODEL_0aa7acf3f9c84a498b0e583105e86bd8"
     }
    },
    "684f038732ea49f1ab30a8984b415378": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4555fb305254c9e9d7fb63d9bb26bec",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5b0e47b9c7d74e6ca45746080c1c45b2",
      "value": "100% 548M/548M [00:16&lt;00:00, 32.7MB/s]"
     }
    },
    "6c8e1a0cba1e49c29aeb80e8cd9f51d8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ffafda6efc244eabd05a499661e1efe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7018b77e581340e3b70b9c78e81f94d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7428a9bdac9a439d9caa83f4f95f440f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_450ec91b8b154f3d9b1bb9de297132bf",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2214f935952241c896122dc30fdba56c",
      "value": 231508
     }
    },
    "74fa92af960941d58ffa5372a43d7d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7519dd0cc6b74d19afe6b2c290be14be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9e1b5fbfcf341e8a35d6b3c43ab3b13",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b62b06e0f98429c9390451a3bbce25d",
      "value": 456318
     }
    },
    "75eea0a1cb604f85aad6950b4cfe643e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7754dc2db14f46819ae717b97d258a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "778bbca832614cc88c611191a2d5fbff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2db97fa8ea044d7397bfc87a36629403",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e4c89670ac2248348acdb8250660de80",
      "value": "100% 939/939 [00:00&lt;00:00, 23.7kB/s]"
     }
    },
    "78349090137f4988a315a27f5eee097a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b7054cd78724aaeb2c677f04a2eba92",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c69931beb6e4e4fa379eb61a4855770",
      "value": 1042301
     }
    },
    "798c179ba832479abd0865f4122248ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_600b0c9be1084a829610ec52c5b89b44",
      "max": 260793700,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9a268fa34bd420db35632a38e010bd5",
      "value": 260793700
     }
    },
    "79a635b14f984f20b209e7e3dae38e56": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52574793d4324d098cc8826a1eb23212",
       "IPY_MODEL_7a0a55e08bfb4d9baa0777f6944936ae"
      ],
      "layout": "IPY_MODEL_54b79428b57744e696a8bee377960d2f"
     }
    },
    "79ebefb8cd7247d8b2f70c163723f817": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a0a55e08bfb4d9baa0777f6944936ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49fdd42cc5b0438cbd334a8fd66e7716",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_de054ece27e24ec7b7cbf067edbb2519",
      "value": "100% 230/230 [00:00&lt;00:00, 6.31kB/s]"
     }
    },
    "7acff30ed03845f390d2d28cd19142a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8306512600a449d9abc49c9c1d8b134",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c9b659972c542ff8c46a9b6f3601d9d",
      "value": 548118077
     }
    },
    "7b68753c33cc487db8941fae011fc6f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd7f78deed014a2ebda3f1a1f71c7510",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_21e86d0a58414b618e9b1e44ae0807f8",
      "value": "100% 224/224 [00:00&lt;00:00, 5.50kB/s]"
     }
    },
    "7c9b659972c542ff8c46a9b6f3601d9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7ece8e8c8c7e40378209c1d208096bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "81f0dc6a296c4dac97414eefbad06679": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52f34b5a72794d2fa14a71405a36edf5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9a6f3dc00aca4b0fb527191f2407cd3f",
      "value": "100% 230/230 [00:00&lt;00:00, 6.51kB/s]"
     }
    },
    "8350cd49576b4add8baa17b646de87c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7acff30ed03845f390d2d28cd19142a4",
       "IPY_MODEL_1fd2d488f6ab465386ac54de28cfa8be"
      ],
      "layout": "IPY_MODEL_bbd6d784245c4d2791c1a56a90e29292"
     }
    },
    "835df7e043fb439492cc8a0a79c8c1c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8515a6cd31b94ce3b90f393af567f60e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7428a9bdac9a439d9caa83f4f95f440f",
       "IPY_MODEL_52c4fadf83ee4ebf825dab5a3ca82177"
      ],
      "layout": "IPY_MODEL_da53fd3c326f48cd86f6c87d3d3e2515"
     }
    },
    "85a4f525707c4556a9c5ba9133154fd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbdf54eef1794e98bfc5ecbffdd49e59",
       "IPY_MODEL_16e58bf38bca4a7791e8a0bcf38d22bb"
      ],
      "layout": "IPY_MODEL_b0cacd0e5c00408d9760a97c2b8f2592"
     }
    },
    "864d218ed3894b8f9d178fb6d91d57ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c8ac724bedf46ffbad468fcb7a62854",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c4cfab5a2ec042d8a8b3a1d72bc60c4e",
      "value": "100% 546/546 [00:00&lt;00:00, 14.4kB/s]"
     }
    },
    "8717d409a1d34953ab3c933f39a98a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04eedd44f1784e44b967e3e21da0cc2b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ecc1252ff4bf478aa8358e5172eff480",
      "value": "100% 261M/261M [00:07&lt;00:00, 34.8MB/s]"
     }
    },
    "8ca99b53bae64f0f8826568b6458129c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d9dc4691f09421b82249759d59c52c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f5b4ac2eea540769fcf2462f2650186": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "910717bfec544ddfaad283feb5467b3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919cefdc63b84520a870546833239c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8da77a81bcc49538821479245cb868d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_40c30550ae044c3c81bb99d07b2b243f",
      "value": "100% 456k/456k [00:00&lt;00:00, 1.09MB/s]"
     }
    },
    "92bc2dd05ba24674b3bc505380775244": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "937686845ad344beac6752c4064ce331": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1eb6ce27e0743e99832105cca5f57e0",
      "max": 263273408,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37e659127e874855b294e29f3232c697",
      "value": 263273408
     }
    },
    "94c91ef43dce4460b08a36cbe0d25f16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a6f3dc00aca4b0fb527191f2407cd3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c5d8864d6c142f484dc995faabbcaa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ea22147ff7841c3b4e7ca3d44ed1e97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a02b997d8bd24a55ac5dd69c931e8e2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a4555fb305254c9e9d7fb63d9bb26bec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5c6c1f941914d2f9772e1a5016da6f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a673e9e0877144d28b7318f6c9f8842f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a82f1b1be1644d73891f311d0a1eb8eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a8da77a81bcc49538821479245cb868d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b088a650a0474d888ff52d1788de765c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0cacd0e5c00408d9760a97c2b8f2592": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1eb6ce27e0743e99832105cca5f57e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b20275d471674a2fad60015b59eaee8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_15f69272b659412fb72b3fbbf2de3481",
       "IPY_MODEL_81f0dc6a296c4dac97414eefbad06679"
      ],
      "layout": "IPY_MODEL_79ebefb8cd7247d8b2f70c163723f817"
     }
    },
    "b494596a80e740c4b78ce7cc1b4102f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b5845154f5c84d47966f9818145e995d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b797454c7f0b4dfe9972afad19682e9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b088a650a0474d888ff52d1788de765c",
      "max": 754,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f5b4ac2eea540769fcf2462f2650186",
      "value": 754
     }
    },
    "b87879eb1fb643cbba9f4e49cdeb9921": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b921eb0bb0f94d73a168dd1b5ddafa47": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b94d9faee7c64a4f8d00e7544746a392": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9fefded8b384a19a5370b5694fe97cd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba13ab99460f4dd8ac2d35059e890718": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bab4c89b8a46433da1649903e8832caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bbd6d784245c4d2791c1a56a90e29292": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcae8e6ce22b48a18c1ecad412a7ea80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bcb6792329da49cb853b67a24837f4ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c281ba50ecf445058cc8f669c949ce52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dbb3164b60949f89945db07f24b88c6",
       "IPY_MODEL_778bbca832614cc88c611191a2d5fbff"
      ],
      "layout": "IPY_MODEL_4d78a3d10ada4b7f817bc8ec8afcfa31"
     }
    },
    "c43e481e306a46c08639e3cdf903b758": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06b046743c9c4677bc50476642edf86b",
       "IPY_MODEL_1d474aaa9fcc4958906303906618f82d"
      ],
      "layout": "IPY_MODEL_25b9f6330a4d4fa4b275b7c1e0d2d678"
     }
    },
    "c4714f21d8514605a5d00cc86b897f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7519dd0cc6b74d19afe6b2c290be14be",
       "IPY_MODEL_5fbb258deee44ee09493d75ec60555b2"
      ],
      "layout": "IPY_MODEL_a673e9e0877144d28b7318f6c9f8842f"
     }
    },
    "c4cbcbcf9b2d487b95e493243912cfed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c533512948e24ecbb61011528ec5eb55",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_484daf2c6f5540d9b8fdd65129d97ea5",
      "value": "100% 213k/213k [00:00&lt;00:00, 967kB/s]"
     }
    },
    "c4cfab5a2ec042d8a8b3a1d72bc60c4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c533512948e24ecbb61011528ec5eb55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c566e7dfc8004acb91afe56d80d2cd09": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9e1b5fbfcf341e8a35d6b3c43ab3b13": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbdf54eef1794e98bfc5ecbffdd49e59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3843c77b6fb44d36aeb29f590d7b014e",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bab4c89b8a46433da1649903e8832caf",
      "value": 230
     }
    },
    "ccd661cec496474ebdfefa7e1f4a8926": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5c6c1f941914d2f9772e1a5016da6f0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_168914421c44490ba1d9ebef4dd2285b",
      "value": "100% 230/230 [00:00&lt;00:00, 4.95kB/s]"
     }
    },
    "cd2d2f02d6e14ec7bcbae92b5ab41ae2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd7e975035794a52ae29cca9542b788d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd7f78deed014a2ebda3f1a1f71c7510": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce12705a19764d7eaf45685f0cd31f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ce75547630244baabe6837ab3f5f4c54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d071e6f1dbb84a42b3734f8453b4d5f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1482d85804847ada9a43c0311f823cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_937686845ad344beac6752c4064ce331",
       "IPY_MODEL_110aa51584474035b9885ddb15006d46"
      ],
      "layout": "IPY_MODEL_30c428ba615a470ab6405498d5b47e26"
     }
    },
    "d49515c60cf847319a766c6ea71757d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d582e8dc3b464327abf3e4da0a7f9519": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5c91fb5e7ea4308a2585f2406632f82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ba929806a8b450f937a4d279d4cdcc1",
       "IPY_MODEL_919cefdc63b84520a870546833239c4a"
      ],
      "layout": "IPY_MODEL_e608bc0b294147488585d19d64164fa0"
     }
    },
    "da53fd3c326f48cd86f6c87d3d3e2515": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcc90a4b6edf461eb1f4785d94bb699c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd7e975035794a52ae29cca9542b788d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d582e8dc3b464327abf3e4da0a7f9519",
      "value": "100% 1.04M/1.04M [00:00&lt;00:00, 2.79MB/s]"
     }
    },
    "dcd9948b90884364aaa0302020454586": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd22f69726974e8692b388a3ead4481c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba13ab99460f4dd8ac2d35059e890718",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9c5d8864d6c142f484dc995faabbcaa4",
      "value": "100% 754/754 [00:00&lt;00:00, 26.2kB/s]"
     }
    },
    "ddeeea9f818f4da29fef748f1a11c6ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de054ece27e24ec7b7cbf067edbb2519": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df4f9a6441854f9599cf310df5dacfb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0482685b18a4b08a4d47cf94b16d2d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_910717bfec544ddfaad283feb5467b3a",
      "max": 224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4108dada4e7a436db3c1bf1d62eb71ce",
      "value": 224
     }
    },
    "e1b3ac54a9e84afeb4614e6b8f8ff4ba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4c89670ac2248348acdb8250660de80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e57097898c444dc59fc6bdf63569479e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e608bc0b294147488585d19d64164fa0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6c0481c0bfb465ba265bcdfad70951c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1cacdf03f2fc4ee2b6201d699daeb6c3",
       "IPY_MODEL_0d6be7f7746e4e21b34658476fe1cc63"
      ],
      "layout": "IPY_MODEL_ee95415ffe6b42c1adcadfd1bf0f9ad9"
     }
    },
    "e762d375038844ceb64ec475ab72810e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e921a62ea30847178f0ec3f0b077d52d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea9ac97b52ab4b7a8829625a44436d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ecc1252ff4bf478aa8358e5172eff480": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed7a086eb21143cd86db4196d1e669c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ee95415ffe6b42c1adcadfd1bf0f9ad9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eeac5bb82c744e728b10fe7a1b461b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f042a38cc7b24a5e8a80669573d940ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb7c9e6d106043caad53cce96c89b484",
       "IPY_MODEL_7b68753c33cc487db8941fae011fc6f6"
      ],
      "layout": "IPY_MODEL_94c91ef43dce4460b08a36cbe0d25f16"
     }
    },
    "f8306512600a449d9abc49c9c1d8b134": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9a268fa34bd420db35632a38e010bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fa1e59f9356944169597cc80b15ceb34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb7c9e6d106043caad53cce96c89b484": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9fefded8b384a19a5370b5694fe97cd",
      "max": 224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b494596a80e740c4b78ce7cc1b4102f3",
      "value": 224
     }
    },
    "fc7d08ca0dda4e3abd5cc5ed64ff4aa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23147d17f077408f84782289fea9d2d1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9ea22147ff7841c3b4e7ca3d44ed1e97",
      "value": "100% 268M/268M [00:08&lt;00:00, 32.8MB/s]"
     }
    },
    "fe20bb95b2784f4a8d15f0961440298c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
